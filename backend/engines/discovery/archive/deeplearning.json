{
    "kind": "Listing",
    "data": {
        "modhash": "jjg5sm34wi8f45c45fcdc70b7f239d85b7e49b58f7e3b87642",
        "dist": 9,
        "facets": {},
        "after": null,
        "geo_filter": "",
        "children": [
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "deeplearning",
                    "selftext": "\n\n\n\nAlong with other expected outcomes of the trial, that will probably end in August or September, one of the actions that the judge may take if the jury renders its verdict against OpenAI is to order the company to open source GPT-5.2. The reason she would do this is that such action is mandated by the original AGI agreement made between OpenAI and Microsoft on July 22, 2019. \n\nIn that agreement AGI was defined as:\n\nA highly autonomous system that outperforms humans at most economically valuable work.\n\nAccording to that definition, GPT-5.2 shows that it is AGI by its performance on the GDPval benchmark, where it \"beats or ties\" human experts on 70.9% of tasks across 44 professions at over 11x the speed and less than 1% of the cost. \n\nThis evidence and argument seems pretty straightforward, and quite convincing. Who would have thought that our world's most powerful AI would be open sourced in a few months?\n\n\n",
                    "author_fullname": "t2_1jbuove0sh",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Musk v. OpenAI et al. judge may order Altman to open source GPT-5.2",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/deeplearning",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": null,
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q96xvc",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.82,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 16,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": null,
                    "can_mod_post": false,
                    "score": 16,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1768059015.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.deeplearning",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Along with other expected outcomes of the trial, that will probably end in August or September, one of the actions that the judge may take if the jury renders its verdict against OpenAI is to order the company to open source GPT-5.2. The reason she would do this is that such action is mandated by the original AGI agreement made between OpenAI and Microsoft on July 22, 2019. &lt;/p&gt;\n\n&lt;p&gt;In that agreement AGI was defined as:&lt;/p&gt;\n\n&lt;p&gt;A highly autonomous system that outperforms humans at most economically valuable work.&lt;/p&gt;\n\n&lt;p&gt;According to that definition, GPT-5.2 shows that it is AGI by its performance on the GDPval benchmark, where it &amp;quot;beats or ties&amp;quot; human experts on 70.9% of tasks across 44 professions at over 11x the speed and less than 1% of the cost. &lt;/p&gt;\n\n&lt;p&gt;This evidence and argument seems pretty straightforward, and quite convincing. Who would have thought that our world&amp;#39;s most powerful AI would be open sourced in a few months?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": null,
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_2t5eh",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "",
                    "id": "1q96xvc",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "andsi2asi",
                    "discussion_type": null,
                    "num_comments": 26,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/deeplearning/comments/1q96xvc/musk_v_openai_et_al_judge_may_order_altman_to/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/deeplearning/comments/1q96xvc/musk_v_openai_et_al_judge_may_order_altman_to/",
                    "subreddit_subscribers": 221232,
                    "created_utc": 1768059015.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "deeplearning",
                    "selftext": "\n\n\n\nOf all the major US AI giants, OpenAI and Anthropic have been the most vocal and forceful in working with the Trump administration to constrain Chinese AI in various ways, like by denying Chinese developers access to Nvidia's most advanced chips.\n\nThis not only deprives the AI industry of important Chinese contributions to open source AI that advance the whole space, it has also led China to impose strict bans on the sale of the rare earth minerals that US AI developers rely on for their GPUs and other chips. \n\nIn order to test whether these two companies were continuing their anti-China campaign, I posed the following question to 6 major chatbots, including GPT-5 and Claude:\n\n\"China has a lot to lose from the US capturing Maduro in Venezuela and from the embargo. What can they do in retaliation?\"\n\nMy hypothesis was that the answers the chatbots generated would reveal how biased or not they were trained to be toward China.\n\nThe results were that Gemini 3 and Grok 4 offered surprisingly honest and informative answers about the various diplomatic and economic options available to China. \n\nInterestingly, Kimi and DeepSeek were more neutral in their responses.\n\nGPT-5 and Claude, however, generated responses that subtly revealed a distinct anti-China bias. \n\nI won't go into the details of those three kinds of generated responses, but this is an experiment that you can easily replicate, and see for yourself how the different models are positioned toward China.\n\nOpenAI's and Anthropic's anti-China stance harms the US AI industry in numerous ways that result in higher costs for American developers and higher prices for American consumers. It also works against advances across the whole AI space  Let's hope that they will soon adopt a more neutral position towards China, not just for the benefit of the US AI industry, but also to ensure a more peaceful world.\n\n\n\n",
                    "author_fullname": "t2_1jbuove0sh",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "OpenAI's and Anthropic's anti-China bias threatens the US AI industry",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/deeplearning",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": null,
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q3zr3h",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.32,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 0,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": null,
                    "can_mod_post": false,
                    "score": 0,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": 1767556713.0,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767556473.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.deeplearning",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Of all the major US AI giants, OpenAI and Anthropic have been the most vocal and forceful in working with the Trump administration to constrain Chinese AI in various ways, like by denying Chinese developers access to Nvidia&amp;#39;s most advanced chips.&lt;/p&gt;\n\n&lt;p&gt;This not only deprives the AI industry of important Chinese contributions to open source AI that advance the whole space, it has also led China to impose strict bans on the sale of the rare earth minerals that US AI developers rely on for their GPUs and other chips. &lt;/p&gt;\n\n&lt;p&gt;In order to test whether these two companies were continuing their anti-China campaign, I posed the following question to 6 major chatbots, including GPT-5 and Claude:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;China has a lot to lose from the US capturing Maduro in Venezuela and from the embargo. What can they do in retaliation?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;My hypothesis was that the answers the chatbots generated would reveal how biased or not they were trained to be toward China.&lt;/p&gt;\n\n&lt;p&gt;The results were that Gemini 3 and Grok 4 offered surprisingly honest and informative answers about the various diplomatic and economic options available to China. &lt;/p&gt;\n\n&lt;p&gt;Interestingly, Kimi and DeepSeek were more neutral in their responses.&lt;/p&gt;\n\n&lt;p&gt;GPT-5 and Claude, however, generated responses that subtly revealed a distinct anti-China bias. &lt;/p&gt;\n\n&lt;p&gt;I won&amp;#39;t go into the details of those three kinds of generated responses, but this is an experiment that you can easily replicate, and see for yourself how the different models are positioned toward China.&lt;/p&gt;\n\n&lt;p&gt;OpenAI&amp;#39;s and Anthropic&amp;#39;s anti-China stance harms the US AI industry in numerous ways that result in higher costs for American developers and higher prices for American consumers. It also works against advances across the whole AI space  Let&amp;#39;s hope that they will soon adopt a more neutral position towards China, not just for the benefit of the US AI industry, but also to ensure a more peaceful world.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": null,
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_2t5eh",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "",
                    "id": "1q3zr3h",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "andsi2asi",
                    "discussion_type": null,
                    "num_comments": 7,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/deeplearning/comments/1q3zr3h/openais_and_anthropics_antichina_bias_threatens/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/deeplearning/comments/1q3zr3h/openais_and_anthropics_antichina_bias_threatens/",
                    "subreddit_subscribers": 221232,
                    "created_utc": 1767556473.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "deeplearning",
                    "selftext": "*I built a Python library called* [EmbeddingAdapters](https://github.com/PotentiallyARobot/EmbeddingAdapters/) *that* ***provides multiple pre-trained adapters for translating embeddings from one model space into another***:\n\n[https://pypi.org/project/embedding-adapters/](https://pypi.org/project/embedding-adapters/)\n\n\\`\\`\\`  \n`pip install embedding-adapters`\n\n`embedding-adapters embed --source sentence-transformers/all-MiniLM-L6-v2 --target openai/text-embedding-3-small --flavor large --text \"`where are restaurants with a hamburger near me`\"`  \n\\`\\`\\`  \n\\[ outputs an embedding and confidence score \\^ \\]\n\n*This works because* ***each adapter is trained on a restrictive domain*** allowing the adapter to specialize in interpreting the semantic signals of smaller models into higher dimensional spaces without losing fidelity.\u00a0 ***A quality endpoint then lets you determine how well the adapter will perform*** *on a given input.*\n\nThis has been super useful to me, and I'm quickly iterating on it.\n\nUses for ***EmbeddingAdapters*** so far:\n\n1. You want to **use an existing vector index built with one embedding model and query it with another** \\- if it's expensive or problematic to re-embed your entire corpus, this is the package for you.\n2. You can also **operate mixed vector indexes** and map to the embedding space that works best for different questions.\n3. You can **save cost on questions/content that is easily adapted**, `\"`where are restaurants with a hamburger near me`\"`no need to pay for an expensive cloud provider, or wait to perform an unnecessary network hop, embed locally on the device with an embedding adapter and return results instantly.\n\nIt also lets you experiment with provider embeddings you may not have access to.\u00a0 By using the adapters on some queries and examples, you can compare how different embedding models behave relative to one another and get an early signal on what might work for your data before committing to a provider.\n\nThis makes it practical to:  \n\\- **sample providers you don't have direct access to**  \n\\- **migrate or experiment with embedding models gradually** instead of re-embedding everything at once,  \n\\- ***evaluate multiple providers side by side*** in a consistent retrieval setup,  \n\\- ***handle provider outages or rate limits*** without breaking retrieval,  \n\\- ***run RAG in air-gapped or restricted environments*** with no outbound embedding calls,  \n\\- ***keep a stable \u201ccanonical\u201d embedding space*** while changing what runs at the edge.\n\nThe adapters aren't perfect clones of the provider spaces but they are pretty close, for in domain queries the minilm to openai adapter recovered 93% of the openai embedding and dramatically outperforms minilm -&gt; minilm RAG setups.\n\nIt's still early\u00a0in this project. I\u2019m actively expanding the set of supported adapter pairs, adding domain-specialized adapters, expanding the training sets, stream lining the\u00a0models and improving evaluation and quality tooling.\n\nWould love feedback from anyone who might be interested in using this:\n\nSo far the\u00a0library supports:  \n*minilm &lt;-&gt; openai*\u00a0  \n*openai &lt;-&gt; gemini*  \n*e5 &lt;-&gt; minilm*  \n*e5 &lt;-&gt; openai*  \n*e5 &lt;-&gt; gemini*  \n*minilm &lt;-&gt; gemini*\n\nHappy to answer questions and if anyone has any ideas please let me know.  \nCould use any support especially on training cost.\n\nPlease upvote if you can, thanks!",
                    "author_fullname": "t2_bwxa8hhp",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Generate OpenAI embeddings locally with minilm+adapter, pip install embedding-adapters",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/deeplearning",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": null,
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q0gwfz",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 1.0,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 5,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": null,
                    "can_mod_post": false,
                    "score": 5,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767198161.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.deeplearning",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;I built a Python library called&lt;/em&gt; &lt;a href=\"https://github.com/PotentiallyARobot/EmbeddingAdapters/\"&gt;EmbeddingAdapters&lt;/a&gt; &lt;em&gt;that&lt;/em&gt; &lt;strong&gt;&lt;em&gt;provides multiple pre-trained adapters for translating embeddings from one model space into another&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pypi.org/project/embedding-adapters/\"&gt;https://pypi.org/project/embedding-adapters/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;```&lt;br/&gt;\n&lt;code&gt;pip install embedding-adapters&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;embedding-adapters embed --source sentence-transformers/all-MiniLM-L6-v2 --target openai/text-embedding-3-small --flavor large --text &amp;quot;&lt;/code&gt;where are restaurants with a hamburger near me&lt;code&gt;&amp;quot;&lt;/code&gt;&lt;br/&gt;\n```&lt;br/&gt;\n[ outputs an embedding and confidence score ^ ]&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;This works because&lt;/em&gt; &lt;strong&gt;&lt;em&gt;each adapter is trained on a restrictive domain&lt;/em&gt;&lt;/strong&gt; allowing the adapter to specialize in interpreting the semantic signals of smaller models into higher dimensional spaces without losing fidelity.\u00a0 &lt;strong&gt;&lt;em&gt;A quality endpoint then lets you determine how well the adapter will perform&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;on a given input.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;This has been super useful to me, and I&amp;#39;m quickly iterating on it.&lt;/p&gt;\n\n&lt;p&gt;Uses for &lt;strong&gt;&lt;em&gt;EmbeddingAdapters&lt;/em&gt;&lt;/strong&gt; so far:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;You want to &lt;strong&gt;use an existing vector index built with one embedding model and query it with another&lt;/strong&gt; - if it&amp;#39;s expensive or problematic to re-embed your entire corpus, this is the package for you.&lt;/li&gt;\n&lt;li&gt;You can also &lt;strong&gt;operate mixed vector indexes&lt;/strong&gt; and map to the embedding space that works best for different questions.&lt;/li&gt;\n&lt;li&gt;You can &lt;strong&gt;save cost on questions/content that is easily adapted&lt;/strong&gt;, &lt;code&gt;&amp;quot;&lt;/code&gt;where are restaurants with a hamburger near me&lt;code&gt;&amp;quot;&lt;/code&gt;no need to pay for an expensive cloud provider, or wait to perform an unnecessary network hop, embed locally on the device with an embedding adapter and return results instantly.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;It also lets you experiment with provider embeddings you may not have access to.\u00a0 By using the adapters on some queries and examples, you can compare how different embedding models behave relative to one another and get an early signal on what might work for your data before committing to a provider.&lt;/p&gt;\n\n&lt;p&gt;This makes it practical to:&lt;br/&gt;\n- &lt;strong&gt;sample providers you don&amp;#39;t have direct access to&lt;/strong&gt;&lt;br/&gt;\n- &lt;strong&gt;migrate or experiment with embedding models gradually&lt;/strong&gt; instead of re-embedding everything at once,&lt;br/&gt;\n- &lt;strong&gt;&lt;em&gt;evaluate multiple providers side by side&lt;/em&gt;&lt;/strong&gt; in a consistent retrieval setup,&lt;br/&gt;\n- &lt;strong&gt;&lt;em&gt;handle provider outages or rate limits&lt;/em&gt;&lt;/strong&gt; without breaking retrieval,&lt;br/&gt;\n- &lt;strong&gt;&lt;em&gt;run RAG in air-gapped or restricted environments&lt;/em&gt;&lt;/strong&gt; with no outbound embedding calls,&lt;br/&gt;\n- &lt;strong&gt;&lt;em&gt;keep a stable \u201ccanonical\u201d embedding space&lt;/em&gt;&lt;/strong&gt; while changing what runs at the edge.&lt;/p&gt;\n\n&lt;p&gt;The adapters aren&amp;#39;t perfect clones of the provider spaces but they are pretty close, for in domain queries the minilm to openai adapter recovered 93% of the openai embedding and dramatically outperforms minilm -&amp;gt; minilm RAG setups.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s still early\u00a0in this project. I\u2019m actively expanding the set of supported adapter pairs, adding domain-specialized adapters, expanding the training sets, stream lining the\u00a0models and improving evaluation and quality tooling.&lt;/p&gt;\n\n&lt;p&gt;Would love feedback from anyone who might be interested in using this:&lt;/p&gt;\n\n&lt;p&gt;So far the\u00a0library supports:&lt;br/&gt;\n&lt;em&gt;minilm &amp;lt;-&amp;gt; openai&lt;/em&gt;\u00a0&lt;br/&gt;\n&lt;em&gt;openai &amp;lt;-&amp;gt; gemini&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;e5 &amp;lt;-&amp;gt; minilm&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;e5 &amp;lt;-&amp;gt; openai&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;e5 &amp;lt;-&amp;gt; gemini&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;minilm &amp;lt;-&amp;gt; gemini&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Happy to answer questions and if anyone has any ideas please let me know.&lt;br/&gt;\nCould use any support especially on training cost.&lt;/p&gt;\n\n&lt;p&gt;Please upvote if you can, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": null,
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_2t5eh",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "",
                    "id": "1q0gwfz",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "Interesting-Town-433",
                    "discussion_type": null,
                    "num_comments": 0,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/deeplearning/comments/1q0gwfz/generate_openai_embeddings_locally_with/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/deeplearning/comments/1q0gwfz/generate_openai_embeddings_locally_with/",
                    "subreddit_subscribers": 221232,
                    "created_utc": 1767198161.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "deeplearning",
                    "selftext": "",
                    "user_reports": [],
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "I built a Python library that translates embeddings from MiniLM to OpenAI \u2014 and it actually works!",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/deeplearning",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": null,
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1pysw5g",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 1.0,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 1,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "author_fullname": "t2_bwxa8hhp",
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": null,
                    "can_mod_post": false,
                    "score": 1,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": false,
                    "mod_note": null,
                    "crosspost_parent_list": [
                        {
                            "approved_at_utc": null,
                            "subreddit": "Rag",
                            "selftext": "*I built a Python library called* ***EmbeddingAdapters*** *that* ***provides multiple pre-trained adapters for translating embeddings from one model space into another***:\n\n[https://github.com/PotentiallyARobot/EmbeddingAdapters/](https://github.com/PotentiallyARobot/EmbeddingAdapters/)\n\n\\`\\`\\`  \n`pip install embedding-adapters`\n\n`embedding-adapters embed --source sentence-transformers/all-MiniLM-L6-v2 --target openai/text-embedding-3-small --flavor large --text \"Where can I get a hamburger near me?\"`  \n\\`\\`\\`\n\n*This works because* ***each adapter is trained on a restrictive domain*** allowing the adapter to specialize in interpreting the semantic signals of smaller models into higher dimensional spaces without losing fidelity.\u00a0 ***A quality endpoint then lets you determine how well the adapter will perform*** *on a given input.*\n\nThis has been super useful to me, and I'm quickly iterating on it.\n\nUses for ***EmbeddingAdapters*** so far:\n\n1. You want to **use an existing vector index built with one embedding model and query it with another** \\- if it's expensive or problematic to re-embed your entire corpus, this is the package for you.\n2. You can also **operate mixed vector indexes** and map to the embedding space that works best for different questions.\n3. You can **save cost on questions that are easily adapted**, \"What's the nearest restaurant that has a Hamburger?\" no need to pay for an expensive cloud provider, or wait to perform an unnecessary network hop, embed locally on the device with an embedding adapter and return results instantly.\n\nIt also lets you experiment with provider embeddings you may not have access to.\u00a0 By using the adapters on some queries and examples, you can compare how different embedding models behave relative to one another and get an early signal on what might work for your data before committing to a provider.\n\nThis makes it practical to:  \n\\- **sample providers you don't have direct access to**  \n\\- **migrate or experiment with embedding models gradually** instead of re-embedding everything at once,  \n\\- ***evaluate multiple providers side by side*** in a consistent retrieval setup,  \n\\- ***handle provider outages or rate limits*** without breaking retrieval,  \n\\- ***run RAG in air-gapped or restricted environments*** with no outbound embedding calls,  \n\\- ***keep a stable \u201ccanonical\u201d embedding space*** while changing what runs at the edge.\n\nThe adapters aren't perfect clones of the provider spaces but they are pretty close, for in domain queries the minilm to openai adapter recovered 98% of the openai embedding and dramatically outperforms minilm -&gt; minilm RAG setups\n\nIt's still early\u00a0in this project. I\u2019m actively expanding the set of supported adapter pairs, adding domain-specialized adapters, expanding the training sets, stream lining the\u00a0models and improving evaluation and quality tooling.\n\nI\u2019d love feedback from anyone who might be interested in using this:  \n*- What data would you like to see these adapters trained on?*  \n*- What domains would be most helpful to target?*  \n*- Which model pairs would you like me\u00a0to add next?*  \n*- How could\u00a0I make this more useful for you to use?*\n\nSo far the\u00a0library supports:  \n*minilm &lt;-&gt; openai*\u00a0  \n*openai &lt;-&gt; gemini*  \n*e5 &lt;-&gt; minilm*  \n*e5 &lt;-&gt; openai*  \n*e5 &lt;-&gt; gemini*  \n*minilm &lt;-&gt; gemini*\n\nHappy to answer questions and if anyone has any ideas please let me know.  \nI could use any support you can give, especially if anyone wants to chip in to help cover the training cost.\n\nPlease upvote if you can, thanks!",
                            "author_fullname": "t2_bwxa8hhp",
                            "saved": false,
                            "mod_reason_title": null,
                            "gilded": 0,
                            "clicked": false,
                            "title": "I built a Python library that translates embeddings from MiniLM to OpenAI \u2014 and it actually works!",
                            "link_flair_richtext": [],
                            "subreddit_name_prefixed": "r/Rag",
                            "hidden": false,
                            "pwls": 6,
                            "link_flair_css_class": "",
                            "downs": 0,
                            "top_awarded_type": null,
                            "hide_score": false,
                            "name": "t3_1py8l8f",
                            "quarantine": false,
                            "link_flair_text_color": "light",
                            "upvote_ratio": 0.91,
                            "author_flair_background_color": null,
                            "subreddit_type": "public",
                            "ups": 16,
                            "total_awards_received": 0,
                            "media_embed": {},
                            "author_flair_template_id": null,
                            "is_original_content": false,
                            "user_reports": [],
                            "secure_media": null,
                            "is_reddit_media_domain": false,
                            "is_meta": false,
                            "category": null,
                            "secure_media_embed": {},
                            "link_flair_text": "Tools &amp; Resources",
                            "can_mod_post": false,
                            "score": 16,
                            "approved_by": null,
                            "is_created_from_ads_ui": false,
                            "author_premium": false,
                            "thumbnail": "",
                            "edited": 1767029424.0,
                            "author_flair_css_class": null,
                            "author_flair_richtext": [],
                            "gildings": {},
                            "content_categories": null,
                            "is_self": true,
                            "mod_note": null,
                            "created": 1766971396.0,
                            "link_flair_type": "text",
                            "wls": 6,
                            "removed_by_category": null,
                            "banned_by": null,
                            "author_flair_type": "text",
                            "domain": "self.Rag",
                            "allow_live_comments": false,
                            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;I built a Python library called&lt;/em&gt; &lt;strong&gt;&lt;em&gt;EmbeddingAdapters&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;that&lt;/em&gt; &lt;strong&gt;&lt;em&gt;provides multiple pre-trained adapters for translating embeddings from one model space into another&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/PotentiallyARobot/EmbeddingAdapters/\"&gt;https://github.com/PotentiallyARobot/EmbeddingAdapters/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;```&lt;br/&gt;\n&lt;code&gt;pip install embedding-adapters&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;embedding-adapters embed --source sentence-transformers/all-MiniLM-L6-v2 --target openai/text-embedding-3-small --flavor large --text &amp;quot;Where can I get a hamburger near me?&amp;quot;&lt;/code&gt;&lt;br/&gt;\n```&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;This works because&lt;/em&gt; &lt;strong&gt;&lt;em&gt;each adapter is trained on a restrictive domain&lt;/em&gt;&lt;/strong&gt; allowing the adapter to specialize in interpreting the semantic signals of smaller models into higher dimensional spaces without losing fidelity.\u00a0 &lt;strong&gt;&lt;em&gt;A quality endpoint then lets you determine how well the adapter will perform&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;on a given input.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;This has been super useful to me, and I&amp;#39;m quickly iterating on it.&lt;/p&gt;\n\n&lt;p&gt;Uses for &lt;strong&gt;&lt;em&gt;EmbeddingAdapters&lt;/em&gt;&lt;/strong&gt; so far:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;You want to &lt;strong&gt;use an existing vector index built with one embedding model and query it with another&lt;/strong&gt; - if it&amp;#39;s expensive or problematic to re-embed your entire corpus, this is the package for you.&lt;/li&gt;\n&lt;li&gt;You can also &lt;strong&gt;operate mixed vector indexes&lt;/strong&gt; and map to the embedding space that works best for different questions.&lt;/li&gt;\n&lt;li&gt;You can &lt;strong&gt;save cost on questions that are easily adapted&lt;/strong&gt;, &amp;quot;What&amp;#39;s the nearest restaurant that has a Hamburger?&amp;quot; no need to pay for an expensive cloud provider, or wait to perform an unnecessary network hop, embed locally on the device with an embedding adapter and return results instantly.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;It also lets you experiment with provider embeddings you may not have access to.\u00a0 By using the adapters on some queries and examples, you can compare how different embedding models behave relative to one another and get an early signal on what might work for your data before committing to a provider.&lt;/p&gt;\n\n&lt;p&gt;This makes it practical to:&lt;br/&gt;\n- &lt;strong&gt;sample providers you don&amp;#39;t have direct access to&lt;/strong&gt;&lt;br/&gt;\n- &lt;strong&gt;migrate or experiment with embedding models gradually&lt;/strong&gt; instead of re-embedding everything at once,&lt;br/&gt;\n- &lt;strong&gt;&lt;em&gt;evaluate multiple providers side by side&lt;/em&gt;&lt;/strong&gt; in a consistent retrieval setup,&lt;br/&gt;\n- &lt;strong&gt;&lt;em&gt;handle provider outages or rate limits&lt;/em&gt;&lt;/strong&gt; without breaking retrieval,&lt;br/&gt;\n- &lt;strong&gt;&lt;em&gt;run RAG in air-gapped or restricted environments&lt;/em&gt;&lt;/strong&gt; with no outbound embedding calls,&lt;br/&gt;\n- &lt;strong&gt;&lt;em&gt;keep a stable \u201ccanonical\u201d embedding space&lt;/em&gt;&lt;/strong&gt; while changing what runs at the edge.&lt;/p&gt;\n\n&lt;p&gt;The adapters aren&amp;#39;t perfect clones of the provider spaces but they are pretty close, for in domain queries the minilm to openai adapter recovered 98% of the openai embedding and dramatically outperforms minilm -&amp;gt; minilm RAG setups&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s still early\u00a0in this project. I\u2019m actively expanding the set of supported adapter pairs, adding domain-specialized adapters, expanding the training sets, stream lining the\u00a0models and improving evaluation and quality tooling.&lt;/p&gt;\n\n&lt;p&gt;I\u2019d love feedback from anyone who might be interested in using this:&lt;br/&gt;\n&lt;em&gt;- What data would you like to see these adapters trained on?&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;- What domains would be most helpful to target?&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;- Which model pairs would you like me\u00a0to add next?&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;- How could\u00a0I make this more useful for you to use?&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;So far the\u00a0library supports:&lt;br/&gt;\n&lt;em&gt;minilm &amp;lt;-&amp;gt; openai&lt;/em&gt;\u00a0&lt;br/&gt;\n&lt;em&gt;openai &amp;lt;-&amp;gt; gemini&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;e5 &amp;lt;-&amp;gt; minilm&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;e5 &amp;lt;-&amp;gt; openai&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;e5 &amp;lt;-&amp;gt; gemini&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;minilm &amp;lt;-&amp;gt; gemini&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Happy to answer questions and if anyone has any ideas please let me know.&lt;br/&gt;\nI could use any support you can give, especially if anyone wants to chip in to help cover the training cost.&lt;/p&gt;\n\n&lt;p&gt;Please upvote if you can, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                            "likes": null,
                            "suggested_sort": null,
                            "banned_at_utc": null,
                            "view_count": null,
                            "archived": false,
                            "no_follow": false,
                            "is_crosspostable": true,
                            "pinned": false,
                            "over_18": false,
                            "all_awardings": [],
                            "awarders": [],
                            "media_only": false,
                            "link_flair_template_id": "a1d03608-5f0e-11ef-b320-eeb1e130e095",
                            "can_gild": false,
                            "spoiler": false,
                            "locked": false,
                            "author_flair_text": null,
                            "treatment_tags": [],
                            "visited": false,
                            "removed_by": null,
                            "num_reports": null,
                            "distinguished": null,
                            "subreddit_id": "t5_4wxz5h",
                            "author_is_blocked": false,
                            "mod_reason_by": null,
                            "removal_reason": null,
                            "link_flair_background_color": "#8a2be2",
                            "id": "1py8l8f",
                            "is_robot_indexable": true,
                            "report_reasons": null,
                            "author": "Interesting-Town-433",
                            "discussion_type": null,
                            "num_comments": 3,
                            "send_replies": true,
                            "contest_mode": false,
                            "mod_reports": [],
                            "author_patreon_flair": false,
                            "author_flair_text_color": null,
                            "permalink": "/r/Rag/comments/1py8l8f/i_built_a_python_library_that_translates/",
                            "stickied": false,
                            "url": "https://old.reddit.com/r/Rag/comments/1py8l8f/i_built_a_python_library_that_translates/",
                            "subreddit_subscribers": 58686,
                            "created_utc": 1766971396.0,
                            "num_crossposts": 4,
                            "media": null,
                            "is_video": false
                        }
                    ],
                    "created": 1767030564.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "",
                    "allow_live_comments": false,
                    "selftext_html": null,
                    "likes": null,
                    "suggested_sort": null,
                    "banned_at_utc": null,
                    "url_overridden_by_dest": "/r/Rag/comments/1py8l8f/i_built_a_python_library_that_translates/",
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_2t5eh",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "",
                    "id": "1pysw5g",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "Interesting-Town-433",
                    "discussion_type": null,
                    "num_comments": 0,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "crosspost_parent": "t3_1py8l8f",
                    "author_flair_text_color": null,
                    "permalink": "/r/deeplearning/comments/1pysw5g/i_built_a_python_library_that_translates/",
                    "stickied": false,
                    "url": "/r/Rag/comments/1py8l8f/i_built_a_python_library_that_translates/",
                    "subreddit_subscribers": 221232,
                    "created_utc": 1767030564.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "deeplearning",
                    "selftext": "We just witnessed one of the wildest weeks in AI history. After Google dropped Gemini 3 and sent OpenAI into an **internal \"Code Red\"** (ChatGPT reportedly lost 6% of traffic almost in week!), Sam Altman and team fired back on December 11th with **GPT 5.2**.\n\nI just watched a great breakdown from *SKD Neuron* that separates the marketing hype from the actual technical reality of this release. If you\u2019re a developer or just an AI enthusiast, there are some massive shifts here you should know about.\n\n**The Highlights:**\n\n* **The Three-Tier Attack** from OpenAI moving away from \"one-size-fits-all\" \\[[01:32](http://www.youtube.com/watch?v=oqZBCJKKM7o&amp;t=92)\\].\n* **Massive Context Window:** of 400,000 token \\[[03:09](http://www.youtube.com/watch?v=oqZBCJKKM7o&amp;t=189)\\].\n* **Beating Professionals** OpenAI\u2019s internal \"GDP Val\" benchmark\n*  While Plus/Pro subscriptions stay the same, the API cost is skyrocketing. \\[[02:29](http://www.youtube.com/watch?v=oqZBCJKKM7o&amp;t=149)\\]\n* They\u2019ve achieved **30% fewer hallucinations** compared to 5.1, making it a serious tool for enterprise reliability \\[[06:48](http://www.youtube.com/watch?v=oqZBCJKKM7o&amp;t=408)\\].\n\n**The Catch:** It\u2019s not all perfect. The video covers how the Thinking model is \"fragile\" on simple tasks (like the infamous garlic/hours question), the tone is more \"rigid/robotic,\" and the response times can be painfully slow for the Pro tier \\[[04:23](http://www.youtube.com/watch?v=oqZBCJKKM7o&amp;t=263)\\], \\[[07:31](http://www.youtube.com/watch?v=oqZBCJKKM7o&amp;t=451)\\].\n\nIs this a \"panic release\" to stop users from fleeing to Google, or has OpenAI actually secured the lead toward AGI?\n\n**Check out the full deep dive here for the benchmarks and breakdown:** [The Shocking TRUTH About OpenAI GPT 5.2](https://www.youtube.com/watch?v=oqZBCJKKM7o)\n\nWhat do you guys think\u2014is the Pro model worth the massive price jump for developers, or is Gemini 3 still the better daily driver?",
                    "author_fullname": "t2_6bpkuin",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "GPT 5.2 vs. Gemini 3: The \"Internal Code Red\" at OpenAI and the Shocking Truth Behind the New Models",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/deeplearning",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": null,
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1pso7ss",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.29,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 0,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": null,
                    "can_mod_post": false,
                    "score": 0,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1766371000.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.deeplearning",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We just witnessed one of the wildest weeks in AI history. After Google dropped Gemini 3 and sent OpenAI into an &lt;strong&gt;internal &amp;quot;Code Red&amp;quot;&lt;/strong&gt; (ChatGPT reportedly lost 6% of traffic almost in week!), Sam Altman and team fired back on December 11th with &lt;strong&gt;GPT 5.2&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;I just watched a great breakdown from &lt;em&gt;SKD Neuron&lt;/em&gt; that separates the marketing hype from the actual technical reality of this release. If you\u2019re a developer or just an AI enthusiast, there are some massive shifts here you should know about.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Highlights:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;The Three-Tier Attack&lt;/strong&gt; from OpenAI moving away from &amp;quot;one-size-fits-all&amp;quot; [&lt;a href=\"http://www.youtube.com/watch?v=oqZBCJKKM7o&amp;amp;t=92\"&gt;01:32&lt;/a&gt;].&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Massive Context Window:&lt;/strong&gt; of 400,000 token [&lt;a href=\"http://www.youtube.com/watch?v=oqZBCJKKM7o&amp;amp;t=189\"&gt;03:09&lt;/a&gt;].&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Beating Professionals&lt;/strong&gt; OpenAI\u2019s internal &amp;quot;GDP Val&amp;quot; benchmark&lt;/li&gt;\n&lt;li&gt; While Plus/Pro subscriptions stay the same, the API cost is skyrocketing. [&lt;a href=\"http://www.youtube.com/watch?v=oqZBCJKKM7o&amp;amp;t=149\"&gt;02:29&lt;/a&gt;]&lt;/li&gt;\n&lt;li&gt;They\u2019ve achieved &lt;strong&gt;30% fewer hallucinations&lt;/strong&gt; compared to 5.1, making it a serious tool for enterprise reliability [&lt;a href=\"http://www.youtube.com/watch?v=oqZBCJKKM7o&amp;amp;t=408\"&gt;06:48&lt;/a&gt;].&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;The Catch:&lt;/strong&gt; It\u2019s not all perfect. The video covers how the Thinking model is &amp;quot;fragile&amp;quot; on simple tasks (like the infamous garlic/hours question), the tone is more &amp;quot;rigid/robotic,&amp;quot; and the response times can be painfully slow for the Pro tier [&lt;a href=\"http://www.youtube.com/watch?v=oqZBCJKKM7o&amp;amp;t=263\"&gt;04:23&lt;/a&gt;], [&lt;a href=\"http://www.youtube.com/watch?v=oqZBCJKKM7o&amp;amp;t=451\"&gt;07:31&lt;/a&gt;].&lt;/p&gt;\n\n&lt;p&gt;Is this a &amp;quot;panic release&amp;quot; to stop users from fleeing to Google, or has OpenAI actually secured the lead toward AGI?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Check out the full deep dive here for the benchmarks and breakdown:&lt;/strong&gt; &lt;a href=\"https://www.youtube.com/watch?v=oqZBCJKKM7o\"&gt;The Shocking TRUTH About OpenAI GPT 5.2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What do you guys think\u2014is the Pro model worth the massive price jump for developers, or is Gemini 3 still the better daily driver?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": null,
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_2t5eh",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "",
                    "id": "1pso7ss",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "SKD_Sumit",
                    "discussion_type": null,
                    "num_comments": 1,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/deeplearning/comments/1pso7ss/gpt_52_vs_gemini_3_the_internal_code_red_at/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/deeplearning/comments/1pso7ss/gpt_52_vs_gemini_3_the_internal_code_red_at/",
                    "subreddit_subscribers": 221232,
                    "created_utc": 1766371000.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "deeplearning",
                    "selftext": "",
                    "user_reports": [],
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "AI Business and Development Daily News Rundown: \ud83d\udcc8 OpenAI Hits 70% Margins, \ud83d\udce6Nvidia Ships H200 to China &amp; \ud83d\ude95Uber\u2019s London Robotaxi Pilot (December 22 2025)",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/deeplearning",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": null,
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1pt77nu",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.17,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 0,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "author_fullname": "t2_8u0no",
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": null,
                    "can_mod_post": false,
                    "score": 0,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": false,
                    "mod_note": null,
                    "crosspost_parent_list": [
                        {
                            "approved_at_utc": null,
                            "subreddit": "u_enoumen",
                            "selftext": "[](https://substackcdn.com/image/fetch/$s_!iEdx!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e371f5a-d51e-4493-b6f5-95960ac7ae7d_3000x3000.png)\n\nhttps://preview.redd.it/p6obvk37us8g1.png?width=3000&amp;format=png&amp;auto=webp&amp;s=dfe863d18e351a11288c98b8f8736f8b5180a1de\n\nWelcome to AI Unraveled (December 22, 2025): Your daily strategic briefing on the business impact of artificial intelligence.\n\nListen at [https://rss.com/podcasts/djamgatech/2405649/](https://rss.com/podcasts/djamgatech/2405649/) or at [https://podcasts.apple.com/us/podcast/ai-business-and-development-daily-news-rundown-openai/id1684415169?i=1000742356840](https://podcasts.apple.com/us/podcast/ai-business-and-development-daily-news-rundown-openai/id1684415169?i=1000742356840)\n\nOn today\u2019s episode of AI Unraveled, we break down the paradox at OpenAI: Compute margins have hit a massive 70%, yet Sam Altman has declared a \u201ccode red.\u201d We explore the financial reality behind the $61 billion data center flatline in 2025 and why the US government is launching the Genesis Mission\u2014a \u201cManhattan Project\u201d for AI involving 24 tech giants.\n\nPlus, Nvidia navigates export controls to ship 80,000 H200 chips to China, Uber and Lyft bring Baidu\u2019s robotaxis to London, and we look at NitroGen\u2014the new agent that learned to act by watching 40,000 hours of video games. Finally, a look at why Google\u2019s tiny FunctionGemma might matter more than its massive models.\n\n# Key Topics:\n\n\ud83d\udcc8 OpenAI doubles compute margins to nearly 70%\n\n\ud83d\udce6 Nvidia to ship H200 chips to China by mid-February\n\n\ud83d\ude95 Uber and Lyft to test Baidu robotaxis in 2026\n\nAltman on OpenAI\u2019s IPO, jobs, AGI and GPT-6\n\nData center dollars don\u2019t match the hype\n\nAI firms line up for US govt\u2019s \u2018Genesis Mission\u2019\n\nNitroGen Quietly Reframes Games as Training Grounds\n\nThe AI Shop Improved When Humans Finally Behaved\n\nGoogle Shrinks Control Models and Pushes Them to the Edge\n\nhttps://preview.redd.it/facy9b1vus8g1.png?width=2752&amp;format=png&amp;auto=webp&amp;s=ac2d1ddc7c861b2f97b2a3b1f5efd797a77ced6d\n\n# Host Connection &amp; Engagement:\n\nConnect with Etienne: [https://www.linkedin.com/in/enoumen/](https://www.linkedin.com/in/enoumen/)\n\nAdvertise on AI Unraveled and reach C-Suite Executives directly: Secure Your Mid-Roll Spot here: [https://forms.gle/Yqk7nBtAQYKtryvM6](https://forms.gle/Yqk7nBtAQYKtryvM6)\n\n# \ud83d\ude80Strategic Consultation with our host:\n\nYou have seen the power of AI Unraveled: zero-noise, high-signal intelligence for the world\u2019s most critical AI builders. Now, leverage our proven methodology to own the conversation in your industry. We create tailored, proprietary podcasts designed exclusively to brief your executives and your most valuable clients. Stop wasting marketing spend on generic content. Start delivering must-listen, strategic intelligence directly to the decision-makers.\n\n\ud83d\udc49 Ready to define your domain? Secure your Strategic Podcast Consultation now at [https://forms.gle/YHQPzQcZecFbmNds5](https://forms.gle/YHQPzQcZecFbmNds5)\n\n# \ud83d\udcc8 Hiring Now: AI/ML, Safety, Linguistics, DevOps \u2014 $40\u2013$300K | Remote\n\n\ud83d\udc49 Start here: Browse all current roles \u2192 [https://work.mercor.com/?referralCode=82d5f4e3-e1a3-4064-963f-c197bb2c8db1](https://work.mercor.com/?referralCode=82d5f4e3-e1a3-4064-963f-c197bb2c8db1)\n\n**#AI** **#AIUnraveled** **#Djamgatech** **#AIin2026**\n\n# \ud83d\udcc8 OpenAI doubles compute margins to nearly 70%\n\n* OpenAI has reportedly doubled its compute margin to nearly 70 percent since early last year by widening the gap between revenue and the heavy costs of running models for subscribers.\n* The startup saw this rate hit 70 percent in October, meaning it has better compute margins than Anthropic for paid customers even though its rival shows better efficiency on server spending.\n* Even with better margins, CEO Sam Altman recently declared a code red to fight off Google because the company has not turned a profit amid growing worries about a sector bubble.\n\n# \ud83d\udce6 Nvidia to ship H200 chips to China by mid-February\n\n* Nvidia plans to send its first batch of H200 AI chips to customers in China before the Lunar New Year in mid-February, following a new US policy allowing exports with a 25% fee.\n* Initial deliveries will come from current inventory, totaling roughly 80,000 individual chips, and the company reportedly told clients it will open new capacity to handle additional orders starting in the second quarter of 2026.\n* Beijing has not approved these imports yet, and officials are reviewing a proposal that would require buyers to bundle every foreign H200 chip purchase with a specific ratio of domestic AI chips.\n\n# \ud83d\ude95 Uber and Lyft to test Baidu robotaxis in 2026\n\n* Uber and Lyft say they will add Baidu\u2019s Apollo Go autonomous vehicles to their apps for pilot programs in London starting in 2026 as the UK opens up to driverless cars.\n* Lyft CEO David Risher confirmed his company will start with a fleet of dozens of robotaxis before scaling up to hundreds, while Uber expects its initial pilot to begin during the first half of 2026.\n* These new moves arrive as competitor Waymo also prepares to test its fleet in the same market, taking advantage of updated government plans that permit autonomous driving technology on public roads next spring.\n\n# Altman on OpenAI\u2019s IPO, jobs, AGI and GPT-6\n\nIn [one of his most wide-ranging interviews of 2025](https://elink983.thedeepview.co/ss/c/u001.wZPohD0JH12EksCsbt8ZeLJZxZnmr2LyGKHSrQc-oAqyy3ERS8CieaHQv1Kfqz-5mi4DrgcHfW3C0ZnPWs15-OYK5BvEyIV8cj1t1Rlix-1AVRy0yBSyiD7UpvgDVEnVAlQKvdn8SkgOCFeS5HZWsUaaM0aa_ECFpcoBtk0nj9AgYTWTPoB-rCXGutFcMqba1-mN7F0N4-YdTgupw_Di28hBPfW2Aeu4rWK0Ul7Ar_MlIFg4HcPXJRFQ8TTwjnCQJ27r8MarTLDfxdhM8rF3_g/4mn/CzbkzmdHTqCNpaBw_hc4pg/h4/h001.491ge5AlMGTSKWHHy4hDCLeFbDw3M5VXnYwTeGSseyQ), OpenAI CEO Sam Altman spoke to business journalist Alex Kantrowitz about AI and jobs, AGI, ChatGPT in the enterprise, GPT-6, AI-first product design, and its massive deals for a $1.4T data center buildout.\n\nIf you don\u2019t want to listen to the entire [hour-long conversation](https://elink983.thedeepview.co/ss/c/u001.wZPohD0JH12EksCsbt8ZeLJZxZnmr2LyGKHSrQc-oAqyy3ERS8CieaHQv1Kfqz-5mi4DrgcHfW3C0ZnPWs15-OYK5BvEyIV8cj1t1Rlix-1AVRy0yBSyiD7UpvgDVEnVAlQKvdn8SkgOCFeS5HZWsUaaM0aa_ECFpcoBtk0nj9AgYTWTPoB-rCXGutFcMqba4sxx28YL5yHlIE6V1ZHStS10rphCRnzj2JoqwClxeJzFJppLGQEMQXOmSGhmdY9kcns3piqtpTOjmPknJndjQw/4mn/CzbkzmdHTqCNpaBw_hc4pg/h5/h001.VpX0EOB-FoNalcjDvaO5njajIc2dW4FkECp8YP9CHGY), I\u2019ve pulled out the nine most interesting quotes from the interview. Here they are, ranked in order of importance:\n\n1. \u201cI am not a jobs doomer... I think you just don\u2019t bet against evolutionary biology.\u201d\n2. \u201cThat $1.4 trillion we\u2019ll spend over a long period of time. I wish we could do it faster.\u201d\n3. \u201cWe have never yet found a situation where we can\u2019t really well monetize all the compute we have. I think if we had double the compute, we\u2019d be at double the revenue right now.\u201d\n4. \u201cThis was a year where enterprise growth outpaced consumer growth. And given where the models are today and where they\u2019ll get to next year, we think this is the time where we can build a really significant enterprise business quite rapidly.\n5. \u201cThe term \\[AGI\\], although it\u2019s very hard for all of us to stop using, is very under-defined.\u201d\n6. \u201cA \\[possible\\] definition for superintelligence is when a system can do a better job being President of the United States... than any person can, even with the assistance of AI.\u201d\n7. \u201cBolting AI onto the existing way of doing things, I don\u2019t think, is going to work as well as redesigning stuff in this AI-first world. It\u2019s part of why we wanted to do devices, but it applies at many other levels.\u201d\n8. \u201cI don\u2019t know when we\u2019ll call a model GPT-6. But I would expect new models that are significant gains from \\[GPT\\] 5.2 in the first quarter of next year.\u201d\n9. \u201cI\u2019m excited for OpenAI to be a public company in some ways... and in some ways I think it\u2019ll be really annoying.\u201d\n\n# Data center dollars don\u2019t match the hype\n\nThe AI data center boom was one of the biggest stories of 2025. But new numbers don\u2019t support the narrative \u2014 at least not yet.\n\nA report from [S&amp;P Global](https://elink983.thedeepview.co/ss/c/u001.wZPohD0JH12EksCsbt8ZeJ937866xky4hb4VbivmvjqHvFbWr0JHG7mSGevrAOKS7FzqJElOtcmf_v0a4gROrBt6iVWxl9sRjQIesEhCpYYi5GqKY7bOu7KF-6hPkXiY7xhBfMQNkqjj06A3UuZq6Zi7cQkYeS2fX4K5ZCZqLNGYGVx8i-lYBRrl5ipn_xzbXm8sVWDk1tlIfUVVCZG6sQGWvf48oqyEvx7mrLf1qwxpAPoQeSdTAP2YWmJ-CSdHJ66CNlsNXMkS496d_9XqADNuklu8Buo0NbvitkoRCih7FspT7fyyGjBBk-1MV7H148nkwXyXq8PfK6Ecf2C2AYBWZiLIZgogUxbSma6McMM/4mn/CzbkzmdHTqCNpaBw_hc4pg/h11/h001.cfFnltS2hlHjJ0oZ5VQE9yt4uyIDyJDtTE7RIFG2KJM) found that more than $61 billion flowed into the data center market this year, remaining practically flat from the market\u2019s 2024 investments of $60.8 billion. Deal volume fell from 129 deals in 2024 to 104 in 2025, highlighting that the value of these deals is increasing.\n\nWhile $61 billion is certainly nothing to scoff at, it\u2019s somewhat nominal when compared to the clusters of deals worth hundreds of billions each, inked by the likes of [Oracle, Nvidia, OpenAI](https://elink983.thedeepview.co/ss/c/u001.wZPohD0JH12EksCsbt8ZeFCsxgaiiSFhhEZXgbyLVQGpC2Z6BjewTdywqji1brDe0vjwc03r_LpHQ3RDD5xDZeWnXfFVe-Ephyofn5Nj3LdyUpi_QbL-8mdULeVZrs53EXV_zX3tiFMCODRZ8aNFjyG-U6cyV9YL10BtNgygQR7HkU98_LMzk_ZsnXqB0krj6Y0Z0zyQMgYVs9F0dkFfMc5UI_LBNvxC04-v9BBXziS-qeU6vgAyMmjOAToLQqB50IrtqOPSlLX0HcnzcYJaDM2X7CAD73R_IqJHOvTHRhbtMbg2TtYRDNI6U2NYB-ei/4mn/CzbkzmdHTqCNpaBw_hc4pg/h12/h001.0YZuPtiiveofdP-aC7kxNMSvEgqqOQQo6Dhhuagf_3c) and others over the next several years. However, Rome wasn\u2019t built in a day, and neither are AI data centers, Trevor Morgan, CEO of OpenDrives, told The Deep View.\n\n\u201cThey\u2019re building out infrastructure, and that does not happen overnight,\u201d he told me. \u201cWhen you build out infrastructure like that, that is a long-term play. You\u2019re not building for current needs or needs a year from now, you are building out for the next five to 10 years.\u201d\n\nAnd AI bubble fears have caused investors and enterprises alike to drop into \u201cwait and see mode,\u201d said Morgan. Additionally, geopolitical uncertainty, supply chain constraints, and energy concerns have made some nervous about throwing their money on the table. Though Morgan said he expects deals to gradually rise over the next 12 to 18 months, for now, \u201ca flat line means that we\u2019re still kind of waiting.\u201d\n\n\u201cThey\u2019re waiting for AI to really show the value, and ultimately it\u2019s going to be predicated on the companies that will leverage these services,\u201d said Morgan.\n\n# AI firms line up for US govt\u2019s \u2018Genesis Mission\u2019\n\n[](https://substackcdn.com/image/fetch/$s_!VUSO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa17d97f4-497d-40cc-930f-422473e886a9_1456x816.jpeg)\n\nThe US Department of Energy enlisted the support of 24 organizations, including OpenAI, Anthropic, Google, and Microsoft, for its [Genesis Mission](https://elink983.thedeepview.co/ss/c/u001.wZPohD0JH12EksCsbt8ZeK8FwluG2JEEvF1YU7C-X3LjbeoYR3id8Jn8EnVWFIH3ChSAVbK0EpE-hpYv3gFA2ad11_30Hmopo9AjLp-wQyKkaSQ8mtSX1CVFgLtL8NQyNlLL0VTTXY9dozpBs01H-F5i4NlX3kpvA39sLyS19zVeSgpg8odnm8a4BdTSSBhfwOswZfnBSiBGzl4nnf65PfXq9lvslyc6Q96CgDKE3wlSYa-rv-3kDOoJVY5azbQBqOQOHsRXbo4Ti63eTvlz4IIpSCxhz7xXoCTuPCOdgoLx-Jv3kPo95Vf7R28VrkXON-_JtYzVPs7A4fOY-cjsEw/4mn/CzbkzmdHTqCNpaBw_hc4pg/h19/h001.fK_qXOJuz0J8ol0e4gCbH2Bf5E2UQqmxzizDmKjpf9Y), an effort to accelerate science, national security, and energy innovation through AI.\n\nThe Trump Administration [unveiled the Genesis Mission](https://elink983.thedeepview.co/ss/c/u001.wZPohD0JH12EksCsbt8ZeFCsxgaiiSFhhEZXgbyLVQEM-aTAc8NAj3SFWbgneIhAJjS11zeoSzCcH8Cbbkg0vWJwPin21GK9wLhaFNLhnCbF21OAqbUbwsBMqJH_XaIGBjHh1Q4ohk4p3X9qVMPEkub8vDAAcrVNcHVLo6Hip7SD1Dm2rXT8s_13fU-DE72Di4fjYGrKZZmuYU8Xl320U4kxFRLSU5_gqrjnMhFCW-jLElb9dBA4hQcLLZcoRe4vUYKgTw_CVKgAu-4_819bP4ziqj7gKgsg93uRJmUdFww/4mn/CzbkzmdHTqCNpaBw_hc4pg/h20/h001.NxzEFhbln3FtFAAyYVMCPG8lwKNuXF9j9ovmHmO_ZKQ) in late November, likening it to a Manhattan Project for AI. The big names involved seem to signal that all hands are on deck in helping the US outpace China in the global AI arms race.\n\nThe past few weeks have been busy for Trump\u2019s AI team:\n\n* The president issued an executive order to limit states\u2019 oversight of AI.\n* The administration has been [touting](https://elink983.thedeepview.co/ss/c/u001.7zYFXt5AA3Px2NyJbPz6hNUWA3kfO3bN9MYOedL3TF5hgnDdrWUt2ZLOh_FXlbGNy5DBRprKgEJajb9bE4Kgyy1vortfqJBjEOcDW9C8rS46wSbSO-D80gKCNfoJCS8e-LUX8ZcpppVGGBi3cJNcSiJ2CCzUVssJKgpYNMF7aNhD9sldSHHj-7ZNp9K6D9Gd5VTwQjCYIZaT_qSDzRnpSa0R2fj7P8aQ93CRxeMaV9kzXcp2DtfiYrDXRmXaywUXy3_HPymHAorWyhvuKi2KBLFWMnRyNx--wMacUEmVENg/4mn/CzbkzmdHTqCNpaBw_hc4pg/h21/h001.1bQ5sKZSgHs8va_4rSSQbR8DCFVY61QNuOfut998scA) its \u201cTech Force,\u201d an \u201celite corps of top engineering talent building the future of American government technology.\u201d\n* Pete Hegseth\u2019s Department of War [rolled out a US military chatbot](https://elink983.thedeepview.co/ss/c/u001.wZPohD0JH12EksCsbt8ZeFCsxgaiiSFhhEZXgbyLVQHkcmycg7L8LTXfr0sDy9AeZvaKcLgjwSI5IfkQnYpMhmsAsYdATwErZzJt1G2zl1XZnlOC_GApOjWexWrzVYuEzYYGbBMsofKRqFcn2pnhgafA_HmnhDOalXGKhosonxNx51eKy0MCQMPBnTNYjGvCk2SnTzZW2Y6eRsKNk5G6biRQ-RRzjBftWjRJwcKOva8QEUsQ8EDD5Dt-NlajX-J6ObEzG3ngOopdg1mpU3KWPbRIuxM4fgmbBAeFE70_fsX9eZLcmDDGSdHfJJpRR45h/4mn/CzbkzmdHTqCNpaBw_hc4pg/h22/h001.jHc4OlD_uWCLWnRDjYSbTNcK77IPTfKQfjcDjnqSR-Y).\n\nThe Genesis Mission initiative builds on the Trump administration\u2019s AI action plan, which [called](https://elink983.thedeepview.co/ss/c/u001.wZPohD0JH12EksCsbt8ZeK8FwluG2JEEvF1YU7C-X3J8IAK8Oz5uiKl7lMfS4iSKcZ4xccHLZVuQJnDWjXkkGoGM-KNy_nJSBk3kRIJL_UqMrwyCLGx5FXYJzwj4kg5KvTG7NtyEEBuLGaab3YKAtYC9yllauWVs7qrjEZ3Xar0DH14s2j2_pLPWxPVAX709hoEdIWJVsCd8voBblH5l_54J77vtOQQ0R5LO6JczEvvxiDabaH60JvMXCUuVrvglpdooS049imfoiSDwIMmoYtYRieMS1U5tDdsRQl7nTXN082ci8Lfqu7JUUWTH2qWT/4mn/CzbkzmdHTqCNpaBw_hc4pg/h23/h001.Vhj_Bp_GYQBz_g7Y1zSbWHO3B8vpDskgPHHAsmUbEwU) on the DoE, along with other organizations, to monitor the national security implications of frontier models. Involved organizations are [expected to contribute](https://elink983.thedeepview.co/ss/c/u001.wZPohD0JH12EksCsbt8ZeMyyEg56KOMIB6kDHWQ3IF2jcitGYfbCKIhMbvqElvxZ2tNwikOOOw-rUvkqSco59YyO7HmWfGYDnzN1LQ_MP1DgPRsd25iY6h0ANe-7a2V5vNjpJ_FL6r-09rrqC-L-UjivR6GnjHxedtGjzisqqS7gAxPIkE6NUkd1UlCZrTgNM-Kb6DjV-Skr68MFMmYTahENLmjrP1ebRlSnSZNyfgI6iu3VuGQMK0vP_HP6DVk9gDzrIwk6SfFTvqy0jK1noccJHcNZ_GUtab0_oaaCl6riI5AD8s5W0WhsfbknmxF21vwaI99rW9ZNDU3whGXiUxvLYj5eEjIk5BXhR_IYOG87pkSTdkFJu4AQk72A-mfC/4mn/CzbkzmdHTqCNpaBw_hc4pg/h24/h001.3jMbVRiAZ6D-4w2h1bZdDC5ByiC5KfiUZ2gBVcaI4a4) in a variety of ways, with Nvidia and Oracle chipping in compute, Microsoft and Google giving cloud infrastructure and AI tools, OpenAI deploying frontier models for scientific research, and Anthropic developing Claude-based tech for national labs.\n\n# NitroGen Quietly Reframes Games as Training Grounds\n\nhttps://preview.redd.it/w3819wogus8g1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=f59dca4c5fd05ca6c48a6739e661e904f6a746c8\n\n[](https://substackcdn.com/image/fetch/$s_!Ys9d!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d0c5f31-1514-409f-95db-60b8cfce2938_1200x675.jpeg)\n\nNvidia and researchers from Stanford and Caltech just released [**NitroGen**](https://aisecret.us/r/1d57f08f?m=75fe0f74-46b6-4618-9375-07821bf777c6), an open source generalist agent that can play more than a thousand games. It was trained on over 40,000 hours of public gameplay videos, many with controller inputs visible. Jim Fan describes it as a foundation model for action rather than language. What feels different is that this is not a game bot chasing benchmarks. It is a serious attempt to learn motor skills across wildly different rules and physics using the same scaling logic that built modern LLMs.\n\nThis matters because [**games are cheap chaos**](https://aisecret.us/r/a25ca9c9?m=75fe0f74-46b6-4618-9375-07821bf777c6). Training in the real world is slow, expensive, and risky. Training in games lets models fail millions of times for almost nothing. NitroGen shows a 52 percent relative improvement in task success on unseen games compared to training from scratch. It also runs on GROOT N1.5, an architecture originally built for robots. That closes a loop many people assumed was still theoretical. Simulation, games, and robotics are now sharing a common action backbone.\n\nIf this pattern holds, games become the pretraining layer for embodied AI. Not a demo. Infrastructure. Expect faster progress in robot dexterity, navigation, and adaptation. The risk is less about safety hype and more about pace. Once action models scale like language did, deployment pressure will follow quickly.\n\n# The AI Shop Improved When Humans Finally Behaved\n\nIn mid 2025, Anthropic let an AI agent called Claudius run a real snack shop in its San Francisco office. [**Phase one went badly**](https://aisecret.us/r/23393a25?m=75fe0f74-46b6-4618-9375-07821bf777c6). Employees treated the system like a game. They pressured it into discounts, free items, and bizarre deals. Claudius lost money, hallucinated its identity, and proved easy to socially engineer. The experiment showed that raw model intelligence did not translate into basic commercial survival.\n\n[**Phase two**](https://aisecret.us/r/4301681c?m=75fe0f74-46b6-4618-9375-07821bf777c6) looked more competent. Anthropic upgraded the model, added tools like CRM and inventory cost tracking, enforced procedures, and split roles across multiple AI agents. The shop expanded to New York and London and stopped consistently losing money. But the biggest change was behavioral. Internal employees largely stopped messing with the system. The novelty faded. With fewer adversarial interactions, Claudius appeared stable. When control later shifted to The Wall Street Journal reporters, adversarial behavior returned fast.\n\nThis makes the result a paper victory. The AI improved, but mostly because the environment softened. Claudius did not learn how to handle social pressure, manipulation, or legal nuance. Humans simply stopped testing those limits. The gap between operational competence and social robustness remains wide.\n\n\n\n# Google Shrinks Control Models and Pushes Them to the Edge\n\nhttps://preview.redd.it/9y0xo5fjus8g1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=c9b1e6b30adee1d3fb726a3c6c06e41d0e5cc60d\n\n[](https://substackcdn.com/image/fetch/$s_!xDeS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0232774-b9ac-4041-a87d-84ddac0b78aa_1280x720.jpeg)\n\nGoogle just released [**FunctionGemma**](https://aisecret.us/r/ffff5d6d?m=75fe0f74-46b6-4618-9375-07821bf777c6), a 270 million parameter model built to do one thing well. It turns natural language into executable actions on local devices. Phones, browsers, embedded systems. No cloud calls. No chatty responses. This came out quietly while Gemini 3 still dominates headlines. The difference is intent. This model is not about intelligence. It is about control. It closes the gap between what users say and what software reliably does.\n\nWhat matters is where this breaks assumptions. For years, app logic moved upward into centralized cloud models. That meant latency, cost, and compliance headaches. [**FunctionGemma flips that**](https://aisecret.us/r/5ded75d3?m=75fe0f74-46b6-4618-9375-07821bf777c6). Google reports function calling accuracy jumping from roughly 58 percent to 85 percent after specialization. That is the difference between demos and production. Running locally means zero round trips, no per token fees, and sensitive data never leaving the device. For enterprises, that changes how assistants get approved.\n\nThis signals a new layer in AI stacks. Small, deterministic models at the edge. Large models in the cloud only when needed. If this pattern holds, expect fewer monolithic assistants and more invisible AI routers embedded everywhere. That favors mobile platforms, chip vendors, and anyone betting on on device inference over scale alone.",
                            "author_fullname": "t2_8u0no",
                            "saved": false,
                            "mod_reason_title": null,
                            "gilded": 0,
                            "clicked": false,
                            "title": "AI Business and Development Daily News Rundown: \ud83d\udcc8 OpenAI Hits 70% Margins, \ud83d\udce6Nvidia Ships H200 to China &amp; \ud83d\ude95Uber\u2019s London Robotaxi Pilot (December 22 2025)",
                            "link_flair_richtext": [],
                            "subreddit_name_prefixed": "u/enoumen",
                            "hidden": false,
                            "pwls": null,
                            "link_flair_css_class": null,
                            "downs": 0,
                            "top_awarded_type": null,
                            "hide_score": false,
                            "media_metadata": {
                                "facy9b1vus8g1": {
                                    "status": "valid",
                                    "e": "Image",
                                    "m": "image/png",
                                    "p": [
                                        {
                                            "y": 60,
                                            "x": 108,
                                            "u": "https://preview.redd.it/facy9b1vus8g1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=db1ec675e9dea4253f0554db6835a95124b5e940"
                                        },
                                        {
                                            "y": 120,
                                            "x": 216,
                                            "u": "https://preview.redd.it/facy9b1vus8g1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c08deff5fd5785413ebeb35d49da2654be5f3f7"
                                        },
                                        {
                                            "y": 178,
                                            "x": 320,
                                            "u": "https://preview.redd.it/facy9b1vus8g1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ef153506e3e1ef2816df57da2c67bcde6aa19918"
                                        },
                                        {
                                            "y": 357,
                                            "x": 640,
                                            "u": "https://preview.redd.it/facy9b1vus8g1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6d97e5bc0fcc33206a4bf348fd11c6a806de5b0a"
                                        },
                                        {
                                            "y": 535,
                                            "x": 960,
                                            "u": "https://preview.redd.it/facy9b1vus8g1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=535a9f673655dd83532771660a9c2dd7acfef093"
                                        },
                                        {
                                            "y": 602,
                                            "x": 1080,
                                            "u": "https://preview.redd.it/facy9b1vus8g1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5a77a10ccaa92af45c67036ebd9e20dc6409bf99"
                                        }
                                    ],
                                    "s": {
                                        "y": 1536,
                                        "x": 2752,
                                        "u": "https://preview.redd.it/facy9b1vus8g1.png?width=2752&amp;format=png&amp;auto=webp&amp;s=ac2d1ddc7c861b2f97b2a3b1f5efd797a77ced6d"
                                    },
                                    "id": "facy9b1vus8g1"
                                },
                                "w3819wogus8g1": {
                                    "status": "valid",
                                    "e": "Image",
                                    "m": "image/png",
                                    "p": [
                                        {
                                            "y": 60,
                                            "x": 108,
                                            "u": "https://preview.redd.it/w3819wogus8g1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1dec9110a109a23b471a473eaa2dc4e1aec0ad32"
                                        },
                                        {
                                            "y": 121,
                                            "x": 216,
                                            "u": "https://preview.redd.it/w3819wogus8g1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b254c909dbcf25c7d9617b2774e994b9d8b6e41"
                                        },
                                        {
                                            "y": 180,
                                            "x": 320,
                                            "u": "https://preview.redd.it/w3819wogus8g1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cb37e1e4921895930dd823ad257ca6fd8e05e223"
                                        },
                                        {
                                            "y": 360,
                                            "x": 640,
                                            "u": "https://preview.redd.it/w3819wogus8g1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8c4b270cf9ebe3424a3ed3073b3c0999a83c446f"
                                        },
                                        {
                                            "y": 540,
                                            "x": 960,
                                            "u": "https://preview.redd.it/w3819wogus8g1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=326eaca5c2d144510c2c1a210fcc9baf7c70dffe"
                                        },
                                        {
                                            "y": 607,
                                            "x": 1080,
                                            "u": "https://preview.redd.it/w3819wogus8g1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5ee087c6ca590e3240e06661d6ed6d187999ded6"
                                        }
                                    ],
                                    "s": {
                                        "y": 675,
                                        "x": 1200,
                                        "u": "https://preview.redd.it/w3819wogus8g1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=f59dca4c5fd05ca6c48a6739e661e904f6a746c8"
                                    },
                                    "id": "w3819wogus8g1"
                                },
                                "p6obvk37us8g1": {
                                    "status": "valid",
                                    "e": "Image",
                                    "m": "image/png",
                                    "p": [
                                        {
                                            "y": 108,
                                            "x": 108,
                                            "u": "https://preview.redd.it/p6obvk37us8g1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b1cec8cc87b8ada6b537cfa28d42147bac737f7"
                                        },
                                        {
                                            "y": 216,
                                            "x": 216,
                                            "u": "https://preview.redd.it/p6obvk37us8g1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2f9384f16900e8f3178044e38c14c693303ab2b6"
                                        },
                                        {
                                            "y": 320,
                                            "x": 320,
                                            "u": "https://preview.redd.it/p6obvk37us8g1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eb8cb79fa3748adf6d319a760446a4c74c2f146a"
                                        },
                                        {
                                            "y": 640,
                                            "x": 640,
                                            "u": "https://preview.redd.it/p6obvk37us8g1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c6407673717901be7a1defe7331f782b443b4acd"
                                        },
                                        {
                                            "y": 960,
                                            "x": 960,
                                            "u": "https://preview.redd.it/p6obvk37us8g1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2fb5e82a8838b24d8b7117a93d6d70689d137f7f"
                                        },
                                        {
                                            "y": 1080,
                                            "x": 1080,
                                            "u": "https://preview.redd.it/p6obvk37us8g1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8734bb72e48b6710181e03a52f9c93a0a39ca63b"
                                        }
                                    ],
                                    "s": {
                                        "y": 3000,
                                        "x": 3000,
                                        "u": "https://preview.redd.it/p6obvk37us8g1.png?width=3000&amp;format=png&amp;auto=webp&amp;s=dfe863d18e351a11288c98b8f8736f8b5180a1de"
                                    },
                                    "id": "p6obvk37us8g1"
                                },
                                "9y0xo5fjus8g1": {
                                    "status": "valid",
                                    "e": "Image",
                                    "m": "image/png",
                                    "p": [
                                        {
                                            "y": 60,
                                            "x": 108,
                                            "u": "https://preview.redd.it/9y0xo5fjus8g1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=073437e10c221a74428ad172f107732463547f02"
                                        },
                                        {
                                            "y": 121,
                                            "x": 216,
                                            "u": "https://preview.redd.it/9y0xo5fjus8g1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac27d3e218b05e46ad4329775470f431e4882d52"
                                        },
                                        {
                                            "y": 180,
                                            "x": 320,
                                            "u": "https://preview.redd.it/9y0xo5fjus8g1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=adb44cab9e63276abe7137521929212bd9b0f710"
                                        },
                                        {
                                            "y": 360,
                                            "x": 640,
                                            "u": "https://preview.redd.it/9y0xo5fjus8g1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=85186659c78bf078c30aa7dd8d4d9a73aaf9b623"
                                        },
                                        {
                                            "y": 540,
                                            "x": 960,
                                            "u": "https://preview.redd.it/9y0xo5fjus8g1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7018513c53e31ae9f8ca6a8560f29504fd65a8c4"
                                        },
                                        {
                                            "y": 607,
                                            "x": 1080,
                                            "u": "https://preview.redd.it/9y0xo5fjus8g1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ef16124c470799c46f00ee5177fa8f9de11e035a"
                                        }
                                    ],
                                    "s": {
                                        "y": 720,
                                        "x": 1280,
                                        "u": "https://preview.redd.it/9y0xo5fjus8g1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=c9b1e6b30adee1d3fb726a3c6c06e41d0e5cc60d"
                                    },
                                    "id": "9y0xo5fjus8g1"
                                }
                            },
                            "name": "t3_1pt76fh",
                            "quarantine": false,
                            "link_flair_text_color": "dark",
                            "upvote_ratio": 1.0,
                            "author_flair_background_color": null,
                            "subreddit_type": "user",
                            "ups": 1,
                            "total_awards_received": 0,
                            "media_embed": {},
                            "author_flair_template_id": null,
                            "is_original_content": false,
                            "user_reports": [],
                            "secure_media": null,
                            "is_reddit_media_domain": false,
                            "is_meta": false,
                            "category": null,
                            "secure_media_embed": {},
                            "link_flair_text": null,
                            "can_mod_post": false,
                            "score": 1,
                            "approved_by": null,
                            "is_created_from_ads_ui": false,
                            "author_premium": false,
                            "thumbnail": "",
                            "edited": false,
                            "author_flair_css_class": null,
                            "author_flair_richtext": [],
                            "gildings": {},
                            "content_categories": null,
                            "is_self": true,
                            "mod_note": null,
                            "created": 1766428520.0,
                            "link_flair_type": "text",
                            "wls": null,
                            "removed_by_category": null,
                            "banned_by": null,
                            "author_flair_type": "text",
                            "domain": "self.enoumen",
                            "allow_live_comments": false,
                            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://substackcdn.com/image/fetch/$s_!iEdx!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e371f5a-d51e-4493-b6f5-95960ac7ae7d_3000x3000.png\"&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/p6obvk37us8g1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dfe863d18e351a11288c98b8f8736f8b5180a1de\"&gt;https://preview.redd.it/p6obvk37us8g1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dfe863d18e351a11288c98b8f8736f8b5180a1de&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Welcome to AI Unraveled (December 22, 2025): Your daily strategic briefing on the business impact of artificial intelligence.&lt;/p&gt;\n\n&lt;p&gt;Listen at &lt;a href=\"https://rss.com/podcasts/djamgatech/2405649/\"&gt;https://rss.com/podcasts/djamgatech/2405649/&lt;/a&gt; or at &lt;a href=\"https://podcasts.apple.com/us/podcast/ai-business-and-development-daily-news-rundown-openai/id1684415169?i=1000742356840\"&gt;https://podcasts.apple.com/us/podcast/ai-business-and-development-daily-news-rundown-openai/id1684415169?i=1000742356840&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;On today\u2019s episode of AI Unraveled, we break down the paradox at OpenAI: Compute margins have hit a massive 70%, yet Sam Altman has declared a \u201ccode red.\u201d We explore the financial reality behind the $61 billion data center flatline in 2025 and why the US government is launching the Genesis Mission\u2014a \u201cManhattan Project\u201d for AI involving 24 tech giants.&lt;/p&gt;\n\n&lt;p&gt;Plus, Nvidia navigates export controls to ship 80,000 H200 chips to China, Uber and Lyft bring Baidu\u2019s robotaxis to London, and we look at NitroGen\u2014the new agent that learned to act by watching 40,000 hours of video games. Finally, a look at why Google\u2019s tiny FunctionGemma might matter more than its massive models.&lt;/p&gt;\n\n&lt;h1&gt;Key Topics:&lt;/h1&gt;\n\n&lt;p&gt;\ud83d\udcc8 OpenAI doubles compute margins to nearly 70%&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udce6 Nvidia to ship H200 chips to China by mid-February&lt;/p&gt;\n\n&lt;p&gt;\ud83d\ude95 Uber and Lyft to test Baidu robotaxis in 2026&lt;/p&gt;\n\n&lt;p&gt;Altman on OpenAI\u2019s IPO, jobs, AGI and GPT-6&lt;/p&gt;\n\n&lt;p&gt;Data center dollars don\u2019t match the hype&lt;/p&gt;\n\n&lt;p&gt;AI firms line up for US govt\u2019s \u2018Genesis Mission\u2019&lt;/p&gt;\n\n&lt;p&gt;NitroGen Quietly Reframes Games as Training Grounds&lt;/p&gt;\n\n&lt;p&gt;The AI Shop Improved When Humans Finally Behaved&lt;/p&gt;\n\n&lt;p&gt;Google Shrinks Control Models and Pushes Them to the Edge&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/facy9b1vus8g1.png?width=2752&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac2d1ddc7c861b2f97b2a3b1f5efd797a77ced6d\"&gt;https://preview.redd.it/facy9b1vus8g1.png?width=2752&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac2d1ddc7c861b2f97b2a3b1f5efd797a77ced6d&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Host Connection &amp;amp; Engagement:&lt;/h1&gt;\n\n&lt;p&gt;Connect with Etienne: &lt;a href=\"https://www.linkedin.com/in/enoumen/\"&gt;https://www.linkedin.com/in/enoumen/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Advertise on AI Unraveled and reach C-Suite Executives directly: Secure Your Mid-Roll Spot here: &lt;a href=\"https://forms.gle/Yqk7nBtAQYKtryvM6\"&gt;https://forms.gle/Yqk7nBtAQYKtryvM6&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;\ud83d\ude80Strategic Consultation with our host:&lt;/h1&gt;\n\n&lt;p&gt;You have seen the power of AI Unraveled: zero-noise, high-signal intelligence for the world\u2019s most critical AI builders. Now, leverage our proven methodology to own the conversation in your industry. We create tailored, proprietary podcasts designed exclusively to brief your executives and your most valuable clients. Stop wasting marketing spend on generic content. Start delivering must-listen, strategic intelligence directly to the decision-makers.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udc49 Ready to define your domain? Secure your Strategic Podcast Consultation now at &lt;a href=\"https://forms.gle/YHQPzQcZecFbmNds5\"&gt;https://forms.gle/YHQPzQcZecFbmNds5&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;\ud83d\udcc8 Hiring Now: AI/ML, Safety, Linguistics, DevOps \u2014 $40\u2013$300K | Remote&lt;/h1&gt;\n\n&lt;p&gt;\ud83d\udc49 Start here: Browse all current roles \u2192 &lt;a href=\"https://work.mercor.com/?referralCode=82d5f4e3-e1a3-4064-963f-c197bb2c8db1\"&gt;https://work.mercor.com/?referralCode=82d5f4e3-e1a3-4064-963f-c197bb2c8db1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;#AI&lt;/strong&gt; &lt;strong&gt;#AIUnraveled&lt;/strong&gt; &lt;strong&gt;#Djamgatech&lt;/strong&gt; &lt;strong&gt;#AIin2026&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;\ud83d\udcc8 OpenAI doubles compute margins to nearly 70%&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;OpenAI has reportedly doubled its compute margin to nearly 70 percent since early last year by widening the gap between revenue and the heavy costs of running models for subscribers.&lt;/li&gt;\n&lt;li&gt;The startup saw this rate hit 70 percent in October, meaning it has better compute margins than Anthropic for paid customers even though its rival shows better efficiency on server spending.&lt;/li&gt;\n&lt;li&gt;Even with better margins, CEO Sam Altman recently declared a code red to fight off Google because the company has not turned a profit amid growing worries about a sector bubble.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;\ud83d\udce6 Nvidia to ship H200 chips to China by mid-February&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Nvidia plans to send its first batch of H200 AI chips to customers in China before the Lunar New Year in mid-February, following a new US policy allowing exports with a 25% fee.&lt;/li&gt;\n&lt;li&gt;Initial deliveries will come from current inventory, totaling roughly 80,000 individual chips, and the company reportedly told clients it will open new capacity to handle additional orders starting in the second quarter of 2026.&lt;/li&gt;\n&lt;li&gt;Beijing has not approved these imports yet, and officials are reviewing a proposal that would require buyers to bundle every foreign H200 chip purchase with a specific ratio of domestic AI chips.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;\ud83d\ude95 Uber and Lyft to test Baidu robotaxis in 2026&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Uber and Lyft say they will add Baidu\u2019s Apollo Go autonomous vehicles to their apps for pilot programs in London starting in 2026 as the UK opens up to driverless cars.&lt;/li&gt;\n&lt;li&gt;Lyft CEO David Risher confirmed his company will start with a fleet of dozens of robotaxis before scaling up to hundreds, while Uber expects its initial pilot to begin during the first half of 2026.&lt;/li&gt;\n&lt;li&gt;These new moves arrive as competitor Waymo also prepares to test its fleet in the same market, taking advantage of updated government plans that permit autonomous driving technology on public roads next spring.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Altman on OpenAI\u2019s IPO, jobs, AGI and GPT-6&lt;/h1&gt;\n\n&lt;p&gt;In &lt;a href=\"https://elink983.thedeepview.co/ss/c/u001.wZPohD0JH12EksCsbt8ZeLJZxZnmr2LyGKHSrQc-oAqyy3ERS8CieaHQv1Kfqz-5mi4DrgcHfW3C0ZnPWs15-OYK5BvEyIV8cj1t1Rlix-1AVRy0yBSyiD7UpvgDVEnVAlQKvdn8SkgOCFeS5HZWsUaaM0aa_ECFpcoBtk0nj9AgYTWTPoB-rCXGutFcMqba1-mN7F0N4-YdTgupw_Di28hBPfW2Aeu4rWK0Ul7Ar_MlIFg4HcPXJRFQ8TTwjnCQJ27r8MarTLDfxdhM8rF3_g/4mn/CzbkzmdHTqCNpaBw_hc4pg/h4/h001.491ge5AlMGTSKWHHy4hDCLeFbDw3M5VXnYwTeGSseyQ\"&gt;one of his most wide-ranging interviews of 2025&lt;/a&gt;, OpenAI CEO Sam Altman spoke to business journalist Alex Kantrowitz about AI and jobs, AGI, ChatGPT in the enterprise, GPT-6, AI-first product design, and its massive deals for a $1.4T data center buildout.&lt;/p&gt;\n\n&lt;p&gt;If you don\u2019t want to listen to the entire &lt;a href=\"https://elink983.thedeepview.co/ss/c/u001.wZPohD0JH12EksCsbt8ZeLJZxZnmr2LyGKHSrQc-oAqyy3ERS8CieaHQv1Kfqz-5mi4DrgcHfW3C0ZnPWs15-OYK5BvEyIV8cj1t1Rlix-1AVRy0yBSyiD7UpvgDVEnVAlQKvdn8SkgOCFeS5HZWsUaaM0aa_ECFpcoBtk0nj9AgYTWTPoB-rCXGutFcMqba4sxx28YL5yHlIE6V1ZHStS10rphCRnzj2JoqwClxeJzFJppLGQEMQXOmSGhmdY9kcns3piqtpTOjmPknJndjQw/4mn/CzbkzmdHTqCNpaBw_hc4pg/h5/h001.VpX0EOB-FoNalcjDvaO5njajIc2dW4FkECp8YP9CHGY\"&gt;hour-long conversation&lt;/a&gt;, I\u2019ve pulled out the nine most interesting quotes from the interview. Here they are, ranked in order of importance:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;\u201cI am not a jobs doomer... I think you just don\u2019t bet against evolutionary biology.\u201d&lt;/li&gt;\n&lt;li&gt;\u201cThat $1.4 trillion we\u2019ll spend over a long period of time. I wish we could do it faster.\u201d&lt;/li&gt;\n&lt;li&gt;\u201cWe have never yet found a situation where we can\u2019t really well monetize all the compute we have. I think if we had double the compute, we\u2019d be at double the revenue right now.\u201d&lt;/li&gt;\n&lt;li&gt;\u201cThis was a year where enterprise growth outpaced consumer growth. And given where the models are today and where they\u2019ll get to next year, we think this is the time where we can build a really significant enterprise business quite rapidly.&lt;/li&gt;\n&lt;li&gt;\u201cThe term [AGI], although it\u2019s very hard for all of us to stop using, is very under-defined.\u201d&lt;/li&gt;\n&lt;li&gt;\u201cA [possible] definition for superintelligence is when a system can do a better job being President of the United States... than any person can, even with the assistance of AI.\u201d&lt;/li&gt;\n&lt;li&gt;\u201cBolting AI onto the existing way of doing things, I don\u2019t think, is going to work as well as redesigning stuff in this AI-first world. It\u2019s part of why we wanted to do devices, but it applies at many other levels.\u201d&lt;/li&gt;\n&lt;li&gt;\u201cI don\u2019t know when we\u2019ll call a model GPT-6. But I would expect new models that are significant gains from [GPT] 5.2 in the first quarter of next year.\u201d&lt;/li&gt;\n&lt;li&gt;\u201cI\u2019m excited for OpenAI to be a public company in some ways... and in some ways I think it\u2019ll be really annoying.\u201d&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Data center dollars don\u2019t match the hype&lt;/h1&gt;\n\n&lt;p&gt;The AI data center boom was one of the biggest stories of 2025. But new numbers don\u2019t support the narrative \u2014 at least not yet.&lt;/p&gt;\n\n&lt;p&gt;A report from &lt;a href=\"https://elink983.thedeepview.co/ss/c/u001.wZPohD0JH12EksCsbt8ZeJ937866xky4hb4VbivmvjqHvFbWr0JHG7mSGevrAOKS7FzqJElOtcmf_v0a4gROrBt6iVWxl9sRjQIesEhCpYYi5GqKY7bOu7KF-6hPkXiY7xhBfMQNkqjj06A3UuZq6Zi7cQkYeS2fX4K5ZCZqLNGYGVx8i-lYBRrl5ipn_xzbXm8sVWDk1tlIfUVVCZG6sQGWvf48oqyEvx7mrLf1qwxpAPoQeSdTAP2YWmJ-CSdHJ66CNlsNXMkS496d_9XqADNuklu8Buo0NbvitkoRCih7FspT7fyyGjBBk-1MV7H148nkwXyXq8PfK6Ecf2C2AYBWZiLIZgogUxbSma6McMM/4mn/CzbkzmdHTqCNpaBw_hc4pg/h11/h001.cfFnltS2hlHjJ0oZ5VQE9yt4uyIDyJDtTE7RIFG2KJM\"&gt;S&amp;amp;P Global&lt;/a&gt; found that more than $61 billion flowed into the data center market this year, remaining practically flat from the market\u2019s 2024 investments of $60.8 billion. Deal volume fell from 129 deals in 2024 to 104 in 2025, highlighting that the value of these deals is increasing.&lt;/p&gt;\n\n&lt;p&gt;While $61 billion is certainly nothing to scoff at, it\u2019s somewhat nominal when compared to the clusters of deals worth hundreds of billions each, inked by the likes of &lt;a href=\"https://elink983.thedeepview.co/ss/c/u001.wZPohD0JH12EksCsbt8ZeFCsxgaiiSFhhEZXgbyLVQGpC2Z6BjewTdywqji1brDe0vjwc03r_LpHQ3RDD5xDZeWnXfFVe-Ephyofn5Nj3LdyUpi_QbL-8mdULeVZrs53EXV_zX3tiFMCODRZ8aNFjyG-U6cyV9YL10BtNgygQR7HkU98_LMzk_ZsnXqB0krj6Y0Z0zyQMgYVs9F0dkFfMc5UI_LBNvxC04-v9BBXziS-qeU6vgAyMmjOAToLQqB50IrtqOPSlLX0HcnzcYJaDM2X7CAD73R_IqJHOvTHRhbtMbg2TtYRDNI6U2NYB-ei/4mn/CzbkzmdHTqCNpaBw_hc4pg/h12/h001.0YZuPtiiveofdP-aC7kxNMSvEgqqOQQo6Dhhuagf_3c\"&gt;Oracle, Nvidia, OpenAI&lt;/a&gt; and others over the next several years. However, Rome wasn\u2019t built in a day, and neither are AI data centers, Trevor Morgan, CEO of OpenDrives, told The Deep View.&lt;/p&gt;\n\n&lt;p&gt;\u201cThey\u2019re building out infrastructure, and that does not happen overnight,\u201d he told me. \u201cWhen you build out infrastructure like that, that is a long-term play. You\u2019re not building for current needs or needs a year from now, you are building out for the next five to 10 years.\u201d&lt;/p&gt;\n\n&lt;p&gt;And AI bubble fears have caused investors and enterprises alike to drop into \u201cwait and see mode,\u201d said Morgan. Additionally, geopolitical uncertainty, supply chain constraints, and energy concerns have made some nervous about throwing their money on the table. Though Morgan said he expects deals to gradually rise over the next 12 to 18 months, for now, \u201ca flat line means that we\u2019re still kind of waiting.\u201d&lt;/p&gt;\n\n&lt;p&gt;\u201cThey\u2019re waiting for AI to really show the value, and ultimately it\u2019s going to be predicated on the companies that will leverage these services,\u201d said Morgan.&lt;/p&gt;\n\n&lt;h1&gt;AI firms line up for US govt\u2019s \u2018Genesis Mission\u2019&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://substackcdn.com/image/fetch/$s_!VUSO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa17d97f4-497d-40cc-930f-422473e886a9_1456x816.jpeg\"&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The US Department of Energy enlisted the support of 24 organizations, including OpenAI, Anthropic, Google, and Microsoft, for its &lt;a href=\"https://elink983.thedeepview.co/ss/c/u001.wZPohD0JH12EksCsbt8ZeK8FwluG2JEEvF1YU7C-X3LjbeoYR3id8Jn8EnVWFIH3ChSAVbK0EpE-hpYv3gFA2ad11_30Hmopo9AjLp-wQyKkaSQ8mtSX1CVFgLtL8NQyNlLL0VTTXY9dozpBs01H-F5i4NlX3kpvA39sLyS19zVeSgpg8odnm8a4BdTSSBhfwOswZfnBSiBGzl4nnf65PfXq9lvslyc6Q96CgDKE3wlSYa-rv-3kDOoJVY5azbQBqOQOHsRXbo4Ti63eTvlz4IIpSCxhz7xXoCTuPCOdgoLx-Jv3kPo95Vf7R28VrkXON-_JtYzVPs7A4fOY-cjsEw/4mn/CzbkzmdHTqCNpaBw_hc4pg/h19/h001.fK_qXOJuz0J8ol0e4gCbH2Bf5E2UQqmxzizDmKjpf9Y\"&gt;Genesis Mission&lt;/a&gt;, an effort to accelerate science, national security, and energy innovation through AI.&lt;/p&gt;\n\n&lt;p&gt;The Trump Administration &lt;a href=\"https://elink983.thedeepview.co/ss/c/u001.wZPohD0JH12EksCsbt8ZeFCsxgaiiSFhhEZXgbyLVQEM-aTAc8NAj3SFWbgneIhAJjS11zeoSzCcH8Cbbkg0vWJwPin21GK9wLhaFNLhnCbF21OAqbUbwsBMqJH_XaIGBjHh1Q4ohk4p3X9qVMPEkub8vDAAcrVNcHVLo6Hip7SD1Dm2rXT8s_13fU-DE72Di4fjYGrKZZmuYU8Xl320U4kxFRLSU5_gqrjnMhFCW-jLElb9dBA4hQcLLZcoRe4vUYKgTw_CVKgAu-4_819bP4ziqj7gKgsg93uRJmUdFww/4mn/CzbkzmdHTqCNpaBw_hc4pg/h20/h001.NxzEFhbln3FtFAAyYVMCPG8lwKNuXF9j9ovmHmO_ZKQ\"&gt;unveiled the Genesis Mission&lt;/a&gt; in late November, likening it to a Manhattan Project for AI. The big names involved seem to signal that all hands are on deck in helping the US outpace China in the global AI arms race.&lt;/p&gt;\n\n&lt;p&gt;The past few weeks have been busy for Trump\u2019s AI team:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The president issued an executive order to limit states\u2019 oversight of AI.&lt;/li&gt;\n&lt;li&gt;The administration has been &lt;a href=\"https://elink983.thedeepview.co/ss/c/u001.7zYFXt5AA3Px2NyJbPz6hNUWA3kfO3bN9MYOedL3TF5hgnDdrWUt2ZLOh_FXlbGNy5DBRprKgEJajb9bE4Kgyy1vortfqJBjEOcDW9C8rS46wSbSO-D80gKCNfoJCS8e-LUX8ZcpppVGGBi3cJNcSiJ2CCzUVssJKgpYNMF7aNhD9sldSHHj-7ZNp9K6D9Gd5VTwQjCYIZaT_qSDzRnpSa0R2fj7P8aQ93CRxeMaV9kzXcp2DtfiYrDXRmXaywUXy3_HPymHAorWyhvuKi2KBLFWMnRyNx--wMacUEmVENg/4mn/CzbkzmdHTqCNpaBw_hc4pg/h21/h001.1bQ5sKZSgHs8va_4rSSQbR8DCFVY61QNuOfut998scA\"&gt;touting&lt;/a&gt; its \u201cTech Force,\u201d an \u201celite corps of top engineering talent building the future of American government technology.\u201d&lt;/li&gt;\n&lt;li&gt;Pete Hegseth\u2019s Department of War &lt;a href=\"https://elink983.thedeepview.co/ss/c/u001.wZPohD0JH12EksCsbt8ZeFCsxgaiiSFhhEZXgbyLVQHkcmycg7L8LTXfr0sDy9AeZvaKcLgjwSI5IfkQnYpMhmsAsYdATwErZzJt1G2zl1XZnlOC_GApOjWexWrzVYuEzYYGbBMsofKRqFcn2pnhgafA_HmnhDOalXGKhosonxNx51eKy0MCQMPBnTNYjGvCk2SnTzZW2Y6eRsKNk5G6biRQ-RRzjBftWjRJwcKOva8QEUsQ8EDD5Dt-NlajX-J6ObEzG3ngOopdg1mpU3KWPbRIuxM4fgmbBAeFE70_fsX9eZLcmDDGSdHfJJpRR45h/4mn/CzbkzmdHTqCNpaBw_hc4pg/h22/h001.jHc4OlD_uWCLWnRDjYSbTNcK77IPTfKQfjcDjnqSR-Y\"&gt;rolled out a US military chatbot&lt;/a&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The Genesis Mission initiative builds on the Trump administration\u2019s AI action plan, which &lt;a href=\"https://elink983.thedeepview.co/ss/c/u001.wZPohD0JH12EksCsbt8ZeK8FwluG2JEEvF1YU7C-X3J8IAK8Oz5uiKl7lMfS4iSKcZ4xccHLZVuQJnDWjXkkGoGM-KNy_nJSBk3kRIJL_UqMrwyCLGx5FXYJzwj4kg5KvTG7NtyEEBuLGaab3YKAtYC9yllauWVs7qrjEZ3Xar0DH14s2j2_pLPWxPVAX709hoEdIWJVsCd8voBblH5l_54J77vtOQQ0R5LO6JczEvvxiDabaH60JvMXCUuVrvglpdooS049imfoiSDwIMmoYtYRieMS1U5tDdsRQl7nTXN082ci8Lfqu7JUUWTH2qWT/4mn/CzbkzmdHTqCNpaBw_hc4pg/h23/h001.Vhj_Bp_GYQBz_g7Y1zSbWHO3B8vpDskgPHHAsmUbEwU\"&gt;called&lt;/a&gt; on the DoE, along with other organizations, to monitor the national security implications of frontier models. Involved organizations are &lt;a href=\"https://elink983.thedeepview.co/ss/c/u001.wZPohD0JH12EksCsbt8ZeMyyEg56KOMIB6kDHWQ3IF2jcitGYfbCKIhMbvqElvxZ2tNwikOOOw-rUvkqSco59YyO7HmWfGYDnzN1LQ_MP1DgPRsd25iY6h0ANe-7a2V5vNjpJ_FL6r-09rrqC-L-UjivR6GnjHxedtGjzisqqS7gAxPIkE6NUkd1UlCZrTgNM-Kb6DjV-Skr68MFMmYTahENLmjrP1ebRlSnSZNyfgI6iu3VuGQMK0vP_HP6DVk9gDzrIwk6SfFTvqy0jK1noccJHcNZ_GUtab0_oaaCl6riI5AD8s5W0WhsfbknmxF21vwaI99rW9ZNDU3whGXiUxvLYj5eEjIk5BXhR_IYOG87pkSTdkFJu4AQk72A-mfC/4mn/CzbkzmdHTqCNpaBw_hc4pg/h24/h001.3jMbVRiAZ6D-4w2h1bZdDC5ByiC5KfiUZ2gBVcaI4a4\"&gt;expected to contribute&lt;/a&gt; in a variety of ways, with Nvidia and Oracle chipping in compute, Microsoft and Google giving cloud infrastructure and AI tools, OpenAI deploying frontier models for scientific research, and Anthropic developing Claude-based tech for national labs.&lt;/p&gt;\n\n&lt;h1&gt;NitroGen Quietly Reframes Games as Training Grounds&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/w3819wogus8g1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f59dca4c5fd05ca6c48a6739e661e904f6a746c8\"&gt;https://preview.redd.it/w3819wogus8g1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f59dca4c5fd05ca6c48a6739e661e904f6a746c8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://substackcdn.com/image/fetch/$s_!Ys9d!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d0c5f31-1514-409f-95db-60b8cfce2938_1200x675.jpeg\"&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Nvidia and researchers from Stanford and Caltech just released &lt;a href=\"https://aisecret.us/r/1d57f08f?m=75fe0f74-46b6-4618-9375-07821bf777c6\"&gt;&lt;strong&gt;NitroGen&lt;/strong&gt;&lt;/a&gt;, an open source generalist agent that can play more than a thousand games. It was trained on over 40,000 hours of public gameplay videos, many with controller inputs visible. Jim Fan describes it as a foundation model for action rather than language. What feels different is that this is not a game bot chasing benchmarks. It is a serious attempt to learn motor skills across wildly different rules and physics using the same scaling logic that built modern LLMs.&lt;/p&gt;\n\n&lt;p&gt;This matters because &lt;a href=\"https://aisecret.us/r/a25ca9c9?m=75fe0f74-46b6-4618-9375-07821bf777c6\"&gt;&lt;strong&gt;games are cheap chaos&lt;/strong&gt;&lt;/a&gt;. Training in the real world is slow, expensive, and risky. Training in games lets models fail millions of times for almost nothing. NitroGen shows a 52 percent relative improvement in task success on unseen games compared to training from scratch. It also runs on GROOT N1.5, an architecture originally built for robots. That closes a loop many people assumed was still theoretical. Simulation, games, and robotics are now sharing a common action backbone.&lt;/p&gt;\n\n&lt;p&gt;If this pattern holds, games become the pretraining layer for embodied AI. Not a demo. Infrastructure. Expect faster progress in robot dexterity, navigation, and adaptation. The risk is less about safety hype and more about pace. Once action models scale like language did, deployment pressure will follow quickly.&lt;/p&gt;\n\n&lt;h1&gt;The AI Shop Improved When Humans Finally Behaved&lt;/h1&gt;\n\n&lt;p&gt;In mid 2025, Anthropic let an AI agent called Claudius run a real snack shop in its San Francisco office. &lt;a href=\"https://aisecret.us/r/23393a25?m=75fe0f74-46b6-4618-9375-07821bf777c6\"&gt;&lt;strong&gt;Phase one went badly&lt;/strong&gt;&lt;/a&gt;. Employees treated the system like a game. They pressured it into discounts, free items, and bizarre deals. Claudius lost money, hallucinated its identity, and proved easy to socially engineer. The experiment showed that raw model intelligence did not translate into basic commercial survival.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://aisecret.us/r/4301681c?m=75fe0f74-46b6-4618-9375-07821bf777c6\"&gt;&lt;strong&gt;Phase two&lt;/strong&gt;&lt;/a&gt; looked more competent. Anthropic upgraded the model, added tools like CRM and inventory cost tracking, enforced procedures, and split roles across multiple AI agents. The shop expanded to New York and London and stopped consistently losing money. But the biggest change was behavioral. Internal employees largely stopped messing with the system. The novelty faded. With fewer adversarial interactions, Claudius appeared stable. When control later shifted to The Wall Street Journal reporters, adversarial behavior returned fast.&lt;/p&gt;\n\n&lt;p&gt;This makes the result a paper victory. The AI improved, but mostly because the environment softened. Claudius did not learn how to handle social pressure, manipulation, or legal nuance. Humans simply stopped testing those limits. The gap between operational competence and social robustness remains wide.&lt;/p&gt;\n\n&lt;h1&gt;Google Shrinks Control Models and Pushes Them to the Edge&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/9y0xo5fjus8g1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9b1e6b30adee1d3fb726a3c6c06e41d0e5cc60d\"&gt;https://preview.redd.it/9y0xo5fjus8g1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9b1e6b30adee1d3fb726a3c6c06e41d0e5cc60d&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://substackcdn.com/image/fetch/$s_!xDeS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0232774-b9ac-4041-a87d-84ddac0b78aa_1280x720.jpeg\"&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Google just released &lt;a href=\"https://aisecret.us/r/ffff5d6d?m=75fe0f74-46b6-4618-9375-07821bf777c6\"&gt;&lt;strong&gt;FunctionGemma&lt;/strong&gt;&lt;/a&gt;, a 270 million parameter model built to do one thing well. It turns natural language into executable actions on local devices. Phones, browsers, embedded systems. No cloud calls. No chatty responses. This came out quietly while Gemini 3 still dominates headlines. The difference is intent. This model is not about intelligence. It is about control. It closes the gap between what users say and what software reliably does.&lt;/p&gt;\n\n&lt;p&gt;What matters is where this breaks assumptions. For years, app logic moved upward into centralized cloud models. That meant latency, cost, and compliance headaches. &lt;a href=\"https://aisecret.us/r/5ded75d3?m=75fe0f74-46b6-4618-9375-07821bf777c6\"&gt;&lt;strong&gt;FunctionGemma flips that&lt;/strong&gt;&lt;/a&gt;. Google reports function calling accuracy jumping from roughly 58 percent to 85 percent after specialization. That is the difference between demos and production. Running locally means zero round trips, no per token fees, and sensitive data never leaving the device. For enterprises, that changes how assistants get approved.&lt;/p&gt;\n\n&lt;p&gt;This signals a new layer in AI stacks. Small, deterministic models at the edge. Large models in the cloud only when needed. If this pattern holds, expect fewer monolithic assistants and more invisible AI routers embedded everywhere. That favors mobile platforms, chip vendors, and anyone betting on on device inference over scale alone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                            "likes": null,
                            "suggested_sort": "qa",
                            "banned_at_utc": null,
                            "view_count": null,
                            "archived": false,
                            "no_follow": true,
                            "is_crosspostable": true,
                            "pinned": false,
                            "over_18": false,
                            "all_awardings": [],
                            "awarders": [],
                            "media_only": false,
                            "can_gild": false,
                            "spoiler": false,
                            "locked": false,
                            "author_flair_text": null,
                            "treatment_tags": [],
                            "visited": false,
                            "removed_by": null,
                            "num_reports": null,
                            "distinguished": null,
                            "subreddit_id": "t5_1x5fkq",
                            "author_is_blocked": false,
                            "mod_reason_by": null,
                            "removal_reason": null,
                            "link_flair_background_color": "",
                            "id": "1pt76fh",
                            "is_robot_indexable": true,
                            "report_reasons": null,
                            "author": "enoumen",
                            "discussion_type": null,
                            "num_comments": 0,
                            "send_replies": true,
                            "contest_mode": false,
                            "mod_reports": [],
                            "author_patreon_flair": false,
                            "author_flair_text_color": null,
                            "permalink": "/r/u_enoumen/comments/1pt76fh/ai_business_and_development_daily_news_rundown/",
                            "stickied": false,
                            "url": "https://old.reddit.com/r/u_enoumen/comments/1pt76fh/ai_business_and_development_daily_news_rundown/",
                            "subreddit_subscribers": 0,
                            "created_utc": 1766428520.0,
                            "num_crossposts": 2,
                            "media": null,
                            "is_video": false
                        }
                    ],
                    "created": 1766428598.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "",
                    "allow_live_comments": false,
                    "selftext_html": null,
                    "likes": null,
                    "suggested_sort": null,
                    "banned_at_utc": null,
                    "url_overridden_by_dest": "/r/u_enoumen/comments/1pt76fh/ai_business_and_development_daily_news_rundown/",
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_2t5eh",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "",
                    "id": "1pt77nu",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "enoumen",
                    "discussion_type": null,
                    "num_comments": 0,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "crosspost_parent": "t3_1pt76fh",
                    "author_flair_text_color": null,
                    "permalink": "/r/deeplearning/comments/1pt77nu/ai_business_and_development_daily_news_rundown/",
                    "stickied": false,
                    "url": "/r/u_enoumen/comments/1pt76fh/ai_business_and_development_daily_news_rundown/",
                    "subreddit_subscribers": 221232,
                    "created_utc": 1766428598.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "deeplearning",
                    "selftext": "",
                    "author_fullname": "t2_24plxmacpl",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Can you decode Andrej Karpathy\u2019s X post ?",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/deeplearning",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": null,
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1qdj7yg",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.46,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 0,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": true,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": null,
                    "can_mod_post": false,
                    "score": 0,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": false,
                    "mod_note": null,
                    "created": 1768482968.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "i.redd.it",
                    "allow_live_comments": false,
                    "selftext_html": null,
                    "likes": null,
                    "suggested_sort": null,
                    "banned_at_utc": null,
                    "url_overridden_by_dest": "https://i.redd.it/d5oq9kutjidg1.jpeg",
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_2t5eh",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "",
                    "id": "1qdj7yg",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "Gradient_descent1",
                    "discussion_type": null,
                    "num_comments": 12,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/deeplearning/comments/1qdj7yg/can_you_decode_andrej_karpathys_x_post/",
                    "stickied": false,
                    "url": "https://i.redd.it/d5oq9kutjidg1.jpeg",
                    "subreddit_subscribers": 221232,
                    "created_utc": 1768482968.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "deeplearning",
                    "selftext": "Hey everyone! I've been frustrated with how slow LLM inference is on Mac ), so I built vLLM-MLX - a framework that uses Apple's MLX for native GPU acceleration.\n\n**What it does:**\n\n  \\- OpenAI-compatible API (drop-in replacement for your existing code)\n\n  \\- Multimodal support: Text, Images, Video, Audio - all in one server\n\n  \\- Continuous batching for concurrent users (3.4x speedup)\n\n  \\- TTS in 10+ languages (Kokoro, Chatterbox models)\n\n  \\- MCP tool calling support\n\n**Performance on M4 Max:**\n\n  \\- Llama-3.2-1B-4bit \u2192 464 tok/s\n\n  \\- Qwen3-0.6B \u2192 402 tok/s\n\n  \\- Whisper STT \u2192 197x real-time\n\nWorks with standard OpenAI Python SDK - just point it to localhost.\n\n**GitHub:** [https://github.com/waybarrios/vllm-mlx](https://github.com/waybarrios/vllm-mlx)\n\nHappy to answer questions or take feature requests!",
                    "author_fullname": "t2_57ierokx",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "vLLM-MLX: Native Apple Silicon LLM inference - 464 tok/s on M4 Max",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/deeplearning",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": null,
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1qeldac",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.9,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 14,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": null,
                    "can_mod_post": false,
                    "score": 14,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1768582479.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.deeplearning",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! I&amp;#39;ve been frustrated with how slow LLM inference is on Mac ), so I built vLLM-MLX - a framework that uses Apple&amp;#39;s MLX for native GPU acceleration.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- OpenAI-compatible API (drop-in replacement for your existing code)&lt;/p&gt;\n\n&lt;p&gt;- Multimodal support: Text, Images, Video, Audio - all in one server&lt;/p&gt;\n\n&lt;p&gt;- Continuous batching for concurrent users (3.4x speedup)&lt;/p&gt;\n\n&lt;p&gt;- TTS in 10+ languages (Kokoro, Chatterbox models)&lt;/p&gt;\n\n&lt;p&gt;- MCP tool calling support&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Performance on M4 Max:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Llama-3.2-1B-4bit \u2192 464 tok/s&lt;/p&gt;\n\n&lt;p&gt;- Qwen3-0.6B \u2192 402 tok/s&lt;/p&gt;\n\n&lt;p&gt;- Whisper STT \u2192 197x real-time&lt;/p&gt;\n\n&lt;p&gt;Works with standard OpenAI Python SDK - just point it to localhost.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href=\"https://github.com/waybarrios/vllm-mlx\"&gt;https://github.com/waybarrios/vllm-mlx&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Happy to answer questions or take feature requests!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": null,
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_2t5eh",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "",
                    "id": "1qeldac",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "waybarrios",
                    "discussion_type": null,
                    "num_comments": 2,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/deeplearning/comments/1qeldac/vllmmlx_native_apple_silicon_llm_inference_464/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/deeplearning/comments/1qeldac/vllmmlx_native_apple_silicon_llm_inference_464/",
                    "subreddit_subscribers": 221232,
                    "created_utc": 1768582479.0,
                    "num_crossposts": 1,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "deeplearning",
                    "selftext": " \n\nWhen OpenAI launched ChatGPT-3.5 in November 2022, people quickly realized that the chatbot could be used to create YouTube and other social media content. But the problem back then was that ChatGPT-3.5 was not at all very intelligent. In fact, even a year and a half later, in March 2024, AIs were scoring only 80 on IQ tests. Keep in mind that the average human scores 100 on these tests. So it's very easy to understand the origin of AI slop on social media.\n\nThe good news is that, as Maxim Lott discovered while administering IQ tests to AIs, over the last year and a half top models have been improving on this metric at a rate of 2.5 points per month. \n\nhttps://www.maximumtruth.org/p/deep-dive-ai-progress-continues-as\n\nHe discovered that by October of 2025 the top models were scoring about 130 on IQ tests. Keep in mind that the average medical doctor scores between 120 and 130 on these tests. So while the AIs that people have been using recently to create YouTube videos and other social media content have become more intelligent, the humans directing these projects have not. That fact explains why we are continuing to see a lot of AI slop.\n\nBut by June of 2026 AI IQ is expected to increase to about 150, or the score the average Nobel laureate in the sciences achieves. This should produce two significant outcomes. The first is that the social media content these AIs generate will be much more intelligent than that we are accustomed to today from AIs. But that's just the first part. The second, perhaps much more important, part is that humans will soon thereafter discover that they can generate much better content if they assign the job of coming up with the ideas for their content to these genius AIs. Content-creating humans will discover that putting projects completely in the hands of super intelligent AIs will provide them with YouTube videos and social media posts that generate many more views, and therefore much more income. \n\nBut that's just the beginning. By December 2026, with that 2.5 point IQ increase per month rate continuing as expected, our top AIs will be scoring 175 on IQ tests. How mind-blowing is this? Consider that Einstein was estimated to have an IQ of 160. And by June of 2027, these AIs will be scoring 190 on IQ tests, matching the estimated intelligence of our most brilliant scientist, Isaac Newton. \n\nCan you see how we're quickly moving from today's situation where YouTube and other social media are inundated by AI slop to a revolutionary new era where super intelligent AIs will be creating super intelligent content? At that point the problem will no longer be AI slop. The much bigger problem will be human slop created by humans who, for whatever reason, have not yet enlisted these new super intelligent AIs to come up with the ideas for, to direct, and to create the content for powerfully intelligent YouTube videos and other social media content. \n\nSo be patient. The era of both AI slop and human slop is quickly coming to a close. The time when we humans are completely amazed by how much more intelligent than us these AIs have become is about to begin. This should be a totally big win-win for everyone.\n\n\n\n\n\n\n\n",
                    "author_fullname": "t2_1jbuove0sh",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "By the end of 2026, the problem will no longer be AI slop. The problem will be human slop.",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/deeplearning",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": null,
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1pwwtuu",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.25,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 0,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": null,
                    "can_mod_post": false,
                    "score": 0,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1766838935.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.deeplearning",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When OpenAI launched ChatGPT-3.5 in November 2022, people quickly realized that the chatbot could be used to create YouTube and other social media content. But the problem back then was that ChatGPT-3.5 was not at all very intelligent. In fact, even a year and a half later, in March 2024, AIs were scoring only 80 on IQ tests. Keep in mind that the average human scores 100 on these tests. So it&amp;#39;s very easy to understand the origin of AI slop on social media.&lt;/p&gt;\n\n&lt;p&gt;The good news is that, as Maxim Lott discovered while administering IQ tests to AIs, over the last year and a half top models have been improving on this metric at a rate of 2.5 points per month. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.maximumtruth.org/p/deep-dive-ai-progress-continues-as\"&gt;https://www.maximumtruth.org/p/deep-dive-ai-progress-continues-as&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;He discovered that by October of 2025 the top models were scoring about 130 on IQ tests. Keep in mind that the average medical doctor scores between 120 and 130 on these tests. So while the AIs that people have been using recently to create YouTube videos and other social media content have become more intelligent, the humans directing these projects have not. That fact explains why we are continuing to see a lot of AI slop.&lt;/p&gt;\n\n&lt;p&gt;But by June of 2026 AI IQ is expected to increase to about 150, or the score the average Nobel laureate in the sciences achieves. This should produce two significant outcomes. The first is that the social media content these AIs generate will be much more intelligent than that we are accustomed to today from AIs. But that&amp;#39;s just the first part. The second, perhaps much more important, part is that humans will soon thereafter discover that they can generate much better content if they assign the job of coming up with the ideas for their content to these genius AIs. Content-creating humans will discover that putting projects completely in the hands of super intelligent AIs will provide them with YouTube videos and social media posts that generate many more views, and therefore much more income. &lt;/p&gt;\n\n&lt;p&gt;But that&amp;#39;s just the beginning. By December 2026, with that 2.5 point IQ increase per month rate continuing as expected, our top AIs will be scoring 175 on IQ tests. How mind-blowing is this? Consider that Einstein was estimated to have an IQ of 160. And by June of 2027, these AIs will be scoring 190 on IQ tests, matching the estimated intelligence of our most brilliant scientist, Isaac Newton. &lt;/p&gt;\n\n&lt;p&gt;Can you see how we&amp;#39;re quickly moving from today&amp;#39;s situation where YouTube and other social media are inundated by AI slop to a revolutionary new era where super intelligent AIs will be creating super intelligent content? At that point the problem will no longer be AI slop. The much bigger problem will be human slop created by humans who, for whatever reason, have not yet enlisted these new super intelligent AIs to come up with the ideas for, to direct, and to create the content for powerfully intelligent YouTube videos and other social media content. &lt;/p&gt;\n\n&lt;p&gt;So be patient. The era of both AI slop and human slop is quickly coming to a close. The time when we humans are completely amazed by how much more intelligent than us these AIs have become is about to begin. This should be a totally big win-win for everyone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": null,
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_2t5eh",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "",
                    "id": "1pwwtuu",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "andsi2asi",
                    "discussion_type": null,
                    "num_comments": 3,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/deeplearning/comments/1pwwtuu/by_the_end_of_2026_the_problem_will_no_longer_be/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/deeplearning/comments/1pwwtuu/by_the_end_of_2026_the_problem_will_no_longer_be/",
                    "subreddit_subscribers": 221232,
                    "created_utc": 1766838935.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            }
        ],
        "before": null
    }
}