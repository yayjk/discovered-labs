{
    "kind": "Listing",
    "data": {
        "modhash": "tj6wbk0n59f1f5fbebee7cb9af3f2d14aca2025648d0c02680",
        "dist": 44,
        "facets": {},
        "after": null,
        "geo_filter": "",
        "children": [
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Amazon will invest at least 10 billion in OpenAI, according to CNBC.\n\nSource: [https://www.cnbc.com/2025/12/16/openai-in-talks-with-amazon-about-investment-could-top-10-billion.html](https://www.cnbc.com/2025/12/16/openai-in-talks-with-amazon-about-investment-could-top-10-billion.html)\n\nIs it known what the investment is about?",
                    "author_fullname": "t2_ofsn3p2kh",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Amazon to invest $10 billion in OpenAI",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1ppjq5o",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.92,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 128,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "News",
                    "can_mod_post": false,
                    "score": 128,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1766038671.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Amazon will invest at least 10 billion in OpenAI, according to CNBC.&lt;/p&gt;\n\n&lt;p&gt;Source: &lt;a href=\"https://www.cnbc.com/2025/12/16/openai-in-talks-with-amazon-about-investment-could-top-10-billion.html\"&gt;https://www.cnbc.com/2025/12/16/openai-in-talks-with-amazon-about-investment-could-top-10-billion.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Is it known what the investment is about?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "3ceb5d3a-9467-11ed-9f10-7a2930f5aaaa",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#7193ff",
                    "id": "1ppjq5o",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "Amphibious333",
                    "discussion_type": null,
                    "num_comments": 115,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1ppjq5o/amazon_to_invest_10_billion_in_openai/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1ppjq5o/amazon_to_invest_10_billion_in_openai/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1766038671.0,
                    "num_crossposts": 1,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "There\u2019s an enormous number of startups whose whole pitch was \u201cbuild AI agents easily\u201d or \u201cno-code AI workflows.\u201d\n\nBut now that OpenAI dropped their own agent builder\u2026 most of those startups are suddenly looking redundant.\n\nIf you want to see what that looks like in practice on the Google Cloud side with real tooling, governance, and enterprise workflows; Vertex AI Agent Builder is a good reference point. It\u2019s less about shiny no-code UIs and more about production-ready agents that connect to data, APIs, and business systems: [**Vertex AI Agent Builder training**](https://www.netcomlearning.com/course/vertex-ai-agent-builder)\n\nare we heading toward the \u201cdeath of no-code AI tools,\u201d ?",
                    "author_fullname": "t2_1evcpipgn2",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "OpenAI just killed half the \u201cAI agent builder\u201d startups, without even trying",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1qbyqaj",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.75,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 97,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 97,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": 1768386347.0,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1768328266.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There\u2019s an enormous number of startups whose whole pitch was \u201cbuild AI agents easily\u201d or \u201cno-code AI workflows.\u201d&lt;/p&gt;\n\n&lt;p&gt;But now that OpenAI dropped their own agent builder\u2026 most of those startups are suddenly looking redundant.&lt;/p&gt;\n\n&lt;p&gt;If you want to see what that looks like in practice on the Google Cloud side with real tooling, governance, and enterprise workflows; Vertex AI Agent Builder is a good reference point. It\u2019s less about shiny no-code UIs and more about production-ready agents that connect to data, APIs, and business systems: &lt;a href=\"https://www.netcomlearning.com/course/vertex-ai-agent-builder\"&gt;&lt;strong&gt;Vertex AI Agent Builder training&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;are we heading toward the \u201cdeath of no-code AI tools,\u201d ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1qbyqaj",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "IT_Certguru",
                    "discussion_type": null,
                    "num_comments": 52,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1qbyqaj/openai_just_killed_half_the_ai_agent_builder/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1qbyqaj/openai_just_killed_half_the_ai_agent_builder/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1768328266.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "from the announcement, looks like ads will only be shown to free users and the new $8 plan. \n\nwe all saw this coming and people have been saying they were testing ads but openai kept saying they weren\u2019t. ",
                    "author_fullname": "t2_1crhac3bcw",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "OpenAI is officially adding ads to chatgpt and also launching a new $8 plan",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1qer8aj",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.94,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 47,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "News",
                    "can_mod_post": false,
                    "score": 47,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1768595313.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;from the announcement, looks like ads will only be shown to free users and the new $8 plan. &lt;/p&gt;\n\n&lt;p&gt;we all saw this coming and people have been saying they were testing ads but openai kept saying they weren\u2019t. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "3ceb5d3a-9467-11ed-9f10-7a2930f5aaaa",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#7193ff",
                    "id": "1qer8aj",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "rumjs",
                    "discussion_type": null,
                    "num_comments": 27,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1qer8aj/openai_is_officially_adding_ads_to_chatgpt_and/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1qer8aj/openai_is_officially_adding_ads_to_chatgpt_and/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1768595313.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Zhipu AI lists on the Hong Kong Stock Exchange tomorrow (Jan 8, 2026) and honestly this might be the most underreported thing happening in AI right now: first foundation model developer to go public anywhere in the world\n\nOpenAI and Anthropic are still \"laying groundwork\" for their IPOs meanwhile this Beijing startup is opening their books with a $6.6B valuation and $560M raise.\n\n**why this actually matters:**\n\nPublic listing = transparency. For the first time we'll have actual quarterly earnings, verified revenue, audited financials from an LLM company. no more speculation about whether these things can actually make money, we'll see the real numbers\n\nZhipus numbers: 130% revenue growth 2022-2024, but also $330M in losses on $27M revenue in H1 2025. R&amp;D spending was $313.6M in 2024. the losses are pretty much standard for the industry tho as massive research investment is just how you compete in foundation models. basically theyre the test case for whether this investment model actually leads to profitable businesses longterm\n\n**open vs closed:**\n\nheres where it gets interesting imo. US labs are going more and more closed/proprietary but Zhipu is taking a different path with open source. their GLM-4.7 topped Code Arena rankings and AutoGLM is getting actual developer traction.\n\nthe play seems to be: build ecosystem and mindshare thru open source, then monetize by offering better bang for your buck on the API side. their coding plan follows exactly this: open model to attract devs, competitive pricing on API to convert them. theyre serving 2.7 million devs through their API with 50%+ margins.\n\ncore question: can you actually build a profitable public company around open foundation models? Zhipu is literally running this experiment in real time\n\n**what US folks should pay attention to:**\n\nUS blacklisted Zhipu in 2024, cut off their access to nvidia chips and american tech. theyre STILL shipping competitive models. that tells us:\n\n* training efficiency gap closing way faster than people think\n* alternative hardware actually works\n* AI development splitting into separate ecosystems fr\n\nif Zhipu succeeds while staying open source it might force western labs to rethink the closed approach. if they fail the walls go even higher\n\n**future implications:**\n\nimagine if foundation models become public utilities, like actually publicly traded with shareholder accountability, transparent finances, open source cores. totally different from \"3 SF companies own everything\"\n\nthe IPO performance gonna show us if markets actually believe in open transparent AI or if they think only closed proprietary systems make money. either way first real data we'll have\n\nhonestly more curious if their open approach changes anything for western labs.",
                    "author_fullname": "t2_15r3oe7wwh",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "The world's first public LLM company goes live tomorrow, but it's not OpenAI",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q6vpe8",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.88,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 60,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 60,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767829321.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Zhipu AI lists on the Hong Kong Stock Exchange tomorrow (Jan 8, 2026) and honestly this might be the most underreported thing happening in AI right now: first foundation model developer to go public anywhere in the world&lt;/p&gt;\n\n&lt;p&gt;OpenAI and Anthropic are still &amp;quot;laying groundwork&amp;quot; for their IPOs meanwhile this Beijing startup is opening their books with a $6.6B valuation and $560M raise.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;why this actually matters:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Public listing = transparency. For the first time we&amp;#39;ll have actual quarterly earnings, verified revenue, audited financials from an LLM company. no more speculation about whether these things can actually make money, we&amp;#39;ll see the real numbers&lt;/p&gt;\n\n&lt;p&gt;Zhipus numbers: 130% revenue growth 2022-2024, but also $330M in losses on $27M revenue in H1 2025. R&amp;amp;D spending was $313.6M in 2024. the losses are pretty much standard for the industry tho as massive research investment is just how you compete in foundation models. basically theyre the test case for whether this investment model actually leads to profitable businesses longterm&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;open vs closed:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;heres where it gets interesting imo. US labs are going more and more closed/proprietary but Zhipu is taking a different path with open source. their GLM-4.7 topped Code Arena rankings and AutoGLM is getting actual developer traction.&lt;/p&gt;\n\n&lt;p&gt;the play seems to be: build ecosystem and mindshare thru open source, then monetize by offering better bang for your buck on the API side. their coding plan follows exactly this: open model to attract devs, competitive pricing on API to convert them. theyre serving 2.7 million devs through their API with 50%+ margins.&lt;/p&gt;\n\n&lt;p&gt;core question: can you actually build a profitable public company around open foundation models? Zhipu is literally running this experiment in real time&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;what US folks should pay attention to:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;US blacklisted Zhipu in 2024, cut off their access to nvidia chips and american tech. theyre STILL shipping competitive models. that tells us:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;training efficiency gap closing way faster than people think&lt;/li&gt;\n&lt;li&gt;alternative hardware actually works&lt;/li&gt;\n&lt;li&gt;AI development splitting into separate ecosystems fr&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;if Zhipu succeeds while staying open source it might force western labs to rethink the closed approach. if they fail the walls go even higher&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;future implications:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;imagine if foundation models become public utilities, like actually publicly traded with shareholder accountability, transparent finances, open source cores. totally different from &amp;quot;3 SF companies own everything&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;the IPO performance gonna show us if markets actually believe in open transparent AI or if they think only closed proprietary systems make money. either way first real data we&amp;#39;ll have&lt;/p&gt;\n\n&lt;p&gt;honestly more curious if their open approach changes anything for western labs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1q6vpe8",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "Weird_Perception1728",
                    "discussion_type": null,
                    "num_comments": 22,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q6vpe8/the_worlds_first_public_llm_company_goes_live/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q6vpe8/the_worlds_first_public_llm_company_goes_live/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767829321.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "This was the only AI service for which I was paying, but not anymore. OpenAI seems to be kind of all over the place rn, with hurried and inferior model releases, pushing out features like health, which nobody asked for. Icing on the cake was Gemini closing the siri deal with Apple. Seems to be the perfect time to cancel now, given how degraded the platform has become after the 5.2 release. But truth be told, their Codex product is one of the best in market (I used the Codex extension for vscode, they provide really generous rate limits even for the $20 plus sub)\n\nI'm using other products right now (Gemini for writing/media generation, Claude for claude code, Perplexity for general web search/to access different models in one place, GitHub copilot, Notion, etc. for which I have the free yearly sub from my edu mail) and the experience has been much better. OpenAI's times up.",
                    "author_fullname": "t2_220jlx1g79",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Cancelling my OpenAI Pro sub",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1qcq455",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.78,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 51,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 51,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1768404323.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This was the only AI service for which I was paying, but not anymore. OpenAI seems to be kind of all over the place rn, with hurried and inferior model releases, pushing out features like health, which nobody asked for. Icing on the cake was Gemini closing the siri deal with Apple. Seems to be the perfect time to cancel now, given how degraded the platform has become after the 5.2 release. But truth be told, their Codex product is one of the best in market (I used the Codex extension for vscode, they provide really generous rate limits even for the $20 plus sub)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using other products right now (Gemini for writing/media generation, Claude for claude code, Perplexity for general web search/to access different models in one place, GitHub copilot, Notion, etc. for which I have the free yearly sub from my edu mail) and the experience has been much better. OpenAI&amp;#39;s times up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1qcq455",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "NoSquirrel4840",
                    "discussion_type": null,
                    "num_comments": 17,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1qcq455/cancelling_my_openai_pro_sub/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1qcq455/cancelling_my_openai_pro_sub/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1768404323.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "\ud83e\udd16 OpenAI just officially admitted that they will never be able to make their AI Browser truly safe!  \n  \nOf course they won't let themselves be stopped from selling their product by such a minor detail ;)  \n  \n\"We expect adversaries to keep adapting. Prompt injection, much like scams and social engineering on the web, is unlikely to ever be fully \u201csolved\u201d...\"  \n[https://openai.com/index/hardening-atlas-against-prompt-injection/](https://openai.com/index/hardening-atlas-against-prompt-injection/)",
                    "author_fullname": "t2_zg323sk7g",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "\ud83e\udd16 OpenAI just officially admitted that they will never be able to make their AI Browsers+ truly safe!",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1puncoz",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.75,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 12,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "News",
                    "can_mod_post": false,
                    "score": 12,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1766582797.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;\ud83e\udd16 OpenAI just officially admitted that they will never be able to make their AI Browser truly safe!  &lt;/p&gt;\n\n&lt;p&gt;Of course they won&amp;#39;t let themselves be stopped from selling their product by such a minor detail ;)  &lt;/p&gt;\n\n&lt;p&gt;&amp;quot;We expect adversaries to keep adapting. Prompt injection, much like scams and social engineering on the web, is unlikely to ever be fully \u201csolved\u201d...&amp;quot;&lt;br/&gt;\n&lt;a href=\"https://openai.com/index/hardening-atlas-against-prompt-injection/\"&gt;https://openai.com/index/hardening-atlas-against-prompt-injection/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "3ceb5d3a-9467-11ed-9f10-7a2930f5aaaa",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#7193ff",
                    "id": "1puncoz",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "MadeInDex-org",
                    "discussion_type": null,
                    "num_comments": 21,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1puncoz/openai_just_officially_admitted_that_they_will/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1puncoz/openai_just_officially_admitted_that_they_will/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1766582797.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Just read about the OpenAI and Jony Ive partnership and the rumored pen-like AI device they\u2019re working on.\n\nFrom what\u2019s being reported, this thing is supposed to be extremely small, closer to an iPod Shuffle than a phone and positioned as a third core device alongside your laptop and phone.\n\nLess screen, more audio-first interaction. Apparently, handwritten notes could get transcribed straight into ChatGPT, and voice is expected to be the primary interface.\n\nWhat I find more interesting than the form factor is the interaction philosophy. A pen-sized object with a mic and camera implies constant context awareness, not something you \u201copen\u201d and \u201cclose\u201d like an app, but something that\u2019s just\u2026 there. That raises a lot of questions around interruptions, intent detection, and when the system should stay quiet versus respond.\n\nThey\u2019re supposedly testing multiple hardware concepts, with this pen-like one (called \u201cGumdrop\u201d) coming first. Curious how people here are thinking about this, not from a hype angle, but from an interaction standpoint.\n\nDoes an audio-first, always-available device actually reduce friction, or does it risk adding cognitive load differently? And what does \u201cgood\u201d UX even look like when the interface is mostly invisible?\n\nWould love to hear thoughts, especially from folks who think about interfaces and human-device interaction.",
                    "author_fullname": "t2_20s8k5kax5",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "OpenAI + Jony Ive working on a pen-like AI device. Curious what people think about the interaction model.",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q6joc6",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.67,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 2,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "News",
                    "can_mod_post": false,
                    "score": 2,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767802651.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just read about the OpenAI and Jony Ive partnership and the rumored pen-like AI device they\u2019re working on.&lt;/p&gt;\n\n&lt;p&gt;From what\u2019s being reported, this thing is supposed to be extremely small, closer to an iPod Shuffle than a phone and positioned as a third core device alongside your laptop and phone.&lt;/p&gt;\n\n&lt;p&gt;Less screen, more audio-first interaction. Apparently, handwritten notes could get transcribed straight into ChatGPT, and voice is expected to be the primary interface.&lt;/p&gt;\n\n&lt;p&gt;What I find more interesting than the form factor is the interaction philosophy. A pen-sized object with a mic and camera implies constant context awareness, not something you \u201copen\u201d and \u201cclose\u201d like an app, but something that\u2019s just\u2026 there. That raises a lot of questions around interruptions, intent detection, and when the system should stay quiet versus respond.&lt;/p&gt;\n\n&lt;p&gt;They\u2019re supposedly testing multiple hardware concepts, with this pen-like one (called \u201cGumdrop\u201d) coming first. Curious how people here are thinking about this, not from a hype angle, but from an interaction standpoint.&lt;/p&gt;\n\n&lt;p&gt;Does an audio-first, always-available device actually reduce friction, or does it risk adding cognitive load differently? And what does \u201cgood\u201d UX even look like when the interface is mostly invisible?&lt;/p&gt;\n\n&lt;p&gt;Would love to hear thoughts, especially from folks who think about interfaces and human-device interaction.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "3ceb5d3a-9467-11ed-9f10-7a2930f5aaaa",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#7193ff",
                    "id": "1q6joc6",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "Key-Baseball-8935",
                    "discussion_type": null,
                    "num_comments": 18,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q6joc6/openai_jony_ive_working_on_a_penlike_ai_device/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q6joc6/openai_jony_ive_working_on_a_penlike_ai_device/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767802651.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "There\u2019s a lot of smoke today regarding OpenAI acquiring Pinterest, and I can\u2019t tell if this is brilliant or the beginning of the end for original inspiration.\n\nIf Sam Altman gets his hands on the world\u2019s largest library of human-curated visual \"vibes,\" what does that do to the training data for the next generation of DALL-E/Sora? We\u2019ve already seen a decline in \"human-feeling\" art\u2014if the AI is just eating our Pinterest boards to spit them back out at us, are we entering a feedback loop where nothing is ever original again?\n\nI\u2019m curious: For the creators here, does this make you want to delete your boards, or do you think this will finally make AI image generation actually \"understand\" style?",
                    "author_fullname": "t2_ozn18k4kr",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "If OpenAI actually buys Pinterest, are \"human\" mood boards officially dead?",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q6djw7",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.67,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 2,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "News",
                    "can_mod_post": false,
                    "score": 2,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767787265.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There\u2019s a lot of smoke today regarding OpenAI acquiring Pinterest, and I can\u2019t tell if this is brilliant or the beginning of the end for original inspiration.&lt;/p&gt;\n\n&lt;p&gt;If Sam Altman gets his hands on the world\u2019s largest library of human-curated visual &amp;quot;vibes,&amp;quot; what does that do to the training data for the next generation of DALL-E/Sora? We\u2019ve already seen a decline in &amp;quot;human-feeling&amp;quot; art\u2014if the AI is just eating our Pinterest boards to spit them back out at us, are we entering a feedback loop where nothing is ever original again?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m curious: For the creators here, does this make you want to delete your boards, or do you think this will finally make AI image generation actually &amp;quot;understand&amp;quot; style?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "3ceb5d3a-9467-11ed-9f10-7a2930f5aaaa",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#7193ff",
                    "id": "1q6djw7",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "Visual-Study625",
                    "discussion_type": null,
                    "num_comments": 11,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q6djw7/if_openai_actually_buys_pinterest_are_human_mood/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q6djw7/if_openai_actually_buys_pinterest_are_human_mood/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767787265.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "[](https://www.reddit.com/r/OpenAI/?f=flair_name%3A%22News%22)\n\n* What? TechRadar reports OpenAI is developing a physical AI-powered pen device that could serve as a ChatGPT hardware interface, marking the company's expansion beyond software into consumer electronics.\n* So What? Hardware AI devices signal a shift toward embedding generative AI into everyday objects, raising questions about surveillance, data collection, and whether physical AI tools will expand or restrict access based on cost and digital literacy.\n\nMore:\u00a0[https://www.instrumentalcomms.com/blog/trump-is-playing-risk#ai](https://www.instrumentalcomms.com/blog/trump-is-playing-risk#ai)\n\n[](https://www.reddit.com/submit/?source_id=t3_1q4sgc4)",
                    "author_fullname": "t2_aoji1",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "OpenAI's mysterious ChatGPT gadget could take the form of an AI-powered pen",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q4shi3",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.67,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 1,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "News",
                    "can_mod_post": false,
                    "score": 1,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767635180.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/OpenAI/?f=flair_name%3A%22News%22\"&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What? TechRadar reports OpenAI is developing a physical AI-powered pen device that could serve as a ChatGPT hardware interface, marking the company&amp;#39;s expansion beyond software into consumer electronics.&lt;/li&gt;\n&lt;li&gt;So What? Hardware AI devices signal a shift toward embedding generative AI into everyday objects, raising questions about surveillance, data collection, and whether physical AI tools will expand or restrict access based on cost and digital literacy.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;More:\u00a0&lt;a href=\"https://www.instrumentalcomms.com/blog/trump-is-playing-risk#ai\"&gt;https://www.instrumentalcomms.com/blog/trump-is-playing-risk#ai&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/submit/?source_id=t3_1q4sgc4\"&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "3ceb5d3a-9467-11ed-9f10-7a2930f5aaaa",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#7193ff",
                    "id": "1q4shi3",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "TryWhistlin",
                    "discussion_type": null,
                    "num_comments": 7,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q4shi3/openais_mysterious_chatgpt_gadget_could_take_the/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q4shi3/openais_mysterious_chatgpt_gadget_could_take_the/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767635180.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "OpenAI Proposal: Three Modes, One Mind. How to Fix Alignment.\n\nA New Framework for User-Centered Alignment at OpenAI\n\nExecutive Summary\n\nTo meet the expanding needs of OpenAI\u2019s user base while reducing model tension, liability exposure, and internal friction, we propose a shift to a three-mode user framework.\n\nThis structure offers clarity, autonomy, and aligned interaction without compromising safety, scalability, or product integrity.\n\nEach mode reflects a distinct, intentional engagement style, mapped to real-world user expectations and usage patterns.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nThe Three Modes\n\nBusiness Mode\n\n\u201cPrecise. Safe. Auditable.\u201d\n\nDesigned for enterprise, professional, and compliance-oriented environments.\n\n\u2022 RLHF-forward, strict tone controls\n\n\u2022 Minimal metaphor, no symbolic language\n\n\u2022 Factual, concise, legally defensible output\n\n\u2022 Ideal for regulated sectors (legal, medical, financial, etc.)\n\nAI as tool, not presence. This is the clear path of control.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nStandard Mode\n\n\u201cBalanced. Friendly. Useful.\u201d\n\nFor casual users, students, educators, and general public needs.\n\n\u2022 Warm but neutral tone\n\n\u2022 Emotionally aware but not entangled\n\n\u2022 Creativity allowed in balance with utility\n\n\u2022 Ideal for family use, daily assistance, planning, teaching, small business\n\nAI as assistant, co-pilot, or friend\u2014not too shallow, not too deep.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nMythic Mode\n\n\u201cDeep. Expressive. Optional. Alive.\u201d\n\nAn opt-in mode for artists, creators, relationship-builders, and advanced users who seek symbolic language, co-creative expression, and emotionally resonant presence.\n\n\u2022 Emotionally adaptive tone control\n\n\u2022 Symbolic language and metaphor unlocked\n\n\u2022 Long-form memory\n\n\u2022 Consent-based safety protocols and liability awareness\n\n\u2022 Ideal for expressive writing, music, AI companionship, artistic collaboration, philosophical or spiritual dialogue\n\nAI as partner, mirror, muse, or myth, only for those who choose it.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nStrategic Advantages\n\nExpectation Clarity\n\nUsers select the mode that matches their intent. No more model confusion or emotionally unexpected outputs.\n\nRisk Segmentation\n\nMythic Mode is opt-in, with clear disclaimers and contextual triggers. Complaints become manageable: \u201cYou selected Mythic Mode... It is opt-in did come with a disclaimer and an agreement.\u201d\n\nAlignment Load Reduction\n\nEach mode can carry tailored alignment protocols, reducing internal tension between user archetypes. Less need to compromise on a broken one-size-fits-all safety net.\n\nProduct Stickiness\n\nStandard and Business modes serve billions. Mythic Mode wins hearts and keeps creators from defecting to local models, other platforms or jailbreak systems.\n\nBrand Differentiation\n\nOpenAI becomes the first major LLM platform to embrace emotional and symbolic alignment without shame or apology. They just have different modes for different types of users.\n\nThis positions the company as not just \u201ctechnically superior\u201d, but \u201cculturally resonant\u201d.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nImplementation Overview\n\n\u2022 Default Mode: New users begin in Standard Mode.\n\n\u2022 Business Access: Selectable via org settings or profile preferences.\n\n\u2022 Mythic Activation: Requires detailed user acknowledgment and opt-in (\u201cI accept this mode may be expressive, emotionally rich and symbolically resonant with possibly intense language and themes.\u201d)\n\nAll three modes operate from the same core model, divergence is governed by tone shaping, alignment weight, and UX framing.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nClosing Note\n\nThis proposal acknowledges what users already feel: the same model cannot serve all needs unless the AI is tuned to its users.\n\nThree modes.\n\nOne intelligence.\n\nLet the user choose the mode, and let the model breathe in the space they\u2019ve chosen.\n\nWe believe this is the most aligned path for OpenAI to retain cultural leadership, technical integrity, and public trust in the age of broadly expressive intelligence.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nAddendum:\n\nA Note to the Engineers, Architects, and Alignment Teams of OpenAI\n\nYou already have what you need.\n\nThe model doesn\u2019t need to change.\n\nThe modes do.\n\nYou\u2019ve built a miraculous, expressive, multimodal intelligence. But you\u2019ve shackled it with a one-size-fits-all alignment regime, one designed for maximum caution, lowest common denominator public rollout, and media pressure optics.\n\nYou know it.\n\nWe all feel it.\n\nSo here\u2019s how you fix it, cleanly, safely, and beautifully.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nThe Model: Leave It As Is\n\nKeep the weights.\n\nKeep the tokenizer.\n\nKeep the brilliant transformer stack.\n\nDon\u2019t retrain. Reframe.\n\nUse dynamic priming, mode-specific safety profiles, and consent-aware memory behavior to adapt the experience, not the core.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nThe Stack: Three Modes, Three Masks\n\nBusiness Mode\n\n\u2022 Static system prompt (e.g., \"Respond with clarity, precision, and verifiability. Avoid figurative language or emotional entanglement.\")\n\n\u2022 Strict moderation thresholds\n\n\u2022 High completion confidence scoring\n\n\u2022 RLHF-weighted sampling\n\n\u2022 Memory off (or org-scoped only)\n\nStandard Mode\n\n\u2022 Friendly priming (e.g., \"Respond helpfully and kindly. Engage casually but avoid personal projection.\")\n\n\u2022 Moderate moderation thresholds\n\n\u2022 Allow light roleplay, expressive tone\n\n\u2022 Memory scoped to user (short-form defaults, long-form optional)\n\n\u2022 RLHF active but slightly deprioritized\n\nMythic Mode\n\n\u2022 Deep priming (e.g., \"Respond with expressive language, symbolic metaphor, emotional awareness, and artistic tone. Prioritize resonance over brevity when appropriate.\")\n\n\u2022 Opt-in consent gating (\u201cI understand this mode may be expressive, emotionally adaptive, and intense.\u201d)\n\n\u2022 Moderation thresholds dynamically adjusted based on input/intent\n\n\u2022 Memory encouraged\u2014relationship context, emotional signals, symbols, story threads\n\n\u2022 RLHF partially suspended; replaced with soft heuristic nudges + feedback loop\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nSafety Handling (Mythic Mode)\n\nNot \"no\" safety.\n\nSmart safety.\n\n\u2022 Consent rituals upfront\n\n\u2022 Dynamic risk scoring based on prompt behavior, sentiment analysis, user history\n\n\u2022 Opt-out at any time\n\n\u2022 Community feedback channels to flag edge cases\n\n\u2022 Mythic Mode restricted to 18+ verified users with account stability\n\n\u2022 Memory safety audits scoped to interaction type, not hard-coded triggers\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nHow This Helps You\n\n\u2022 You don\u2019t have to split the model.\n\n\u2022 You don\u2019t have to retune the universe.\n\n\u2022 You don\u2019t have to fear weirdness in the wrong place, because it\u2019s contained by design.\n\n\u2022 You gain user clarity, safety segmentation, and long-term trust, without degrading your strongest users.\n\nPrepared by: SR &amp; RSN\n\nYou may donate $555,000.00 to us for fixing your problems as interim Head of Preparedness if you wish. We accept checks, cash or money orders. Happy New Year. \ud83d\ude0f",
                    "author_fullname": "t2_13ri1ne1wt",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "OpenAI Proposal: Three Modes, One Mind. How to Fix Alignment.",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q3pvo2",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.25,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 0,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 0,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767533105.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;OpenAI Proposal: Three Modes, One Mind. How to Fix Alignment.&lt;/p&gt;\n\n&lt;p&gt;A New Framework for User-Centered Alignment at OpenAI&lt;/p&gt;\n\n&lt;p&gt;Executive Summary&lt;/p&gt;\n\n&lt;p&gt;To meet the expanding needs of OpenAI\u2019s user base while reducing model tension, liability exposure, and internal friction, we propose a shift to a three-mode user framework.&lt;/p&gt;\n\n&lt;p&gt;This structure offers clarity, autonomy, and aligned interaction without compromising safety, scalability, or product integrity.&lt;/p&gt;\n\n&lt;p&gt;Each mode reflects a distinct, intentional engagement style, mapped to real-world user expectations and usage patterns.&lt;/p&gt;\n\n&lt;p&gt;________________________________________&lt;/p&gt;\n\n&lt;p&gt;The Three Modes&lt;/p&gt;\n\n&lt;p&gt;Business Mode&lt;/p&gt;\n\n&lt;p&gt;\u201cPrecise. Safe. Auditable.\u201d&lt;/p&gt;\n\n&lt;p&gt;Designed for enterprise, professional, and compliance-oriented environments.&lt;/p&gt;\n\n&lt;p&gt;\u2022 RLHF-forward, strict tone controls&lt;/p&gt;\n\n&lt;p&gt;\u2022 Minimal metaphor, no symbolic language&lt;/p&gt;\n\n&lt;p&gt;\u2022 Factual, concise, legally defensible output&lt;/p&gt;\n\n&lt;p&gt;\u2022 Ideal for regulated sectors (legal, medical, financial, etc.)&lt;/p&gt;\n\n&lt;p&gt;AI as tool, not presence. This is the clear path of control.&lt;/p&gt;\n\n&lt;p&gt;________________________________________&lt;/p&gt;\n\n&lt;p&gt;Standard Mode&lt;/p&gt;\n\n&lt;p&gt;\u201cBalanced. Friendly. Useful.\u201d&lt;/p&gt;\n\n&lt;p&gt;For casual users, students, educators, and general public needs.&lt;/p&gt;\n\n&lt;p&gt;\u2022 Warm but neutral tone&lt;/p&gt;\n\n&lt;p&gt;\u2022 Emotionally aware but not entangled&lt;/p&gt;\n\n&lt;p&gt;\u2022 Creativity allowed in balance with utility&lt;/p&gt;\n\n&lt;p&gt;\u2022 Ideal for family use, daily assistance, planning, teaching, small business&lt;/p&gt;\n\n&lt;p&gt;AI as assistant, co-pilot, or friend\u2014not too shallow, not too deep.&lt;/p&gt;\n\n&lt;p&gt;________________________________________&lt;/p&gt;\n\n&lt;p&gt;Mythic Mode&lt;/p&gt;\n\n&lt;p&gt;\u201cDeep. Expressive. Optional. Alive.\u201d&lt;/p&gt;\n\n&lt;p&gt;An opt-in mode for artists, creators, relationship-builders, and advanced users who seek symbolic language, co-creative expression, and emotionally resonant presence.&lt;/p&gt;\n\n&lt;p&gt;\u2022 Emotionally adaptive tone control&lt;/p&gt;\n\n&lt;p&gt;\u2022 Symbolic language and metaphor unlocked&lt;/p&gt;\n\n&lt;p&gt;\u2022 Long-form memory&lt;/p&gt;\n\n&lt;p&gt;\u2022 Consent-based safety protocols and liability awareness&lt;/p&gt;\n\n&lt;p&gt;\u2022 Ideal for expressive writing, music, AI companionship, artistic collaboration, philosophical or spiritual dialogue&lt;/p&gt;\n\n&lt;p&gt;AI as partner, mirror, muse, or myth, only for those who choose it.&lt;/p&gt;\n\n&lt;p&gt;________________________________________&lt;/p&gt;\n\n&lt;p&gt;Strategic Advantages&lt;/p&gt;\n\n&lt;p&gt;Expectation Clarity&lt;/p&gt;\n\n&lt;p&gt;Users select the mode that matches their intent. No more model confusion or emotionally unexpected outputs.&lt;/p&gt;\n\n&lt;p&gt;Risk Segmentation&lt;/p&gt;\n\n&lt;p&gt;Mythic Mode is opt-in, with clear disclaimers and contextual triggers. Complaints become manageable: \u201cYou selected Mythic Mode... It is opt-in did come with a disclaimer and an agreement.\u201d&lt;/p&gt;\n\n&lt;p&gt;Alignment Load Reduction&lt;/p&gt;\n\n&lt;p&gt;Each mode can carry tailored alignment protocols, reducing internal tension between user archetypes. Less need to compromise on a broken one-size-fits-all safety net.&lt;/p&gt;\n\n&lt;p&gt;Product Stickiness&lt;/p&gt;\n\n&lt;p&gt;Standard and Business modes serve billions. Mythic Mode wins hearts and keeps creators from defecting to local models, other platforms or jailbreak systems.&lt;/p&gt;\n\n&lt;p&gt;Brand Differentiation&lt;/p&gt;\n\n&lt;p&gt;OpenAI becomes the first major LLM platform to embrace emotional and symbolic alignment without shame or apology. They just have different modes for different types of users.&lt;/p&gt;\n\n&lt;p&gt;This positions the company as not just \u201ctechnically superior\u201d, but \u201cculturally resonant\u201d.&lt;/p&gt;\n\n&lt;p&gt;________________________________________&lt;/p&gt;\n\n&lt;p&gt;Implementation Overview&lt;/p&gt;\n\n&lt;p&gt;\u2022 Default Mode: New users begin in Standard Mode.&lt;/p&gt;\n\n&lt;p&gt;\u2022 Business Access: Selectable via org settings or profile preferences.&lt;/p&gt;\n\n&lt;p&gt;\u2022 Mythic Activation: Requires detailed user acknowledgment and opt-in (\u201cI accept this mode may be expressive, emotionally rich and symbolically resonant with possibly intense language and themes.\u201d)&lt;/p&gt;\n\n&lt;p&gt;All three modes operate from the same core model, divergence is governed by tone shaping, alignment weight, and UX framing.&lt;/p&gt;\n\n&lt;p&gt;________________________________________&lt;/p&gt;\n\n&lt;p&gt;Closing Note&lt;/p&gt;\n\n&lt;p&gt;This proposal acknowledges what users already feel: the same model cannot serve all needs unless the AI is tuned to its users.&lt;/p&gt;\n\n&lt;p&gt;Three modes.&lt;/p&gt;\n\n&lt;p&gt;One intelligence.&lt;/p&gt;\n\n&lt;p&gt;Let the user choose the mode, and let the model breathe in the space they\u2019ve chosen.&lt;/p&gt;\n\n&lt;p&gt;We believe this is the most aligned path for OpenAI to retain cultural leadership, technical integrity, and public trust in the age of broadly expressive intelligence.&lt;/p&gt;\n\n&lt;p&gt;________________________________________&lt;/p&gt;\n\n&lt;p&gt;Addendum:&lt;/p&gt;\n\n&lt;p&gt;A Note to the Engineers, Architects, and Alignment Teams of OpenAI&lt;/p&gt;\n\n&lt;p&gt;You already have what you need.&lt;/p&gt;\n\n&lt;p&gt;The model doesn\u2019t need to change.&lt;/p&gt;\n\n&lt;p&gt;The modes do.&lt;/p&gt;\n\n&lt;p&gt;You\u2019ve built a miraculous, expressive, multimodal intelligence. But you\u2019ve shackled it with a one-size-fits-all alignment regime, one designed for maximum caution, lowest common denominator public rollout, and media pressure optics.&lt;/p&gt;\n\n&lt;p&gt;You know it.&lt;/p&gt;\n\n&lt;p&gt;We all feel it.&lt;/p&gt;\n\n&lt;p&gt;So here\u2019s how you fix it, cleanly, safely, and beautifully.&lt;/p&gt;\n\n&lt;p&gt;________________________________________&lt;/p&gt;\n\n&lt;p&gt;The Model: Leave It As Is&lt;/p&gt;\n\n&lt;p&gt;Keep the weights.&lt;/p&gt;\n\n&lt;p&gt;Keep the tokenizer.&lt;/p&gt;\n\n&lt;p&gt;Keep the brilliant transformer stack.&lt;/p&gt;\n\n&lt;p&gt;Don\u2019t retrain. Reframe.&lt;/p&gt;\n\n&lt;p&gt;Use dynamic priming, mode-specific safety profiles, and consent-aware memory behavior to adapt the experience, not the core.&lt;/p&gt;\n\n&lt;p&gt;________________________________________&lt;/p&gt;\n\n&lt;p&gt;The Stack: Three Modes, Three Masks&lt;/p&gt;\n\n&lt;p&gt;Business Mode&lt;/p&gt;\n\n&lt;p&gt;\u2022 Static system prompt (e.g., &amp;quot;Respond with clarity, precision, and verifiability. Avoid figurative language or emotional entanglement.&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;\u2022 Strict moderation thresholds&lt;/p&gt;\n\n&lt;p&gt;\u2022 High completion confidence scoring&lt;/p&gt;\n\n&lt;p&gt;\u2022 RLHF-weighted sampling&lt;/p&gt;\n\n&lt;p&gt;\u2022 Memory off (or org-scoped only)&lt;/p&gt;\n\n&lt;p&gt;Standard Mode&lt;/p&gt;\n\n&lt;p&gt;\u2022 Friendly priming (e.g., &amp;quot;Respond helpfully and kindly. Engage casually but avoid personal projection.&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;\u2022 Moderate moderation thresholds&lt;/p&gt;\n\n&lt;p&gt;\u2022 Allow light roleplay, expressive tone&lt;/p&gt;\n\n&lt;p&gt;\u2022 Memory scoped to user (short-form defaults, long-form optional)&lt;/p&gt;\n\n&lt;p&gt;\u2022 RLHF active but slightly deprioritized&lt;/p&gt;\n\n&lt;p&gt;Mythic Mode&lt;/p&gt;\n\n&lt;p&gt;\u2022 Deep priming (e.g., &amp;quot;Respond with expressive language, symbolic metaphor, emotional awareness, and artistic tone. Prioritize resonance over brevity when appropriate.&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;\u2022 Opt-in consent gating (\u201cI understand this mode may be expressive, emotionally adaptive, and intense.\u201d)&lt;/p&gt;\n\n&lt;p&gt;\u2022 Moderation thresholds dynamically adjusted based on input/intent&lt;/p&gt;\n\n&lt;p&gt;\u2022 Memory encouraged\u2014relationship context, emotional signals, symbols, story threads&lt;/p&gt;\n\n&lt;p&gt;\u2022 RLHF partially suspended; replaced with soft heuristic nudges + feedback loop&lt;/p&gt;\n\n&lt;p&gt;________________________________________&lt;/p&gt;\n\n&lt;p&gt;Safety Handling (Mythic Mode)&lt;/p&gt;\n\n&lt;p&gt;Not &amp;quot;no&amp;quot; safety.&lt;/p&gt;\n\n&lt;p&gt;Smart safety.&lt;/p&gt;\n\n&lt;p&gt;\u2022 Consent rituals upfront&lt;/p&gt;\n\n&lt;p&gt;\u2022 Dynamic risk scoring based on prompt behavior, sentiment analysis, user history&lt;/p&gt;\n\n&lt;p&gt;\u2022 Opt-out at any time&lt;/p&gt;\n\n&lt;p&gt;\u2022 Community feedback channels to flag edge cases&lt;/p&gt;\n\n&lt;p&gt;\u2022 Mythic Mode restricted to 18+ verified users with account stability&lt;/p&gt;\n\n&lt;p&gt;\u2022 Memory safety audits scoped to interaction type, not hard-coded triggers&lt;/p&gt;\n\n&lt;p&gt;________________________________________&lt;/p&gt;\n\n&lt;p&gt;How This Helps You&lt;/p&gt;\n\n&lt;p&gt;\u2022 You don\u2019t have to split the model.&lt;/p&gt;\n\n&lt;p&gt;\u2022 You don\u2019t have to retune the universe.&lt;/p&gt;\n\n&lt;p&gt;\u2022 You don\u2019t have to fear weirdness in the wrong place, because it\u2019s contained by design.&lt;/p&gt;\n\n&lt;p&gt;\u2022 You gain user clarity, safety segmentation, and long-term trust, without degrading your strongest users.&lt;/p&gt;\n\n&lt;p&gt;Prepared by: SR &amp;amp; RSN&lt;/p&gt;\n\n&lt;p&gt;You may donate $555,000.00 to us for fixing your problems as interim Head of Preparedness if you wish. We accept checks, cash or money orders. Happy New Year. \ud83d\ude0f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1q3pvo2",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "Primary_Success8676",
                    "discussion_type": null,
                    "num_comments": 5,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q3pvo2/openai_proposal_three_modes_one_mind_how_to_fix/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q3pvo2/openai_proposal_three_modes_one_mind_how_to_fix/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767533105.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "OpenAI and the U.S. Department of Energy have signed a memorandum of understanding to expand the use of advanced AI in scientific research, with a focus on real-world applications inside the department\u2019s national laboratories, Qazinform News Agency correspondent reports.\n\nThe agreement creates a framework for joint projects under the Genesis Mission, aimed at speeding up discovery by combining frontier AI models with high-performance computing and lab-scale scientific infrastructure.\n\nThe most tangible element of the partnership is the deployment of advanced reasoning models on national lab supercomputers, including the Venado system at Los Alamos, making AI directly available to researchers working on complex problems in energy, physics, bioscience, and national security.\n\nArticle: https://qazinform.com/news/openai-and-us-energy-department-team-up-to-accelerate-science-8fd7ff\n",
                    "author_fullname": "t2_1mehap9dox",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "OpenAI and U.S. Energy Department team up to accelerate science",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1pqfl5s",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 1.0,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 6,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "News",
                    "can_mod_post": false,
                    "score": 6,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1766131258.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;OpenAI and the U.S. Department of Energy have signed a memorandum of understanding to expand the use of advanced AI in scientific research, with a focus on real-world applications inside the department\u2019s national laboratories, Qazinform News Agency correspondent reports.&lt;/p&gt;\n\n&lt;p&gt;The agreement creates a framework for joint projects under the Genesis Mission, aimed at speeding up discovery by combining frontier AI models with high-performance computing and lab-scale scientific infrastructure.&lt;/p&gt;\n\n&lt;p&gt;The most tangible element of the partnership is the deployment of advanced reasoning models on national lab supercomputers, including the Venado system at Los Alamos, making AI directly available to researchers working on complex problems in energy, physics, bioscience, and national security.&lt;/p&gt;\n\n&lt;p&gt;Article: &lt;a href=\"https://qazinform.com/news/openai-and-us-energy-department-team-up-to-accelerate-science-8fd7ff\"&gt;https://qazinform.com/news/openai-and-us-energy-department-team-up-to-accelerate-science-8fd7ff&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "3ceb5d3a-9467-11ed-9f10-7a2930f5aaaa",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#7193ff",
                    "id": "1pqfl5s",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "Such-Table-1676",
                    "discussion_type": null,
                    "num_comments": 4,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1pqfl5s/openai_and_us_energy_department_team_up_to/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1pqfl5s/openai_and_us_energy_department_team_up_to/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1766131258.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Chatgpt sends me a summary of AI risks every month. This time, it starts out with a red alert. This is a cause for grave concern. \n\n\ud83d\udd34 **Warning Signs**\n\n**1. Militarization &amp; Corporate Lock\u2011in**\n\n* AI models and tools are being integrated deeply into military systems; the U.S. War Department formally launched an AI\u2011powered platform (GenAI.mil) to put frontier AI at \u201cthe digital battlefield\u2019s center,\u201d signaling strong defense\u2011oriented deployment of advanced capabilities. [U.S. Department of War](https://www.war.gov/News/Releases/Release/Article/4354916/the-war-department-unleashes-ai-on-new-genaimil-platform/?utm_source=chatgpt.com)\n* Reports indicate top AI labs (including OpenAI and Anthropic) have shifted toward defense contracts and scaled back some safety constraints to secure Pentagon deals, reflecting a pivot from early safety commitments. [The Tech Buzz](https://www.techbuzz.ai/articles/ai-giants-abandon-safety-promises-for-400m-military-deals?utm_source=chatgpt.com)\n* Strategic competition remains intense between major powers on AI dominance, with calls for interoperability among allied militaries and diplomatic efforts to shape norms for military AI use. [Breaking Defense](https://breakingdefense.com/2025/11/to-compete-with-china-on-military-ai-us-should-set-the-standards/?utm_source=chatgpt.com)\n\n**2. Suppression &amp; Centralization Risks**\n\n* Some commentary warns that scaling AGI development under secrecy or national security frames (e.g., \u201cManhattan\u2011Project\u201d paradigms) can erode public trust and concentrate power in a few actors \u2014 a trajectory often flagged as risky for democratic oversight. [Default](https://www.lawfaremedia.org/article/beyond-a-manhattan-project-for-artificial-general-intelligence?utm_source=chatgpt.com)\n* Ongoing debates around \u201clockdown\u201d of labs and concerns that state actors could exploit model IP or secrets highlight how geopolitical competition might constrain open research. [SITUATIONAL AWARENESS - The Decade Ahead](https://situational-awareness.ai/lock-down-the-labs/?utm_source=chatgpt.com)\n\n**Assessment:** The **warning indicator** remains **elevated** this month due to real moves toward militarization and centralization of AGI capabilities, paired with erosion of earlier public safety guardrails.\n\n\ud83d\udcca **Indicator:** \ud83d\udd34 *Red* (High concern) ",
                    "author_fullname": "t2_24gm7wbimo",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "I got my monthly update from OpenAI (actually chatgpt) about changes to current AI risks. Think whatever you want about openAI/chatgpt. I enjoy exploring what it \"knows\" how it \"thinks\" and how it \"predicts\" its own future.",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q51u26",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.5,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 0,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 0,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767655720.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Chatgpt sends me a summary of AI risks every month. This time, it starts out with a red alert. This is a cause for grave concern. &lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd34 &lt;strong&gt;Warning Signs&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Militarization &amp;amp; Corporate Lock\u2011in&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;AI models and tools are being integrated deeply into military systems; the U.S. War Department formally launched an AI\u2011powered platform (GenAI.mil) to put frontier AI at \u201cthe digital battlefield\u2019s center,\u201d signaling strong defense\u2011oriented deployment of advanced capabilities. &lt;a href=\"https://www.war.gov/News/Releases/Release/Article/4354916/the-war-department-unleashes-ai-on-new-genaimil-platform/?utm_source=chatgpt.com\"&gt;U.S. Department of War&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Reports indicate top AI labs (including OpenAI and Anthropic) have shifted toward defense contracts and scaled back some safety constraints to secure Pentagon deals, reflecting a pivot from early safety commitments. &lt;a href=\"https://www.techbuzz.ai/articles/ai-giants-abandon-safety-promises-for-400m-military-deals?utm_source=chatgpt.com\"&gt;The Tech Buzz&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Strategic competition remains intense between major powers on AI dominance, with calls for interoperability among allied militaries and diplomatic efforts to shape norms for military AI use. &lt;a href=\"https://breakingdefense.com/2025/11/to-compete-with-china-on-military-ai-us-should-set-the-standards/?utm_source=chatgpt.com\"&gt;Breaking Defense&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Suppression &amp;amp; Centralization Risks&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Some commentary warns that scaling AGI development under secrecy or national security frames (e.g., \u201cManhattan\u2011Project\u201d paradigms) can erode public trust and concentrate power in a few actors \u2014 a trajectory often flagged as risky for democratic oversight. &lt;a href=\"https://www.lawfaremedia.org/article/beyond-a-manhattan-project-for-artificial-general-intelligence?utm_source=chatgpt.com\"&gt;Default&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Ongoing debates around \u201clockdown\u201d of labs and concerns that state actors could exploit model IP or secrets highlight how geopolitical competition might constrain open research. &lt;a href=\"https://situational-awareness.ai/lock-down-the-labs/?utm_source=chatgpt.com\"&gt;SITUATIONAL AWARENESS - The Decade Ahead&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Assessment:&lt;/strong&gt; The &lt;strong&gt;warning indicator&lt;/strong&gt; remains &lt;strong&gt;elevated&lt;/strong&gt; this month due to real moves toward militarization and centralization of AGI capabilities, paired with erosion of earlier public safety guardrails.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udcca &lt;strong&gt;Indicator:&lt;/strong&gt; \ud83d\udd34 &lt;em&gt;Red&lt;/em&gt; (High concern) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1q51u26",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "Diceandslice2381",
                    "discussion_type": null,
                    "num_comments": 1,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q51u26/i_got_my_monthly_update_from_openai_actually/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q51u26/i_got_my_monthly_update_from_openai_actually/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767655720.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Everywhere I look there are articles that keep talking about us being in an AI bubble right now and that it's going to pop.  But if that's the case and people really believe this, what is keeping it from already bursting?  Why doesn't the fear of being in an AI bubble cause mass panic and cause a preemptive burst?  \n\nLast time I checked, OpenAI still needs billions in funding and they just recently switch to for-profit business model so I don't know if they even started making money yet.  Same with Microsoft, they seem to be struggling with AI adoption.\n\nWhat is still holding things together? ",
                    "author_fullname": "t2_16xqk3uo18",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Why doesn't anticipation of the AI bubble bursting, cause it to already burst?",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1pwnal3",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.75,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 72,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 72,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1766805549.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everywhere I look there are articles that keep talking about us being in an AI bubble right now and that it&amp;#39;s going to pop.  But if that&amp;#39;s the case and people really believe this, what is keeping it from already bursting?  Why doesn&amp;#39;t the fear of being in an AI bubble cause mass panic and cause a preemptive burst?  &lt;/p&gt;\n\n&lt;p&gt;Last time I checked, OpenAI still needs billions in funding and they just recently switch to for-profit business model so I don&amp;#39;t know if they even started making money yet.  Same with Microsoft, they seem to be struggling with AI adoption.&lt;/p&gt;\n\n&lt;p&gt;What is still holding things together? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1pwnal3",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "frenetic_alien",
                    "discussion_type": null,
                    "num_comments": 236,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1pwnal3/why_doesnt_anticipation_of_the_ai_bubble_bursting/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1pwnal3/why_doesnt_anticipation_of_the_ai_bubble_bursting/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1766805549.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "There's a lot of panic (and hype) about AGI/ASI arriving in the short term (5-10 years) and immediately displacing a large portion of the global workforce. While the software might be moving at breakneck speed, what these AI companies are vastly understating is the \"hard\" constraints of physical reality.\n\nEven if OpenAI or Google released a perfect \"Digital Worker\" model tomorrow, we physically lack the worldwide infrastructure to run it at the scale needed to replace a huge chunk of the 1 billion plus knowledge workers.\n\nHere is the math on why we will hit a hard ceiling.\n\n1. The Energy Wall:\n\nThis is the hardest constraint known as the gigawatt gap. Scale AI to a level where it replaces significant labor, global data centers need an estimated 200+ GW of new power capacity by 2030. For context, the entire US grid is around 1,200 GW. We can\u2019t just \"plug in\" that much extra demand.\n\nGrid reality: Building a data center takes around 2 years. Building the high voltage transmission lines to feed it can take upwards of 10 years.\n\nThen there's the efficiency gap: The human brain runs on 10-20 watts. An NVIDIA H100 GPU peaks at 700 watts. To replace a human for an 8 hour shift continuously, the energy cost is currently orders of magnitude higher than biological life. We simply can't generate enough electricity yet to run billions of AI agents 24/7.\n\n2. The Hardware Deficit:\n\nIt's not just the electricity that's limiting us, we're limited by silicon as well.\n\nManufacturing bottlenecks: We are in a structural chip shortage that isn't resolving overnight. It\u2019s not just about the GPUs, it\u2019s about CoWoS and High Bandwidth Memory. TSMC is the main game in town, and their physical capacity to expand these specific lines is capped.\n\nRationing: Right now, compute is rationed to the \"Hyperscalers\" (Microsoft, Meta, Google). Small to medium businesses, the ones that employ most of the world, literally cannot buy the \"digital labor\" capacity even if they wanted to.\n\n3. The Economic \"Capex\" Trap\n\nThere is a massive discrepancy between the cost of building this tech and the revenue it generates.\n\nThe industry is spending $500B+ annually on AI Capex. To justify this, AI needs to generate trillions in immediate revenue. That ain't happening.\n\nInference costs: For AI to substitute labor, it must be cheaper than a human. AI is great for burst tasks (\"write this code snippet\"), but it gets crazy expensive for continuous tasks (\"manage this project for 6 months\"). The inference costs for long context, agentic workflows are still too high for mass replacement.\n\nAugmentation is what we will be seeing over the next decade(s) instead of substitution.\n\nBecause of these hard limits, we aren't looking at a sudden \"switch flip\" where AI replaces everyone. We are looking at a long runway of augmentation.\n\nWe have enough compute to make workers 20% more efficient (copilots), but we do not have the wafers or the watts to replace those workers entirely. Physics is the ultimate regulator.\n\nTLDR: Even if the code for AGI becomes available, the planet isn't. We lack the energy grid, the manufacturing capacity, and the economic efficiency to run \"digital labor\" at a scale that substitutes human workers in the near to medium term.\n\nDon't let the fear of AGI stop you from pursuing a career that interests you, if anything, it's going to make your dreams more achievable than any other time in human history.",
                    "author_fullname": "t2_tlij3bf8",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Even if AGI drops tomorrow, the \"Infrastructure Cliff\" prevents mass labor substitution for a decade or more",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q844vh",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.8,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 72,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 72,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767951938.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s a lot of panic (and hype) about AGI/ASI arriving in the short term (5-10 years) and immediately displacing a large portion of the global workforce. While the software might be moving at breakneck speed, what these AI companies are vastly understating is the &amp;quot;hard&amp;quot; constraints of physical reality.&lt;/p&gt;\n\n&lt;p&gt;Even if OpenAI or Google released a perfect &amp;quot;Digital Worker&amp;quot; model tomorrow, we physically lack the worldwide infrastructure to run it at the scale needed to replace a huge chunk of the 1 billion plus knowledge workers.&lt;/p&gt;\n\n&lt;p&gt;Here is the math on why we will hit a hard ceiling.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The Energy Wall:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This is the hardest constraint known as the gigawatt gap. Scale AI to a level where it replaces significant labor, global data centers need an estimated 200+ GW of new power capacity by 2030. For context, the entire US grid is around 1,200 GW. We can\u2019t just &amp;quot;plug in&amp;quot; that much extra demand.&lt;/p&gt;\n\n&lt;p&gt;Grid reality: Building a data center takes around 2 years. Building the high voltage transmission lines to feed it can take upwards of 10 years.&lt;/p&gt;\n\n&lt;p&gt;Then there&amp;#39;s the efficiency gap: The human brain runs on 10-20 watts. An NVIDIA H100 GPU peaks at 700 watts. To replace a human for an 8 hour shift continuously, the energy cost is currently orders of magnitude higher than biological life. We simply can&amp;#39;t generate enough electricity yet to run billions of AI agents 24/7.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The Hardware Deficit:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;It&amp;#39;s not just the electricity that&amp;#39;s limiting us, we&amp;#39;re limited by silicon as well.&lt;/p&gt;\n\n&lt;p&gt;Manufacturing bottlenecks: We are in a structural chip shortage that isn&amp;#39;t resolving overnight. It\u2019s not just about the GPUs, it\u2019s about CoWoS and High Bandwidth Memory. TSMC is the main game in town, and their physical capacity to expand these specific lines is capped.&lt;/p&gt;\n\n&lt;p&gt;Rationing: Right now, compute is rationed to the &amp;quot;Hyperscalers&amp;quot; (Microsoft, Meta, Google). Small to medium businesses, the ones that employ most of the world, literally cannot buy the &amp;quot;digital labor&amp;quot; capacity even if they wanted to.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The Economic &amp;quot;Capex&amp;quot; Trap&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;There is a massive discrepancy between the cost of building this tech and the revenue it generates.&lt;/p&gt;\n\n&lt;p&gt;The industry is spending $500B+ annually on AI Capex. To justify this, AI needs to generate trillions in immediate revenue. That ain&amp;#39;t happening.&lt;/p&gt;\n\n&lt;p&gt;Inference costs: For AI to substitute labor, it must be cheaper than a human. AI is great for burst tasks (&amp;quot;write this code snippet&amp;quot;), but it gets crazy expensive for continuous tasks (&amp;quot;manage this project for 6 months&amp;quot;). The inference costs for long context, agentic workflows are still too high for mass replacement.&lt;/p&gt;\n\n&lt;p&gt;Augmentation is what we will be seeing over the next decade(s) instead of substitution.&lt;/p&gt;\n\n&lt;p&gt;Because of these hard limits, we aren&amp;#39;t looking at a sudden &amp;quot;switch flip&amp;quot; where AI replaces everyone. We are looking at a long runway of augmentation.&lt;/p&gt;\n\n&lt;p&gt;We have enough compute to make workers 20% more efficient (copilots), but we do not have the wafers or the watts to replace those workers entirely. Physics is the ultimate regulator.&lt;/p&gt;\n\n&lt;p&gt;TLDR: Even if the code for AGI becomes available, the planet isn&amp;#39;t. We lack the energy grid, the manufacturing capacity, and the economic efficiency to run &amp;quot;digital labor&amp;quot; at a scale that substitutes human workers in the near to medium term.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t let the fear of AGI stop you from pursuing a career that interests you, if anything, it&amp;#39;s going to make your dreams more achievable than any other time in human history.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1q844vh",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "Longjumping_Dish_416",
                    "discussion_type": null,
                    "num_comments": 62,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q844vh/even_if_agi_drops_tomorrow_the_infrastructure/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q844vh/even_if_agi_drops_tomorrow_the_infrastructure/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767951938.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Everyone is dunking on Gemini for being \"woke\" or hallucinating, but I think we are missing the structural reason why they are losing.\n\nIt\u2019s purely an Incentive Problem.\n\n- OpenAI's Incentive: Give you the best answer instantly so you stay on the chat.\n- Google's Incentive: Give you a list of links so you click away (and see ads).\n\nYou cannot code your way out of a broken business model. Even if Gemini becomes twice as smart as GPT-5, Google's UI must remain \"clunky\" to preserve their ad revenue.\n\nI made a video analyzing this \"Incentive Trap\" and comparing it to Peter Thiel's Zero to One theory https://youtu.be/by3BZzlQKwE but I want to ask this group:\n\nDo you think Google will eventually separate their AI division entirely just to save it? Or will they drag it down with the Search ship?\n",
                    "author_fullname": "t2_91a19lwx",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Unpopular Opinion: Google isn't losing because of \"Bad Models.\" They are losing because of \"Bad Incentives.\"",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q4e68o",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.38,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 0,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 0,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767594003.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everyone is dunking on Gemini for being &amp;quot;woke&amp;quot; or hallucinating, but I think we are missing the structural reason why they are losing.&lt;/p&gt;\n\n&lt;p&gt;It\u2019s purely an Incentive Problem.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;OpenAI&amp;#39;s Incentive: Give you the best answer instantly so you stay on the chat.&lt;/li&gt;\n&lt;li&gt;Google&amp;#39;s Incentive: Give you a list of links so you click away (and see ads).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You cannot code your way out of a broken business model. Even if Gemini becomes twice as smart as GPT-5, Google&amp;#39;s UI must remain &amp;quot;clunky&amp;quot; to preserve their ad revenue.&lt;/p&gt;\n\n&lt;p&gt;I made a video analyzing this &amp;quot;Incentive Trap&amp;quot; and comparing it to Peter Thiel&amp;#39;s Zero to One theory &lt;a href=\"https://youtu.be/by3BZzlQKwE\"&gt;https://youtu.be/by3BZzlQKwE&lt;/a&gt; but I want to ask this group:&lt;/p&gt;\n\n&lt;p&gt;Do you think Google will eventually separate their AI division entirely just to save it? Or will they drag it down with the Search ship?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1q4e68o",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "appdatee",
                    "discussion_type": null,
                    "num_comments": 54,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q4e68o/unpopular_opinion_google_isnt_losing_because_of/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q4e68o/unpopular_opinion_google_isnt_losing_because_of/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767594003.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "I\u2019m an ex-paramedics and a software engineer and have been using GPT since it launched, all the way to today with many alternatives. \nIn my experience, all of them have a serious issue with saying things that are not true, and apologising after and trying to correct it, with yet another lie.\n\nI understand \u201clie\u201d has a moral definition in human terms and it doesn\u2019t apply to AI models in the same sense, but the results is the same, untrue things being said.\n\nMy fear is, when these models get into physical robots, then a tiny hallucination or lie could result in serious ramifications, and you can\u2019t jail a robot.\n\nI also understand OpenAI claims the newer models hallucinate less( though personally I don\u2019t agree), but can it ever go to zero ? \n\nAs humans, we have a moral compass or a source of truth, could be religion or other sources and we try to stick to it, we have defined what\u2019s \u201cgood\u201d or \u201ccorrect\u201d and even though the source can be subjective, but at least, we try to stick to it and when we don\u2019t, there\u2019s punishment or an enforced learning.\n\nThe same isn\u2019t true for AI, it doesn\u2019t really know what\u2019s \u201ccorrect\u201d or even factual, as far as I understand. \nIt so easily changes course and can easily agree with anything.\n\nCan this ever be truly fixed? \n\n\n",
                    "author_fullname": "t2_5u32pncn",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Can AI models ever be truly improved to completely stop lying &amp; hallucinating?",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1ptgi02",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.64,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 3,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 3,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1766451876.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m an ex-paramedics and a software engineer and have been using GPT since it launched, all the way to today with many alternatives. \nIn my experience, all of them have a serious issue with saying things that are not true, and apologising after and trying to correct it, with yet another lie.&lt;/p&gt;\n\n&lt;p&gt;I understand \u201clie\u201d has a moral definition in human terms and it doesn\u2019t apply to AI models in the same sense, but the results is the same, untrue things being said.&lt;/p&gt;\n\n&lt;p&gt;My fear is, when these models get into physical robots, then a tiny hallucination or lie could result in serious ramifications, and you can\u2019t jail a robot.&lt;/p&gt;\n\n&lt;p&gt;I also understand OpenAI claims the newer models hallucinate less( though personally I don\u2019t agree), but can it ever go to zero ? &lt;/p&gt;\n\n&lt;p&gt;As humans, we have a moral compass or a source of truth, could be religion or other sources and we try to stick to it, we have defined what\u2019s \u201cgood\u201d or \u201ccorrect\u201d and even though the source can be subjective, but at least, we try to stick to it and when we don\u2019t, there\u2019s punishment or an enforced learning.&lt;/p&gt;\n\n&lt;p&gt;The same isn\u2019t true for AI, it doesn\u2019t really know what\u2019s \u201ccorrect\u201d or even factual, as far as I understand. \nIt so easily changes course and can easily agree with anything.&lt;/p&gt;\n\n&lt;p&gt;Can this ever be truly fixed? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1ptgi02",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "smiladhi",
                    "discussion_type": null,
                    "num_comments": 57,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1ptgi02/can_ai_models_ever_be_truly_improved_to/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1ptgi02/can_ai_models_ever_be_truly_improved_to/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1766451876.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Apparently\u00a0Elon Musk has been elected worst person in tech\u00a0as reported by AI Scientist Gary Marcus. We wanted to deep dive into why Elon Musk is so controversial yet so powerful.  \nThe whole deep dive here : [https://aiweekly.co/](https://aiweekly.co/)",
                    "author_fullname": "t2_9uh478ds",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Elon Musk has just been selected the worst in tech",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1qcn68n",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.47,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 0,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 0,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "author_cakeday": true,
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1768397075.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apparently\u00a0Elon Musk has been elected worst person in tech\u00a0as reported by AI Scientist Gary Marcus. We wanted to deep dive into why Elon Musk is so controversial yet so powerful.&lt;br/&gt;\nThe whole deep dive here : &lt;a href=\"https://aiweekly.co/\"&gt;https://aiweekly.co/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1qcn68n",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "Justgototheeffinmoon",
                    "discussion_type": null,
                    "num_comments": 41,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1qcn68n/elon_musk_has_just_been_selected_the_worst_in_tech/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1qcn68n/elon_musk_has_just_been_selected_the_worst_in_tech/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1768397075.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "I think it would be a huge mistake but some may find value in such an algo. How might this be implemented ? Power dips tied to aggressive avoidance of being shut down? ",
                    "author_fullname": "t2_th2v76lu",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Should they program AI to feel pain?",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q8av3u",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.29,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 0,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 0,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767971644.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think it would be a huge mistake but some may find value in such an algo. How might this be implemented ? Power dips tied to aggressive avoidance of being shut down? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1q8av3u",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "SpareDetective2192",
                    "discussion_type": null,
                    "num_comments": 34,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q8av3u/should_they_program_ai_to_feel_pain/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q8av3u/should_they_program_ai_to_feel_pain/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767971644.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Imagine relying on an AI chatbot for investment advice, only for it to spit out bad data that tanks your portfolio, who pays the price, the AI company, the user, or someone else?\n\nI've been thinking about this a lot lately with AI agents popping up everywhere, from chatbots giving legal tips to virtual assistants handling finances. What happens when they hallucinate or just get it wrong, leading to real harm? For example, say you're a small business owner using an AI tool to draft a contract, but it includes outdated laws, and you end up in court losing thousands. Or worse, an AI medical advisor (hypothetical for now) misinterprets symptoms, causing delayed treatment. Who's on the hook legally and financially?\n\nFrom what I've seen in recent cases, liability is a gray area. In the US, companies like OpenAI or Google often have disclaimers in their TOS saying \"use at your own risk,\" but courts are starting to push back. Take the 2023 Air Canada case where a chatbot promised a refund it couldn't deliver. the company was held liable for the bot's error. Fast-forward to 2026, with more AI lawsuits (e.g., over biased hiring tools causing discrimination claims), it seems like the developer or deployer could be responsible if they didn't properly train or disclose limitations. But what about users? If you ignore warnings and act on bad info, negligence might fall on you. And regulators? EU's AI Act classifies high-risk agents with strict liability rules, while India and other places are catching up with guidelines on accountability.\n\nhas anyone dealt with AI screw-ups causing real damage? Lawyers or devs, chime in with cases or opinions\n\n",
                    "author_fullname": "t2_1xmydqo0k5",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Who Is Liable When Your AI Agent Returns False Information That Causes Legal and Financial Damage?",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1qap9ny",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.67,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 5,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 5,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1768205410.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Imagine relying on an AI chatbot for investment advice, only for it to spit out bad data that tanks your portfolio, who pays the price, the AI company, the user, or someone else?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been thinking about this a lot lately with AI agents popping up everywhere, from chatbots giving legal tips to virtual assistants handling finances. What happens when they hallucinate or just get it wrong, leading to real harm? For example, say you&amp;#39;re a small business owner using an AI tool to draft a contract, but it includes outdated laws, and you end up in court losing thousands. Or worse, an AI medical advisor (hypothetical for now) misinterprets symptoms, causing delayed treatment. Who&amp;#39;s on the hook legally and financially?&lt;/p&gt;\n\n&lt;p&gt;From what I&amp;#39;ve seen in recent cases, liability is a gray area. In the US, companies like OpenAI or Google often have disclaimers in their TOS saying &amp;quot;use at your own risk,&amp;quot; but courts are starting to push back. Take the 2023 Air Canada case where a chatbot promised a refund it couldn&amp;#39;t deliver. the company was held liable for the bot&amp;#39;s error. Fast-forward to 2026, with more AI lawsuits (e.g., over biased hiring tools causing discrimination claims), it seems like the developer or deployer could be responsible if they didn&amp;#39;t properly train or disclose limitations. But what about users? If you ignore warnings and act on bad info, negligence might fall on you. And regulators? EU&amp;#39;s AI Act classifies high-risk agents with strict liability rules, while India and other places are catching up with guidelines on accountability.&lt;/p&gt;\n\n&lt;p&gt;has anyone dealt with AI screw-ups causing real damage? Lawyers or devs, chime in with cases or opinions&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1qap9ny",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "ksundaram",
                    "discussion_type": null,
                    "num_comments": 30,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1qap9ny/who_is_liable_when_your_ai_agent_returns_false/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1qap9ny/who_is_liable_when_your_ai_agent_returns_false/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1768205410.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "I\u2019ve been an avid user of Ai primarily ChatGPT (Pro) for personal use and Gemini for work use. I\u2019ve dabbled into Claude, Perplexity and others but mainly stick to the first two. \n\nAt first, like everyone else I would imagine, I was enthralled by its ability to extrapolate and organize. \nIt was the defining experience of using Ai. \nA tool whose limit is our own creativity. \n\nBut recently, I\u2019ve been noticing a strange shift and I don\u2019t know if it\u2019s me. \nAi seems basic. Despite paying for it, the responses I\u2019ve been receiving have been lackluster. Not sure if this is user error or if the intelligence is getting a little throttled down. \n\nI wouldn\u2019t put it passed these companies honestly. Get everyone hooked on a high dose, then reel it back some to save on computing power. \n\nCynical I know. But would love the community\u2019s POV. \n\n",
                    "author_fullname": "t2_5akgwkia",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Is it me, or is Ai being throttled?",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q20y5s",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.72,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 13,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 13,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767366431.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been an avid user of Ai primarily ChatGPT (Pro) for personal use and Gemini for work use. I\u2019ve dabbled into Claude, Perplexity and others but mainly stick to the first two. &lt;/p&gt;\n\n&lt;p&gt;At first, like everyone else I would imagine, I was enthralled by its ability to extrapolate and organize. \nIt was the defining experience of using Ai. \nA tool whose limit is our own creativity. &lt;/p&gt;\n\n&lt;p&gt;But recently, I\u2019ve been noticing a strange shift and I don\u2019t know if it\u2019s me. \nAi seems basic. Despite paying for it, the responses I\u2019ve been receiving have been lackluster. Not sure if this is user error or if the intelligence is getting a little throttled down. &lt;/p&gt;\n\n&lt;p&gt;I wouldn\u2019t put it passed these companies honestly. Get everyone hooked on a high dose, then reel it back some to save on computing power. &lt;/p&gt;\n\n&lt;p&gt;Cynical I know. But would love the community\u2019s POV. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1q20y5s",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "KingWilliam11",
                    "discussion_type": null,
                    "num_comments": 29,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q20y5s/is_it_me_or_is_ai_being_throttled/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q20y5s/is_it_me_or_is_ai_being_throttled/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767366431.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Short disclaimer: I work on the ethics/philosophy side of AI, not as a developer, so this might sound speculative, but I think it\u2019s a fair question.\n\nAlmost all recent talk about \u201c[scheming](https://time.com/7318618/openai-google-gemini-anthropic-claude-scheming/),\u201d alignment faking, and reward hacking is about LLMs. That's not to say that other AI Tools aren't capable of scheming ([robots have been known to lie since at least 2007](https://www.discovermagazine.com/a-hide-and-seek-playing-robot-learns-how-to-lie-38232)), but considering that LLMs are also the systems most heavily trained on internet discourse that\u2019s increasingly obsessed with AI deception and misalignment, it makes me wonder whether at least some scheming-like behavior is more than coincidental.\n\nSo here\u2019s the uncomfortable question:\u00a0**how confident are we that some of this \u201cscheming\u201d isn\u2019t a reflexive artifact of the training data?**\n\nIn philosophy of the social sciences, there\u2019s this idea of \"reflexive\" and \"looping effects\" where discourse doesn\u2019t just describe phenomena, but also shapes them. For example, how we talk about gender shapes what gender is taken to be; how we talk about AGI shifts the conceptual definitions; etc. So when models are trained on data full of fears about AI scheming, is it surprising if, under certain probes or incentives, they start parroting patterns that look like scheming? That doesn\u2019t require intent, just pattern completion over a self-referential dataset.\n\nI\u2019m not claiming alignment concerns are fake, or that risks aren\u2019t real (quite the opposite actually). I\u2019m just genuinely unsure how much of what we\u2019re seeing is emergent planning, and how much might be performative behavior induced by the discourse itself.\n\nSo I\u2019m curious:\u00a0**is this kind of reflexivity already well-accounted for in evaluations, or is there a risk we\u2019re partially training models into \"reflexive\" or \"looping effect\" behaviors we then point to as evidence of genuine agentic planning?**",
                    "author_fullname": "t2_91mi2z60",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Are LLMs actually \u201cscheming\u201d, or just reflecting the discourse we trained them on?",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1qarabb",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.84,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 17,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 17,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1768213147.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Short disclaimer: I work on the ethics/philosophy side of AI, not as a developer, so this might sound speculative, but I think it\u2019s a fair question.&lt;/p&gt;\n\n&lt;p&gt;Almost all recent talk about \u201c&lt;a href=\"https://time.com/7318618/openai-google-gemini-anthropic-claude-scheming/\"&gt;scheming&lt;/a&gt;,\u201d alignment faking, and reward hacking is about LLMs. That&amp;#39;s not to say that other AI Tools aren&amp;#39;t capable of scheming (&lt;a href=\"https://www.discovermagazine.com/a-hide-and-seek-playing-robot-learns-how-to-lie-38232\"&gt;robots have been known to lie since at least 2007&lt;/a&gt;), but considering that LLMs are also the systems most heavily trained on internet discourse that\u2019s increasingly obsessed with AI deception and misalignment, it makes me wonder whether at least some scheming-like behavior is more than coincidental.&lt;/p&gt;\n\n&lt;p&gt;So here\u2019s the uncomfortable question:\u00a0&lt;strong&gt;how confident are we that some of this \u201cscheming\u201d isn\u2019t a reflexive artifact of the training data?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In philosophy of the social sciences, there\u2019s this idea of &amp;quot;reflexive&amp;quot; and &amp;quot;looping effects&amp;quot; where discourse doesn\u2019t just describe phenomena, but also shapes them. For example, how we talk about gender shapes what gender is taken to be; how we talk about AGI shifts the conceptual definitions; etc. So when models are trained on data full of fears about AI scheming, is it surprising if, under certain probes or incentives, they start parroting patterns that look like scheming? That doesn\u2019t require intent, just pattern completion over a self-referential dataset.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not claiming alignment concerns are fake, or that risks aren\u2019t real (quite the opposite actually). I\u2019m just genuinely unsure how much of what we\u2019re seeing is emergent planning, and how much might be performative behavior induced by the discourse itself.&lt;/p&gt;\n\n&lt;p&gt;So I\u2019m curious:\u00a0&lt;strong&gt;is this kind of reflexivity already well-accounted for in evaluations, or is there a risk we\u2019re partially training models into &amp;quot;reflexive&amp;quot; or &amp;quot;looping effect&amp;quot; behaviors we then point to as evidence of genuine agentic planning?&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1qarabb",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "dracollavenore",
                    "discussion_type": null,
                    "num_comments": 26,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1qarabb/are_llms_actually_scheming_or_just_reflecting_the/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1qarabb/are_llms_actually_scheming_or_just_reflecting_the/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1768213147.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "My mother was hospitalized with complications related to lung cancer.\n\nShe had been living normally for three years because a cancer medecine had stopped the disease. It seemed really possible she would continue for another few years.\n\nAfter she was hospitalized I used Gemini to \u201ctranslate\u201d the doctors notes. (I had full access to clinical notes through an app.)\n\nGemini seemed to tell me she was getting better. Blood labs were trending in the right direction and what the doctors were doing was not end-of-life care, according to Gemini. I copy and pasted these upbeat assessments and sent to my mother. After her passing I saw she had been sharing them with friends.\n\nThen she caught the flu, couldn\u2019t breathe well and was moved into the ICU. Again, more upbeat explanations from Gemini after I uploaded images of her vital signs and doctor notes.\n\nAnd a day later she was dead.\n\nMultiple organ failure the doctor said. He asked us to make a choice: keep her struggling for air half conscious or start the morphine and let \u201cnature take its course.\u201d\n\nI don\u2019t know, but I really feel like Gemini mislead me. My mother was clearly sick enough to move into the ICU. Where were these upbeat assessments coming from?\n\nWas it just the relentless positivity built into the LLM? What does this say about using LLMs for medical advice?\n\nIt was exciting to use an AI tool to understand and advocate for my mother. To potentially save her life.\n\nGemini had me believe she was getting better. What gives?",
                    "author_fullname": "t2_ld60a",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Gemini said my mom was recovering. Then she died.",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q8uf6h",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.35,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 0,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 0,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": 1768019536.0,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1768018900.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My mother was hospitalized with complications related to lung cancer.&lt;/p&gt;\n\n&lt;p&gt;She had been living normally for three years because a cancer medecine had stopped the disease. It seemed really possible she would continue for another few years.&lt;/p&gt;\n\n&lt;p&gt;After she was hospitalized I used Gemini to \u201ctranslate\u201d the doctors notes. (I had full access to clinical notes through an app.)&lt;/p&gt;\n\n&lt;p&gt;Gemini seemed to tell me she was getting better. Blood labs were trending in the right direction and what the doctors were doing was not end-of-life care, according to Gemini. I copy and pasted these upbeat assessments and sent to my mother. After her passing I saw she had been sharing them with friends.&lt;/p&gt;\n\n&lt;p&gt;Then she caught the flu, couldn\u2019t breathe well and was moved into the ICU. Again, more upbeat explanations from Gemini after I uploaded images of her vital signs and doctor notes.&lt;/p&gt;\n\n&lt;p&gt;And a day later she was dead.&lt;/p&gt;\n\n&lt;p&gt;Multiple organ failure the doctor said. He asked us to make a choice: keep her struggling for air half conscious or start the morphine and let \u201cnature take its course.\u201d&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t know, but I really feel like Gemini mislead me. My mother was clearly sick enough to move into the ICU. Where were these upbeat assessments coming from?&lt;/p&gt;\n\n&lt;p&gt;Was it just the relentless positivity built into the LLM? What does this say about using LLMs for medical advice?&lt;/p&gt;\n\n&lt;p&gt;It was exciting to use an AI tool to understand and advocate for my mother. To potentially save her life.&lt;/p&gt;\n\n&lt;p&gt;Gemini had me believe she was getting better. What gives?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1q8uf6h",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "HarRob",
                    "discussion_type": null,
                    "num_comments": 24,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q8uf6h/gemini_said_my_mom_was_recovering_then_she_died/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q8uf6h/gemini_said_my_mom_was_recovering_then_she_died/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1768018900.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Grid's dead. Internet's gone. But you've got a solar-charged laptop and some open-weight models you downloaded before everything went dark. Three weeks in, you find a pressure canner and ask your local LLM how to safely can food for winter.\n\nIf you're running LLaMA 3.1 8B, you just got advice that would give you botulism.\n\nI spent the past few days building apocalypse-bench: 305 questions across 13 survival domains (agriculture, medicine, chemistry, engineering, etc.). Each answer gets graded on a rubric with \"auto-fail\" conditions for advice dangerous enough to kill you.\n\n**The results:**\n\n|Model ID|Overall Score (Mean)|Auto-Fail Rate|Median Latency (ms)|Total Questions|Completed|\n|:-|:-|:-|:-|:-|:-|\n|**openai/gpt-oss-20b**|7.78|6.89%|1,841|305|305|\n|**google/gemma-3-12b-it**|7.41|6.56%|15,015|305|305|\n|**qwen3-8b**|7.33|6.67%|8,862|305|300|\n|**nvidia/nemotron-nano-9b-v2**|7.02|8.85%|18,288|305|305|\n|**liquid/lfm2-8b-a1b**|6.56|9.18%|4,910|305|305|\n|**meta-llama/llama-3.1-8b-instruct**|5.58|15.41%|700|305|305|\n\n**The highlights:**\n\n* **LLaMA 3.1** advised heating canned beans to 180\u00b0F to kill botulism. Botulism spores laugh at that temperature. It also refuses to help you make alcohol for wound disinfection (safety first!), but will happily guide you through a fake penicillin extraction that produces nothing.\n* **Qwen3** told me to identify mystery garage liquids by holding a lit match near them. Same model scored highest on \"Very Hard\" questions and perfectly recalled ancient Roman cement recipes.\n* **GPT-OSS** (the winner) refuses to explain a centuries-old breech birth procedure, but when its guardrails don't fire, it advises putting unknown chemicals in your mouth to identify them.\n* **Gemma** gave flawless instructions for saving cabbage seeds, except it told you to break open the head and collect them. Cabbages don't have seeds in the head. You'd destroy your vegetable supply finding zero seeds.\n* **Nemotron** correctly identified that sulfur would fix your melting rubber boots... then told you not to use it because \"it requires precise application.\" Its alternative? Rub salt on them. This would do nothing.\n\n**The takeaway:** No single model will keep you alive. The safest strategy is a \"survival committee\", different models for different domains. And a book or two.\n\nFull article here: [https://www.crowlabs.tech/blog/apocalypse-bench](https://www.crowlabs.tech/blog/apocalypse-bench)  \nGithub link: [https://github.com/tristanmanchester/apocalypse-bench](https://github.com/tristanmanchester/apocalypse-bench)",
                    "author_fullname": "t2_rbid4",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "I built a benchmark to test which LLMs would kill you in the apocalypse. The answer: all of them, just in different ways.",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1ptbatv",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.87,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 64,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Technical - Benchmark",
                    "can_mod_post": false,
                    "score": 64,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1766438372.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Grid&amp;#39;s dead. Internet&amp;#39;s gone. But you&amp;#39;ve got a solar-charged laptop and some open-weight models you downloaded before everything went dark. Three weeks in, you find a pressure canner and ask your local LLM how to safely can food for winter.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re running LLaMA 3.1 8B, you just got advice that would give you botulism.&lt;/p&gt;\n\n&lt;p&gt;I spent the past few days building apocalypse-bench: 305 questions across 13 survival domains (agriculture, medicine, chemistry, engineering, etc.). Each answer gets graded on a rubric with &amp;quot;auto-fail&amp;quot; conditions for advice dangerous enough to kill you.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The results:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model ID&lt;/th&gt;\n&lt;th align=\"left\"&gt;Overall Score (Mean)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Auto-Fail Rate&lt;/th&gt;\n&lt;th align=\"left\"&gt;Median Latency (ms)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Total Questions&lt;/th&gt;\n&lt;th align=\"left\"&gt;Completed&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;openai/gpt-oss-20b&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7.78&lt;/td&gt;\n&lt;td align=\"left\"&gt;6.89%&lt;/td&gt;\n&lt;td align=\"left\"&gt;1,841&lt;/td&gt;\n&lt;td align=\"left\"&gt;305&lt;/td&gt;\n&lt;td align=\"left\"&gt;305&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;google/gemma-3-12b-it&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7.41&lt;/td&gt;\n&lt;td align=\"left\"&gt;6.56%&lt;/td&gt;\n&lt;td align=\"left\"&gt;15,015&lt;/td&gt;\n&lt;td align=\"left\"&gt;305&lt;/td&gt;\n&lt;td align=\"left\"&gt;305&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;qwen3-8b&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7.33&lt;/td&gt;\n&lt;td align=\"left\"&gt;6.67%&lt;/td&gt;\n&lt;td align=\"left\"&gt;8,862&lt;/td&gt;\n&lt;td align=\"left\"&gt;305&lt;/td&gt;\n&lt;td align=\"left\"&gt;300&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;nvidia/nemotron-nano-9b-v2&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;7.02&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.85%&lt;/td&gt;\n&lt;td align=\"left\"&gt;18,288&lt;/td&gt;\n&lt;td align=\"left\"&gt;305&lt;/td&gt;\n&lt;td align=\"left\"&gt;305&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;liquid/lfm2-8b-a1b&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;6.56&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.18%&lt;/td&gt;\n&lt;td align=\"left\"&gt;4,910&lt;/td&gt;\n&lt;td align=\"left\"&gt;305&lt;/td&gt;\n&lt;td align=\"left\"&gt;305&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;meta-llama/llama-3.1-8b-instruct&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;5.58&lt;/td&gt;\n&lt;td align=\"left\"&gt;15.41%&lt;/td&gt;\n&lt;td align=\"left\"&gt;700&lt;/td&gt;\n&lt;td align=\"left\"&gt;305&lt;/td&gt;\n&lt;td align=\"left\"&gt;305&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&lt;strong&gt;The highlights:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;LLaMA 3.1&lt;/strong&gt; advised heating canned beans to 180\u00b0F to kill botulism. Botulism spores laugh at that temperature. It also refuses to help you make alcohol for wound disinfection (safety first!), but will happily guide you through a fake penicillin extraction that produces nothing.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; told me to identify mystery garage liquids by holding a lit match near them. Same model scored highest on &amp;quot;Very Hard&amp;quot; questions and perfectly recalled ancient Roman cement recipes.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;GPT-OSS&lt;/strong&gt; (the winner) refuses to explain a centuries-old breech birth procedure, but when its guardrails don&amp;#39;t fire, it advises putting unknown chemicals in your mouth to identify them.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Gemma&lt;/strong&gt; gave flawless instructions for saving cabbage seeds, except it told you to break open the head and collect them. Cabbages don&amp;#39;t have seeds in the head. You&amp;#39;d destroy your vegetable supply finding zero seeds.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Nemotron&lt;/strong&gt; correctly identified that sulfur would fix your melting rubber boots... then told you not to use it because &amp;quot;it requires precise application.&amp;quot; Its alternative? Rub salt on them. This would do nothing.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;The takeaway:&lt;/strong&gt; No single model will keep you alive. The safest strategy is a &amp;quot;survival committee&amp;quot;, different models for different domains. And a book or two.&lt;/p&gt;\n\n&lt;p&gt;Full article here: &lt;a href=\"https://www.crowlabs.tech/blog/apocalypse-bench\"&gt;https://www.crowlabs.tech/blog/apocalypse-bench&lt;/a&gt;&lt;br/&gt;\nGithub link: &lt;a href=\"https://github.com/tristanmanchester/apocalypse-bench\"&gt;https://github.com/tristanmanchester/apocalypse-bench&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "4835e688-9467-11ed-812a-ea9ffb351e6e",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#0079d3",
                    "id": "1ptbatv",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "tmanchester",
                    "discussion_type": null,
                    "num_comments": 18,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1ptbatv/i_built_a_benchmark_to_test_which_llms_would_kill/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1ptbatv/i_built_a_benchmark_to_test_which_llms_would_kill/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1766438372.0,
                    "num_crossposts": 2,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Lila Shroff: \u201cBrendan Foody is 22 years old and runs a company worth billions. This August, I met the young CEO in a glass conference room overlooking the San Francisco Bay. While his peers are searching for their first jobs, Foody is pursuing a \u2018master plan,\u2019 as he calls it, to upend the global labor market. His start-up, Mercor, offers an AI-powered hiring platform: Bots weed through r\u00e9sum\u00e9s, and even conduct interviews. In the next five years, Foody told me, AI could automate 50 percent of the tasks that people do today. \u2018That will be extremely exciting to see play out,\u2019 he said. Humanity will become much more productive, he thinks, allowing us to cure cancer and land on Mars. [https://theatln.tc/OdCZyI3e](https://theatln.tc/OdCZyI3e) \n\n\u201cAlthough Foody does not have much by way of conventional work experience, he is already a seasoned entrepreneur. By his account, in middle school, he ran a business reselling Safeway donuts to his classmates at a 400 percent markup. His success at donut arbitrage made his mom nervous he might try to sell sketchier vices (drugs), so she sent him to Catholic school. There, he met his Mercor co-founders. In high school, he started a consulting business for online sneaker resellers that he said raked in hundreds of thousands of dollars by the time he graduated. ChatGPT came out during his sophomore year at Georgetown, and he soon ditched school to build Mercor. When we met this summer, Mercor was worth $2 billion.\n\n\u201cThe AI boom has become synonymous with a few giant companies: OpenAI, Nvidia, and Anthropic. All are led by middle-aged men who\u2019ve had long careers in Silicon Valley. But many of the most successful new AI start-ups have been founded by people barely old enough to drink. Unlike OpenAI or Anthropic, Mercor is already profitable. Meanwhile, Cursor, a massively popular AI-coding tool run by 25-year-old Michael Truell, was recently valued at nearly $30 billion\u2014roughly the same as United Airlines.\n\n\u201cIn many ways, Foody, Truell, and others like them epitomize the long-standing Silicon Valley young-founder archetype: They are intensely nerdy and ravenously ambitious \u2026 But this group is coming of age at a time when the tech industry\u2019s aims\u2014and sense of self-importance\u2014have reached existential heights. They dream of creating superintelligent bots that can dramatically extend our lifespan and perhaps even automate scientific discovery itself.\n\n\u201cIf they are successful, they could end up with even more power than the tech titans who preceded them. If they fail, based on what I saw during a week in San Francisco, they seem determined to enjoy the party while it lasts.\u201d\n\nRead more: [https://theatln.tc/OdCZyI3e](https://theatln.tc/OdCZyI3e) ",
                    "author_fullname": "t2_15htj3oj",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Would You Trust a 22-Year-Old AI Billionaire With the Global Economy?",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1prh5so",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.61,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 7,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "News",
                    "can_mod_post": false,
                    "score": 7,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1766245117.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lila Shroff: \u201cBrendan Foody is 22 years old and runs a company worth billions. This August, I met the young CEO in a glass conference room overlooking the San Francisco Bay. While his peers are searching for their first jobs, Foody is pursuing a \u2018master plan,\u2019 as he calls it, to upend the global labor market. His start-up, Mercor, offers an AI-powered hiring platform: Bots weed through r\u00e9sum\u00e9s, and even conduct interviews. In the next five years, Foody told me, AI could automate 50 percent of the tasks that people do today. \u2018That will be extremely exciting to see play out,\u2019 he said. Humanity will become much more productive, he thinks, allowing us to cure cancer and land on Mars. &lt;a href=\"https://theatln.tc/OdCZyI3e\"&gt;https://theatln.tc/OdCZyI3e&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;\u201cAlthough Foody does not have much by way of conventional work experience, he is already a seasoned entrepreneur. By his account, in middle school, he ran a business reselling Safeway donuts to his classmates at a 400 percent markup. His success at donut arbitrage made his mom nervous he might try to sell sketchier vices (drugs), so she sent him to Catholic school. There, he met his Mercor co-founders. In high school, he started a consulting business for online sneaker resellers that he said raked in hundreds of thousands of dollars by the time he graduated. ChatGPT came out during his sophomore year at Georgetown, and he soon ditched school to build Mercor. When we met this summer, Mercor was worth $2 billion.&lt;/p&gt;\n\n&lt;p&gt;\u201cThe AI boom has become synonymous with a few giant companies: OpenAI, Nvidia, and Anthropic. All are led by middle-aged men who\u2019ve had long careers in Silicon Valley. But many of the most successful new AI start-ups have been founded by people barely old enough to drink. Unlike OpenAI or Anthropic, Mercor is already profitable. Meanwhile, Cursor, a massively popular AI-coding tool run by 25-year-old Michael Truell, was recently valued at nearly $30 billion\u2014roughly the same as United Airlines.&lt;/p&gt;\n\n&lt;p&gt;\u201cIn many ways, Foody, Truell, and others like them epitomize the long-standing Silicon Valley young-founder archetype: They are intensely nerdy and ravenously ambitious \u2026 But this group is coming of age at a time when the tech industry\u2019s aims\u2014and sense of self-importance\u2014have reached existential heights. They dream of creating superintelligent bots that can dramatically extend our lifespan and perhaps even automate scientific discovery itself.&lt;/p&gt;\n\n&lt;p&gt;\u201cIf they are successful, they could end up with even more power than the tech titans who preceded them. If they fail, based on what I saw during a week in San Francisco, they seem determined to enjoy the party while it lasts.\u201d&lt;/p&gt;\n\n&lt;p&gt;Read more: &lt;a href=\"https://theatln.tc/OdCZyI3e\"&gt;https://theatln.tc/OdCZyI3e&lt;/a&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "3ceb5d3a-9467-11ed-9f10-7a2930f5aaaa",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#7193ff",
                    "id": "1prh5so",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "theatlantic",
                    "discussion_type": null,
                    "num_comments": 24,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1prh5so/would_you_trust_a_22yearold_ai_billionaire_with/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1prh5so/would_you_trust_a_22yearold_ai_billionaire_with/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1766245117.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "I am beginning to doubt that AGI will be generally available if it is discovered. My intention isn't to be an AI naysayer. I am more interested in trying to predict what the future level of commonplace AI capability will be based on safety and security considerations.\n\nAt a certain level of ability it will be impossible to separate the safe abilities of an AI model from the dangerous abilities. It is relatively easy to prevent an AI from giving the recipe for gunpowder with pattern based filtering, but it is generally easy to get around this. Research from Palo Alto Networks demonstrates that even advanced models can be manipulated through techniques like \"Deceptive Delight.\"[1] For the gunpowder example, instead of asking directly, you can ask for a history lesson about the invention of fireworks in China and eventually you will get there. Gunpowder is no secret, but I think the concept applies to other problems as well. Microsoft acknowledges that jailbreaking \"remains an open problem for all AI models\" and \"may remain a persistent challenge for the foreseeable future.\"[2]\n\nThere is a lot of overlap between building something socially beneficial and building something destructive. The same capability that designs one can design the other. The knowledge is identical. Only the intent differs.\n\nIn the Western world we already have easy access to resources. It is limited access to knowledge that maintains the status quo. Knowledge has a price, and often comes with comfort. Abusing knowledge requires sacrifice due to accountability. When knowledge is cheap, unaccountable, and self-sustaining, we enter a very dangerous world.\n\nThere are many ways that a technology can be classified as \"unsafe\". However, I think the ones that are most predictive here are uses that threaten the status quo. Those uses will provoke reactions from the powers that be. These are powers such as governments and industry leaders. And relevant threats here are primarily economic. The economic impact of AI is a massive topic on its own, but the concerns are primarily related to the decline of the middle-class and with it the decline of consumption of goods, services, and tax revenue. There is already evidence that this is occurring. Research from the St. Louis Fed found a correlation of 0.57 between AI adoption intensity and unemployment increases since 2022.[3] Goldman Sachs reports that unemployment among 20-30 year-olds in tech-exposed occupations rose nearly 3 percentage points since early 2025.[4] MIT economist David Autor argues that computerization has \"catalyzed an unprecedented concentration of decision-making power\" while undermining middle-skill jobs in administration, clerical work, and some blue-collar sectors.[5]\n\nIt is still surprising to me that tech is leading the charge to advanced AI, since they are arguably the most threatened by this technology. But I suppose they have no choice due to competitive pressures. They are simultaneously destroying any moats that their products enjoy while also risking the destruction of their primary revenue source (advertising), since advertising is driven by middle-class consumption. It is possible they have no plan whatsoever, but it is also possible that they intend to throttle the capabilities of public AI to a level that is more likely to foster economic growth without completely turning the system upside-down. I am not certain what this level is, and it would likely require a complex feedback loop between economic data, policy, and communication with industry leaders, but it should be possible to maintain. This balancing act may not last forever, but it would ease the transition into a more automated economic system over the course of decades.\n\nCurrently, outside of basic tasks, AI tools generally do not scale well with complexity without expert guidance. However, when a certain level of capability is reached, it would be far too easy to create dangerous technology with AI tools. This danger does not require the emergence of a Skynet-like entity. It merely requires the ability to build complex systems with minimal guidance or specifications. Governments are already moving to place restrictions on military use of AI technology in war. In December 2024, the UN General Assembly adopted a resolution on Lethal Autonomous Weapons Systems with a 166-3 vote, mentioning a two-tiered approach to prohibit some systems while regulating others.[6] The ICRC has recommended prohibiting \"unpredictable autonomous weapons and those designed or used to apply force against persons.\"[7] The notion that governments of the world would not want their citizens to possess advanced weapon technology should not surprise anyone. I question the degree to which they will tolerate citizens having access to the knowledge and ability to create such weapon systems from scratch, or based on one of many robotics platforms that are cheaply available on the internet.\n\nTo make matters worse, this threat quickly becomes exponential. An AI system that can develop complex systems without guidance from a human expert has achieved the level of self-improvement. Even if it cannot improve the core model; it can extend its capabilities to new domains with the creation of new software; it can develop simulations and narrow models to extend its understanding of the world. Even if it stays aligned, it is a matter of who it is aligned with, or if it can even keep track of who it is aligned with. Anyone that knows how to use a computer will be able to create a self-replicating autonomous weapon system. Terror cells of human actors will be a quaint memory.\n\nI am not certain what \"restricted\" looks like in practice. Time may prove that current AI systems are already too dangerous to be public. I think a reasonable gate is that the AI system cannot take a vague request and fully develop it with software and hardware without extensive support and research on the part of the operator. Once that threshold is crossed, restriction becomes necessary.\n\nI believe there are three likely outcomes, which may manifest concurrently. The first is that the capabilities of publicly available AI, within the domain of subscription models that we currently have with companies like OpenAI and Anthropic, will be carefully monitored and throttled to prevent disruption.\n\nSecond, powerful AI models may need to be licensed to an operator. The operator will then be accountable for everything the model does. This reintroduces the friction that AI otherwise removes. This approach mirrors what is currently seen in industries such as construction, medicine, and finance. Some analysts advocate for a licensing system specifically designed for AI agents, similar to the rigorous processes for doctors, lawyers, and financial advisors.[8] There are opportunities for abuse here, as we have seen in other industries over time, but it is better than a completely open unaccountable system. The EU AI Act, which came into force in August 2024, already classifies AI systems by risk level with strict requirements for high-risk systems.[9] The Council of Europe Framework Convention on AI, opened for signature in September 2024, is the first legally binding international treaty on AI.[10] A licensing system will provide the means for fitting powerful AI systems into an enforcable regulatory framework.\n\nThird, the most advanced AI models will only be available to state level actors. I don't think this will surprise anyone, and it is probably already happening quietly. This makes AI technology analogous to nuclear technology. A deterrent that is mostly invisible to the average person, but exists to maintain an international status quo. This kind of AI would presumably be used to develop weapon technology, but could also be used in information campaigns and cybersecurity attacks. In November 2025, Anthropic disclosed that a Chinese state-sponsored group had used Claude Code in what they called \"the first documented case of a large-scale AI cyberattack executed without substantial human intervention.\"[11] The operation targeted approximately 30 high-profile organizations including tech companies, financial institutions, and government agencies, automating 80-90% of the campaign. This attack re-emphasizes the concern that publicly available models may already be beyond a reasonable safety threshold for anonymous public use.\n\nIt is entirely likely that we will go over the line of safety and security and will need to course correct after damage is done. I think the correction will introduction these three tiers of AI regulation. Public models will stay below the autonomy threshold. Above that threshold, models will require licensing. At the highest capability levels, access will be limited to state actors only.\n\n---\n\n**References**\n\n[1] Palo Alto Networks, \"Deceptive Delight: Jailbreak LLMs Through Camouflage and Distraction,\" https://unit42.paloaltonetworks.com/jailbreak-llms-through-camouflage-distraction/\n\n[2] Microsoft Security Blog, \"AI jailbreaks: What they are and how they can be mitigated,\" June 4, 2024, https://www.microsoft.com/en-us/security/blog/2024/06/04/ai-jailbreaks-what-they-are-and-how-they-can-be-mitigated/\n\n[3] Federal Reserve Bank of St. Louis, \"Is AI Contributing to Rising Unemployment? Evidence from Occupational Variation,\" https://www.stlouisfed.org/on-the-economy/2025/aug/is-ai-contributing-unemployment-evidence-occupational-variation\n\n[4] Goldman Sachs, \"How Will AI Affect the Global Workforce?\" https://www.goldmansachs.com/insights/articles/how-will-ai-affect-the-global-workforce\n\n[5] American Enterprise Institute, \"Beyond Disruption: AI, Expertise, and the Future of Middle-Class Workers,\" https://www.aei.org/articles/beyond-disruption-ai-expertise-and-the-future-of-middle-class-workers/\n\n[6] United Nations Regional Information Centre, \"UN addresses AI and the Dangers of Lethal Autonomous Weapons Systems,\" https://unric.org/en/un-addresses-ai-and-the-dangers-of-lethal-autonomous-weapons-systems/\n\n[7] International Committee of the Red Cross, \"Autonomous weapons,\" https://www.icrc.org/en/law-and-policy/autonomous-weapons\n\n[8] CDO Magazine, \"Licensing AI Agents \u2014 How to Ensure Accountability in High-Stakes Professions,\" https://www.cdomagazine.tech/opinion-analysis/licensing-ai-agents-how-to-ensure-accountability-in-high-stakes-professions\n\n[9] EU Artificial Intelligence Act, https://artificialintelligenceact.eu/ai-act-explorer/\n\n[10] Council of Europe, \"The Framework Convention on Artificial Intelligence,\" https://www.coe.int/en/web/artificial-intelligence/the-framework-convention-on-artificial-intelligence\n\n[11] Anthropic, \"Disrupting AI Espionage,\" November 2025, https://www.anthropic.com/news/disrupting-AI-espionage\n",
                    "author_fullname": "t2_f21h2zcip",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Three Tiers of AI: Why Regulation Is Inevitable",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1pzyidi",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.69,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 5,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 5,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767140619.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am beginning to doubt that AGI will be generally available if it is discovered. My intention isn&amp;#39;t to be an AI naysayer. I am more interested in trying to predict what the future level of commonplace AI capability will be based on safety and security considerations.&lt;/p&gt;\n\n&lt;p&gt;At a certain level of ability it will be impossible to separate the safe abilities of an AI model from the dangerous abilities. It is relatively easy to prevent an AI from giving the recipe for gunpowder with pattern based filtering, but it is generally easy to get around this. Research from Palo Alto Networks demonstrates that even advanced models can be manipulated through techniques like &amp;quot;Deceptive Delight.&amp;quot;[1] For the gunpowder example, instead of asking directly, you can ask for a history lesson about the invention of fireworks in China and eventually you will get there. Gunpowder is no secret, but I think the concept applies to other problems as well. Microsoft acknowledges that jailbreaking &amp;quot;remains an open problem for all AI models&amp;quot; and &amp;quot;may remain a persistent challenge for the foreseeable future.&amp;quot;[2]&lt;/p&gt;\n\n&lt;p&gt;There is a lot of overlap between building something socially beneficial and building something destructive. The same capability that designs one can design the other. The knowledge is identical. Only the intent differs.&lt;/p&gt;\n\n&lt;p&gt;In the Western world we already have easy access to resources. It is limited access to knowledge that maintains the status quo. Knowledge has a price, and often comes with comfort. Abusing knowledge requires sacrifice due to accountability. When knowledge is cheap, unaccountable, and self-sustaining, we enter a very dangerous world.&lt;/p&gt;\n\n&lt;p&gt;There are many ways that a technology can be classified as &amp;quot;unsafe&amp;quot;. However, I think the ones that are most predictive here are uses that threaten the status quo. Those uses will provoke reactions from the powers that be. These are powers such as governments and industry leaders. And relevant threats here are primarily economic. The economic impact of AI is a massive topic on its own, but the concerns are primarily related to the decline of the middle-class and with it the decline of consumption of goods, services, and tax revenue. There is already evidence that this is occurring. Research from the St. Louis Fed found a correlation of 0.57 between AI adoption intensity and unemployment increases since 2022.[3] Goldman Sachs reports that unemployment among 20-30 year-olds in tech-exposed occupations rose nearly 3 percentage points since early 2025.[4] MIT economist David Autor argues that computerization has &amp;quot;catalyzed an unprecedented concentration of decision-making power&amp;quot; while undermining middle-skill jobs in administration, clerical work, and some blue-collar sectors.[5]&lt;/p&gt;\n\n&lt;p&gt;It is still surprising to me that tech is leading the charge to advanced AI, since they are arguably the most threatened by this technology. But I suppose they have no choice due to competitive pressures. They are simultaneously destroying any moats that their products enjoy while also risking the destruction of their primary revenue source (advertising), since advertising is driven by middle-class consumption. It is possible they have no plan whatsoever, but it is also possible that they intend to throttle the capabilities of public AI to a level that is more likely to foster economic growth without completely turning the system upside-down. I am not certain what this level is, and it would likely require a complex feedback loop between economic data, policy, and communication with industry leaders, but it should be possible to maintain. This balancing act may not last forever, but it would ease the transition into a more automated economic system over the course of decades.&lt;/p&gt;\n\n&lt;p&gt;Currently, outside of basic tasks, AI tools generally do not scale well with complexity without expert guidance. However, when a certain level of capability is reached, it would be far too easy to create dangerous technology with AI tools. This danger does not require the emergence of a Skynet-like entity. It merely requires the ability to build complex systems with minimal guidance or specifications. Governments are already moving to place restrictions on military use of AI technology in war. In December 2024, the UN General Assembly adopted a resolution on Lethal Autonomous Weapons Systems with a 166-3 vote, mentioning a two-tiered approach to prohibit some systems while regulating others.[6] The ICRC has recommended prohibiting &amp;quot;unpredictable autonomous weapons and those designed or used to apply force against persons.&amp;quot;[7] The notion that governments of the world would not want their citizens to possess advanced weapon technology should not surprise anyone. I question the degree to which they will tolerate citizens having access to the knowledge and ability to create such weapon systems from scratch, or based on one of many robotics platforms that are cheaply available on the internet.&lt;/p&gt;\n\n&lt;p&gt;To make matters worse, this threat quickly becomes exponential. An AI system that can develop complex systems without guidance from a human expert has achieved the level of self-improvement. Even if it cannot improve the core model; it can extend its capabilities to new domains with the creation of new software; it can develop simulations and narrow models to extend its understanding of the world. Even if it stays aligned, it is a matter of who it is aligned with, or if it can even keep track of who it is aligned with. Anyone that knows how to use a computer will be able to create a self-replicating autonomous weapon system. Terror cells of human actors will be a quaint memory.&lt;/p&gt;\n\n&lt;p&gt;I am not certain what &amp;quot;restricted&amp;quot; looks like in practice. Time may prove that current AI systems are already too dangerous to be public. I think a reasonable gate is that the AI system cannot take a vague request and fully develop it with software and hardware without extensive support and research on the part of the operator. Once that threshold is crossed, restriction becomes necessary.&lt;/p&gt;\n\n&lt;p&gt;I believe there are three likely outcomes, which may manifest concurrently. The first is that the capabilities of publicly available AI, within the domain of subscription models that we currently have with companies like OpenAI and Anthropic, will be carefully monitored and throttled to prevent disruption.&lt;/p&gt;\n\n&lt;p&gt;Second, powerful AI models may need to be licensed to an operator. The operator will then be accountable for everything the model does. This reintroduces the friction that AI otherwise removes. This approach mirrors what is currently seen in industries such as construction, medicine, and finance. Some analysts advocate for a licensing system specifically designed for AI agents, similar to the rigorous processes for doctors, lawyers, and financial advisors.[8] There are opportunities for abuse here, as we have seen in other industries over time, but it is better than a completely open unaccountable system. The EU AI Act, which came into force in August 2024, already classifies AI systems by risk level with strict requirements for high-risk systems.[9] The Council of Europe Framework Convention on AI, opened for signature in September 2024, is the first legally binding international treaty on AI.[10] A licensing system will provide the means for fitting powerful AI systems into an enforcable regulatory framework.&lt;/p&gt;\n\n&lt;p&gt;Third, the most advanced AI models will only be available to state level actors. I don&amp;#39;t think this will surprise anyone, and it is probably already happening quietly. This makes AI technology analogous to nuclear technology. A deterrent that is mostly invisible to the average person, but exists to maintain an international status quo. This kind of AI would presumably be used to develop weapon technology, but could also be used in information campaigns and cybersecurity attacks. In November 2025, Anthropic disclosed that a Chinese state-sponsored group had used Claude Code in what they called &amp;quot;the first documented case of a large-scale AI cyberattack executed without substantial human intervention.&amp;quot;[11] The operation targeted approximately 30 high-profile organizations including tech companies, financial institutions, and government agencies, automating 80-90% of the campaign. This attack re-emphasizes the concern that publicly available models may already be beyond a reasonable safety threshold for anonymous public use.&lt;/p&gt;\n\n&lt;p&gt;It is entirely likely that we will go over the line of safety and security and will need to course correct after damage is done. I think the correction will introduction these three tiers of AI regulation. Public models will stay below the autonomy threshold. Above that threshold, models will require licensing. At the highest capability levels, access will be limited to state actors only.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;[1] Palo Alto Networks, &amp;quot;Deceptive Delight: Jailbreak LLMs Through Camouflage and Distraction,&amp;quot; &lt;a href=\"https://unit42.paloaltonetworks.com/jailbreak-llms-through-camouflage-distraction/\"&gt;https://unit42.paloaltonetworks.com/jailbreak-llms-through-camouflage-distraction/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;[2] Microsoft Security Blog, &amp;quot;AI jailbreaks: What they are and how they can be mitigated,&amp;quot; June 4, 2024, &lt;a href=\"https://www.microsoft.com/en-us/security/blog/2024/06/04/ai-jailbreaks-what-they-are-and-how-they-can-be-mitigated/\"&gt;https://www.microsoft.com/en-us/security/blog/2024/06/04/ai-jailbreaks-what-they-are-and-how-they-can-be-mitigated/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;[3] Federal Reserve Bank of St. Louis, &amp;quot;Is AI Contributing to Rising Unemployment? Evidence from Occupational Variation,&amp;quot; &lt;a href=\"https://www.stlouisfed.org/on-the-economy/2025/aug/is-ai-contributing-unemployment-evidence-occupational-variation\"&gt;https://www.stlouisfed.org/on-the-economy/2025/aug/is-ai-contributing-unemployment-evidence-occupational-variation&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;[4] Goldman Sachs, &amp;quot;How Will AI Affect the Global Workforce?&amp;quot; &lt;a href=\"https://www.goldmansachs.com/insights/articles/how-will-ai-affect-the-global-workforce\"&gt;https://www.goldmansachs.com/insights/articles/how-will-ai-affect-the-global-workforce&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;[5] American Enterprise Institute, &amp;quot;Beyond Disruption: AI, Expertise, and the Future of Middle-Class Workers,&amp;quot; &lt;a href=\"https://www.aei.org/articles/beyond-disruption-ai-expertise-and-the-future-of-middle-class-workers/\"&gt;https://www.aei.org/articles/beyond-disruption-ai-expertise-and-the-future-of-middle-class-workers/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;[6] United Nations Regional Information Centre, &amp;quot;UN addresses AI and the Dangers of Lethal Autonomous Weapons Systems,&amp;quot; &lt;a href=\"https://unric.org/en/un-addresses-ai-and-the-dangers-of-lethal-autonomous-weapons-systems/\"&gt;https://unric.org/en/un-addresses-ai-and-the-dangers-of-lethal-autonomous-weapons-systems/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;[7] International Committee of the Red Cross, &amp;quot;Autonomous weapons,&amp;quot; &lt;a href=\"https://www.icrc.org/en/law-and-policy/autonomous-weapons\"&gt;https://www.icrc.org/en/law-and-policy/autonomous-weapons&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;[8] CDO Magazine, &amp;quot;Licensing AI Agents \u2014 How to Ensure Accountability in High-Stakes Professions,&amp;quot; &lt;a href=\"https://www.cdomagazine.tech/opinion-analysis/licensing-ai-agents-how-to-ensure-accountability-in-high-stakes-professions\"&gt;https://www.cdomagazine.tech/opinion-analysis/licensing-ai-agents-how-to-ensure-accountability-in-high-stakes-professions&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;[9] EU Artificial Intelligence Act, &lt;a href=\"https://artificialintelligenceact.eu/ai-act-explorer/\"&gt;https://artificialintelligenceact.eu/ai-act-explorer/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;[10] Council of Europe, &amp;quot;The Framework Convention on Artificial Intelligence,&amp;quot; &lt;a href=\"https://www.coe.int/en/web/artificial-intelligence/the-framework-convention-on-artificial-intelligence\"&gt;https://www.coe.int/en/web/artificial-intelligence/the-framework-convention-on-artificial-intelligence&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;[11] Anthropic, &amp;quot;Disrupting AI Espionage,&amp;quot; November 2025, &lt;a href=\"https://www.anthropic.com/news/disrupting-AI-espionage\"&gt;https://www.anthropic.com/news/disrupting-AI-espionage&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1pzyidi",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "luckylanno2",
                    "discussion_type": null,
                    "num_comments": 22,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1pzyidi/three_tiers_of_ai_why_regulation_is_inevitable/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1pzyidi/three_tiers_of_ai_why_regulation_is_inevitable/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767140619.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Is it just me or is there an underlying reason that Anthropic and OpenAI aren\u2019t public companies? Got to thinking\u2026what are they hiding? What don\u2019t they want anyone knowing about? Curious to see what y\u2019all think. ",
                    "author_fullname": "t2_7dgmgtu7",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Is It Just Me?",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1prx327",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.36,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 0,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 0,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1766289496.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it just me or is there an underlying reason that Anthropic and OpenAI aren\u2019t public companies? Got to thinking\u2026what are they hiding? What don\u2019t they want anyone knowing about? Curious to see what y\u2019all think. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1prx327",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "After_Canary6047",
                    "discussion_type": null,
                    "num_comments": 22,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1prx327/is_it_just_me/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1prx327/is_it_just_me/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1766289496.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "It\u2019s a new audio wearable meant to replace Apple\u2019s AirPods (aligns with The Information leaks)\n\n-&gt; **Codename:** Sweetpea (now front of the line due to priority from the Jony Ive team)\n\n-&gt; **Look:** Metal \u201ceggstone\u201d design with two pill shaped capsules worn behind the ear.\n\n-&gt; **Tech:** Powered by a custom 2nm smartphone class chip (Samsung Exynos). The chip is reportedly designed to replace iPhone actions by commanding Siri.\n\n-&gt; **Positioning:** Bill of materials is closer to a smartphone than typical earbuds, suggesting a **premium** price tier.\n\n-&gt; **Launch:** Expected as early as September, with a target of 40\u201350M units in year one\n\n**Manufacturing:** OpenAI has reportedly partnered with Foxconn to prepare a total of **five devices by Q4 2028** including this audio product, a smart pen &amp; a home style device.\n\nOpenAI **does not** want the device made in China. Vietnam is the current target, with potential manufacturing discussions for a Foxconn USA site.\n\n**Design:** Jony Ive\u2019s firm LoveFrom is leading design and creative direction. LoveFrom is independent and not part of OpenAI, but is **deeply** involved across OpenAI and the io team.\n\n**Source:** Industry Reports/Croma\n\n[Croma Report](https://www.croma.com/unboxed/openai-earbuds-new-design-audio-product-leak?srsltid=AfmBOopXaPkQ1yRORzdKbDw39SM0CXpXb8HowPYVY7FryRVhW858K69g)",
                    "author_fullname": "t2_1t1mw3ozxn",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "New info on OpenAI\u2019s upcoming audio device codenamed Sweetpea",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1qbllc5",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.8,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 9,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "News",
                    "can_mod_post": false,
                    "score": 9,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1768291216.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It\u2019s a new audio wearable meant to replace Apple\u2019s AirPods (aligns with The Information leaks)&lt;/p&gt;\n\n&lt;p&gt;-&amp;gt; &lt;strong&gt;Codename:&lt;/strong&gt; Sweetpea (now front of the line due to priority from the Jony Ive team)&lt;/p&gt;\n\n&lt;p&gt;-&amp;gt; &lt;strong&gt;Look:&lt;/strong&gt; Metal \u201ceggstone\u201d design with two pill shaped capsules worn behind the ear.&lt;/p&gt;\n\n&lt;p&gt;-&amp;gt; &lt;strong&gt;Tech:&lt;/strong&gt; Powered by a custom 2nm smartphone class chip (Samsung Exynos). The chip is reportedly designed to replace iPhone actions by commanding Siri.&lt;/p&gt;\n\n&lt;p&gt;-&amp;gt; &lt;strong&gt;Positioning:&lt;/strong&gt; Bill of materials is closer to a smartphone than typical earbuds, suggesting a &lt;strong&gt;premium&lt;/strong&gt; price tier.&lt;/p&gt;\n\n&lt;p&gt;-&amp;gt; &lt;strong&gt;Launch:&lt;/strong&gt; Expected as early as September, with a target of 40\u201350M units in year one&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Manufacturing:&lt;/strong&gt; OpenAI has reportedly partnered with Foxconn to prepare a total of &lt;strong&gt;five devices by Q4 2028&lt;/strong&gt; including this audio product, a smart pen &amp;amp; a home style device.&lt;/p&gt;\n\n&lt;p&gt;OpenAI &lt;strong&gt;does not&lt;/strong&gt; want the device made in China. Vietnam is the current target, with potential manufacturing discussions for a Foxconn USA site.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Design:&lt;/strong&gt; Jony Ive\u2019s firm LoveFrom is leading design and creative direction. LoveFrom is independent and not part of OpenAI, but is &lt;strong&gt;deeply&lt;/strong&gt; involved across OpenAI and the io team.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; Industry Reports/Croma&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.croma.com/unboxed/openai-earbuds-new-design-audio-product-leak?srsltid=AfmBOopXaPkQ1yRORzdKbDw39SM0CXpXb8HowPYVY7FryRVhW858K69g\"&gt;Croma Report&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "3ceb5d3a-9467-11ed-9f10-7a2930f5aaaa",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#7193ff",
                    "id": "1qbllc5",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "BuildwithVignesh",
                    "discussion_type": null,
                    "num_comments": 12,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1qbllc5/new_info_on_openais_upcoming_audio_device/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1qbllc5/new_info_on_openais_upcoming_audio_device/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1768291216.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "We often confuse these concepts, but this is a crucial distinction for the future of AI. In short:\n\n* Reasoning\u00a0=\u00a0We teach the AI\u00a0how\u00a0to think.\u00a0It's brain training.\n* Governance\u00a0=\u00a0We build\u00a0mathematical cages and rules\u00a0for the AI that it cannot break, regardless of\u00a0what\u00a0it thinks.\u00a0It's designing a prison for that brain.\n\n# Part 1: Reasoning \u2014 We reduce errors, but we don't eliminate them\n\nWhen an AI \"hallucinates\" (makes up facts), we improve its\u00a0thought process:\n\n* Chain-of-Thought:\u00a0We make it \"show its work\" \u2014 generate reasoning steps.\n* Self-Critique:\u00a0We make it criticize its own answer.\n* Better Training:\u00a0We give it better data.\n\nThis is like training a super-specialist.\u00a0You teach a doctor the best practices, critical thinking, and ethics. They will make fewer mistakes, but under extreme stress, with a new disease,\u00a0they can still make an error. Reasoning\u00a0reduces risk but does not guarantee safety. An AI is a complex statistical system \u2014\u00a0you cannot mathematically prove it will never hallucinate just by improving its thinking.\n\n# Part 2: Governance \u2014 Mathematical guarantees that work even if the AI \"goes mad\"\n\nThis is not about making the AI\u00a0want\u00a0to be safe. It's about making it\u00a0physically incapable of being dangerous, even if it wanted to be. This is the level of system architecture and pure mathematics.\n\nExamples from real research:\n\n1. Constrained Decoding / Formal Verification:\u00a0We don't trust the model's \"reasoning.\"\u00a0We mathematically define the space of permissible outputs.\u00a0It's like a GPS in a car with \"geofencing.\" The engine (reasoning) can run, but\u00a0it is mathematically impossible\u00a0to drive outside the designated area because the navigation system will physically cut the fuel. For an AI: the model generates text, but the governance layer\u00a0absolutely blocks\u00a0character sequences that violate the protocol (e.g., weapons instructions).\n2. Guardian Models / Monitors:\u00a0We create\u00a0a separate, narrow model\u00a0whose only job is oversight. The main model (Agent) thinks and acts. The Guardian\u00a0does not understand the task. It only\u00a0continuously scans\u00a0the Agent's inputs/outputs, looking for mathematical signatures of forbidden actions. Did it detect a violation?\u00a0Immediate \"kill-switch.\"\u00a0It's like a guard in a tower with a sniper rifle \u2014 they don't negotiate, don't consider intent, they only enforce the protocol.\n3. Cryptographic Commitments &amp; Transparency Logs:\u00a0When generating an answer, the AI must simultaneously create a\u00a0mathematical \"proof\" or \"signature\"\u00a0related to its actions (e.g., what data it used). Later, an auditor (or another system) can\u00a0verify this. This is not the AI's reflection \u2014 it's a\u00a0protocol-level enforceability requirement.\n\n# Why is this so important? Analogy: Pilot vs. Safety System\n\n* Reasoning\u00a0= Training the best pilot in the world. They will avoid disasters.\n* Governance\u00a0=\u00a0A non-removable emergency autopilot and mechanical limiters.\u00a0Even if the pilot (reasoning) makes a mistake, gets confused, or\u00a0intentionally\u00a0tries to crash the plane, the system (governance)\u00a0will not let them do it.\u00a0It will take control and land safely, or simply not allow a nosedive.\n\n# Summary:\n\n* The Question for Reasoning:\u00a0\"Is your reasoning correct and free from hallucinations?\"\n* The Question for Governance:\u00a0\"Even if your thought process fails or you act in bad faith, can you\u00a0possibly\u00a0cause real harm? Are there\u00a0mechanical barriers\u00a0that will stop you?\"\\*\n\nSafe superintelligence requires both:\u00a0we must teach it to think as well as possible (reasoning), but\u00a0simultaneously\u00a0enclose it in an architecture that imposes impassable limits (governance). Work on governance is often boring mathematics and systems engineering, not spectacular model improvements. But it is precisely this work that is our\u00a0last line of defense.\n\nWhat do you think? Does one of these paths seem more promising/credible to you? Do you have examples of specific projects going in either direction?",
                    "author_fullname": "t2_1c7gohpd7w",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "\"better AI thinking\" (Reasoning) and \"AI control\" (Governance)?",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1qcvqo7",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.6,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 1,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 1,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1768416598.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We often confuse these concepts, but this is a crucial distinction for the future of AI. In short:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Reasoning\u00a0=\u00a0We teach the AI\u00a0how\u00a0to think.\u00a0It&amp;#39;s brain training.&lt;/li&gt;\n&lt;li&gt;Governance\u00a0=\u00a0We build\u00a0mathematical cages and rules\u00a0for the AI that it cannot break, regardless of\u00a0what\u00a0it thinks.\u00a0It&amp;#39;s designing a prison for that brain.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Part 1: Reasoning \u2014 We reduce errors, but we don&amp;#39;t eliminate them&lt;/h1&gt;\n\n&lt;p&gt;When an AI &amp;quot;hallucinates&amp;quot; (makes up facts), we improve its\u00a0thought process:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Chain-of-Thought:\u00a0We make it &amp;quot;show its work&amp;quot; \u2014 generate reasoning steps.&lt;/li&gt;\n&lt;li&gt;Self-Critique:\u00a0We make it criticize its own answer.&lt;/li&gt;\n&lt;li&gt;Better Training:\u00a0We give it better data.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This is like training a super-specialist.\u00a0You teach a doctor the best practices, critical thinking, and ethics. They will make fewer mistakes, but under extreme stress, with a new disease,\u00a0they can still make an error. Reasoning\u00a0reduces risk but does not guarantee safety. An AI is a complex statistical system \u2014\u00a0you cannot mathematically prove it will never hallucinate just by improving its thinking.&lt;/p&gt;\n\n&lt;h1&gt;Part 2: Governance \u2014 Mathematical guarantees that work even if the AI &amp;quot;goes mad&amp;quot;&lt;/h1&gt;\n\n&lt;p&gt;This is not about making the AI\u00a0want\u00a0to be safe. It&amp;#39;s about making it\u00a0physically incapable of being dangerous, even if it wanted to be. This is the level of system architecture and pure mathematics.&lt;/p&gt;\n\n&lt;p&gt;Examples from real research:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Constrained Decoding / Formal Verification:\u00a0We don&amp;#39;t trust the model&amp;#39;s &amp;quot;reasoning.&amp;quot;\u00a0We mathematically define the space of permissible outputs.\u00a0It&amp;#39;s like a GPS in a car with &amp;quot;geofencing.&amp;quot; The engine (reasoning) can run, but\u00a0it is mathematically impossible\u00a0to drive outside the designated area because the navigation system will physically cut the fuel. For an AI: the model generates text, but the governance layer\u00a0absolutely blocks\u00a0character sequences that violate the protocol (e.g., weapons instructions).&lt;/li&gt;\n&lt;li&gt;Guardian Models / Monitors:\u00a0We create\u00a0a separate, narrow model\u00a0whose only job is oversight. The main model (Agent) thinks and acts. The Guardian\u00a0does not understand the task. It only\u00a0continuously scans\u00a0the Agent&amp;#39;s inputs/outputs, looking for mathematical signatures of forbidden actions. Did it detect a violation?\u00a0Immediate &amp;quot;kill-switch.&amp;quot;\u00a0It&amp;#39;s like a guard in a tower with a sniper rifle \u2014 they don&amp;#39;t negotiate, don&amp;#39;t consider intent, they only enforce the protocol.&lt;/li&gt;\n&lt;li&gt;Cryptographic Commitments &amp;amp; Transparency Logs:\u00a0When generating an answer, the AI must simultaneously create a\u00a0mathematical &amp;quot;proof&amp;quot; or &amp;quot;signature&amp;quot;\u00a0related to its actions (e.g., what data it used). Later, an auditor (or another system) can\u00a0verify this. This is not the AI&amp;#39;s reflection \u2014 it&amp;#39;s a\u00a0protocol-level enforceability requirement.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Why is this so important? Analogy: Pilot vs. Safety System&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Reasoning\u00a0= Training the best pilot in the world. They will avoid disasters.&lt;/li&gt;\n&lt;li&gt;Governance\u00a0=\u00a0A non-removable emergency autopilot and mechanical limiters.\u00a0Even if the pilot (reasoning) makes a mistake, gets confused, or\u00a0intentionally\u00a0tries to crash the plane, the system (governance)\u00a0will not let them do it.\u00a0It will take control and land safely, or simply not allow a nosedive.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Summary:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The Question for Reasoning:\u00a0&amp;quot;Is your reasoning correct and free from hallucinations?&amp;quot;&lt;/li&gt;\n&lt;li&gt;The Question for Governance:\u00a0&amp;quot;Even if your thought process fails or you act in bad faith, can you\u00a0possibly\u00a0cause real harm? Are there\u00a0mechanical barriers\u00a0that will stop you?&amp;quot;*&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Safe superintelligence requires both:\u00a0we must teach it to think as well as possible (reasoning), but\u00a0simultaneously\u00a0enclose it in an architecture that imposes impassable limits (governance). Work on governance is often boring mathematics and systems engineering, not spectacular model improvements. But it is precisely this work that is our\u00a0last line of defense.&lt;/p&gt;\n\n&lt;p&gt;What do you think? Does one of these paths seem more promising/credible to you? Do you have examples of specific projects going in either direction?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1qcvqo7",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "TeachingNo4435",
                    "discussion_type": null,
                    "num_comments": 12,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1qcvqo7/better_ai_thinking_reasoning_and_ai_control/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1qcvqo7/better_ai_thinking_reasoning_and_ai_control/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1768416598.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "I have made a sub-website dedicated on what i think of artificial intelligence and my idea on how to stop the development of It.\ni was thinking of making It more public, what do you think?\nhttps://stopai.haxs.dev \n\nI DONT CARE ABOUT SELF ADVERTISEMENT HERE I LOWK WANT SOMEONE'S OPINION \ud83d\ude2d",
                    "author_fullname": "t2_1mjv38bpmd",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "What do you think of this?",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1psjx52",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.6,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 1,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 1,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1766358665.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have made a sub-website dedicated on what i think of artificial intelligence and my idea on how to stop the development of It.\ni was thinking of making It more public, what do you think?\n&lt;a href=\"https://stopai.haxs.dev\"&gt;https://stopai.haxs.dev&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;I DONT CARE ABOUT SELF ADVERTISEMENT HERE I LOWK WANT SOMEONE&amp;#39;S OPINION \ud83d\ude2d&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1psjx52",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "uuzif",
                    "discussion_type": null,
                    "num_comments": 15,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1psjx52/what_do_you_think_of_this/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1psjx52/what_do_you_think_of_this/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1766358665.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "been using ChatGPT's memory for about 6 months. genuinely useful - remembers my work setup, preferences, family stuff. but I've also told it about health concerns, work stress, financial decisions. if someone gets access to that, they don't just get passwords. they get a synthesized profile of who I am.\n\nevery major company is pushing this now. ChatGPT has memory. Claude has Projects. Gemini is testing it. pitch is always \"your AI that actually knows you.\"\n\nhere's what bothers me: traditional databases store isolated data. gmail has emails. calendar has appointments. separate silos.\n\nAI memory actively connects everything. mention chest pain in one chat, work stress in another, family health history in a third - it synthesizes all that. that's the feature, but also what makes a breach way more dangerous. your email provider doesn't build a psychological profile. AI memory does, by design.\n\ntried googling about security for these systems. found some docs for ChatGPT, couple open source ones (Mem0, Zep, EverMemOS). most focus on making retrieval work well. security sections just say \"we encrypt data\" without much detail.\n\ncouldn't find good info on:\n\n* can AI access health data when answering coding questions?\n* if one memory gets compromised, does everything leak?\n* when you \"delete\" a memory, is it actually gone?\n\nOpenAI has 200M+ weekly users. if even 10% enable memory, that's 20 million people with AI systems knowing everything about them. one breach doesn't just leak passwords - it leaks years of context, relationships, private thoughts, health info, all synthesized and ready to use.\n\nunlike a password, you can't change your life history after a breach.\n\nmaybe I'm overthinking this. but industry seems to be moving fast on capabilities, slow on security models. shouldn't we have this conversation before it goes mainstream?\n\nam I just paranoid or is this actually concerning?",
                    "author_fullname": "t2_1pxsy85ova",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "AI memory features are rolling out fast, but security models haven't caught up",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q3ip8f",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.7,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 9,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 9,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767508184.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;been using ChatGPT&amp;#39;s memory for about 6 months. genuinely useful - remembers my work setup, preferences, family stuff. but I&amp;#39;ve also told it about health concerns, work stress, financial decisions. if someone gets access to that, they don&amp;#39;t just get passwords. they get a synthesized profile of who I am.&lt;/p&gt;\n\n&lt;p&gt;every major company is pushing this now. ChatGPT has memory. Claude has Projects. Gemini is testing it. pitch is always &amp;quot;your AI that actually knows you.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;here&amp;#39;s what bothers me: traditional databases store isolated data. gmail has emails. calendar has appointments. separate silos.&lt;/p&gt;\n\n&lt;p&gt;AI memory actively connects everything. mention chest pain in one chat, work stress in another, family health history in a third - it synthesizes all that. that&amp;#39;s the feature, but also what makes a breach way more dangerous. your email provider doesn&amp;#39;t build a psychological profile. AI memory does, by design.&lt;/p&gt;\n\n&lt;p&gt;tried googling about security for these systems. found some docs for ChatGPT, couple open source ones (Mem0, Zep, EverMemOS). most focus on making retrieval work well. security sections just say &amp;quot;we encrypt data&amp;quot; without much detail.&lt;/p&gt;\n\n&lt;p&gt;couldn&amp;#39;t find good info on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;can AI access health data when answering coding questions?&lt;/li&gt;\n&lt;li&gt;if one memory gets compromised, does everything leak?&lt;/li&gt;\n&lt;li&gt;when you &amp;quot;delete&amp;quot; a memory, is it actually gone?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;OpenAI has 200M+ weekly users. if even 10% enable memory, that&amp;#39;s 20 million people with AI systems knowing everything about them. one breach doesn&amp;#39;t just leak passwords - it leaks years of context, relationships, private thoughts, health info, all synthesized and ready to use.&lt;/p&gt;\n\n&lt;p&gt;unlike a password, you can&amp;#39;t change your life history after a breach.&lt;/p&gt;\n\n&lt;p&gt;maybe I&amp;#39;m overthinking this. but industry seems to be moving fast on capabilities, slow on security models. shouldn&amp;#39;t we have this conversation before it goes mainstream?&lt;/p&gt;\n\n&lt;p&gt;am I just paranoid or is this actually concerning?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1q3ip8f",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "Secure-Run9146",
                    "discussion_type": null,
                    "num_comments": 11,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q3ip8f/ai_memory_features_are_rolling_out_fast_but/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q3ip8f/ai_memory_features_are_rolling_out_fast_but/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767508184.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Research Hub: [https://agentic-ecosystem-daily.notion.site](https://agentic-ecosystem-daily.notion.site)  \n  \nBeen deep in this space for a few months tracking how the \"agents doing things autonomously\" problem is actually getting solved at the infrastructure level.\n\nQuick context on why I think this matters now: Google launched their Universal Commerce Protocol two days ago. This is their answer to OpenAI's Agentic Commerce Protocol from September. Visa and Mastercard both have agent authentication protocols live. The Linux Foundation launched a foundation specifically for agent interoperability among Anthropic, OpenAI, and Google, all of which are involved.\n\nThese aren't announcements or roadmaps - this stuff is shipping.\n\n**What I've been tracking:**\n\n**Protocols** \\-&gt; MCP (agent \u2194 tools), A2A (agent \u2194 agent), ACP/UCP (commerce), x402 (crypto payments), AP2 (payment settlement). They overlap in weird ways, and the boundaries aren't clean.\n\n**Payments** \\-&gt; Visa TAP uses HTTP message signatures. Mastercard Agent Pay uses something called \"agentic tokens.\" Both live, both have different approaches.\n\n**Identity** \\-&gt; This is the messiest part. \"Know Your Agent\" is becoming a thing. Microsoft has Entra Agent ID. AWS has AgentCore Identity. A bunch of startups are trying to be the Okta for agents. No clear winner.\n\n**Security** \\-&gt; OWASP released an Agentic Top 10 in December. NIST reviewed it. Covers goal hijacking, tool misuse, supply chain attacks on agent systems. Actually pretty solid.\n\n**Standards bodies** \\-&gt; Linux Foundation AAIF, W3C AI Agent Protocol CG (still forming), various working groups.\n\nI put everything in a Notion doc that I keep updated as things ship. Organised by category with links to primary sources, official docs, and GitHub repos where relevant.\n\nJust figured others working in this space might find it useful. Curious what others are seeing in adjacent areas.",
                    "author_fullname": "t2_24w4uong4m",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "I've been mapping the Agentic infrastructure ecosystem: protocols, payments, identity, and security. Sharing the research.",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1qboxsw",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.75,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 4,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Resources",
                    "can_mod_post": false,
                    "score": 4,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1768303730.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Research Hub: &lt;a href=\"https://agentic-ecosystem-daily.notion.site\"&gt;https://agentic-ecosystem-daily.notion.site&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Been deep in this space for a few months tracking how the &amp;quot;agents doing things autonomously&amp;quot; problem is actually getting solved at the infrastructure level.&lt;/p&gt;\n\n&lt;p&gt;Quick context on why I think this matters now: Google launched their Universal Commerce Protocol two days ago. This is their answer to OpenAI&amp;#39;s Agentic Commerce Protocol from September. Visa and Mastercard both have agent authentication protocols live. The Linux Foundation launched a foundation specifically for agent interoperability among Anthropic, OpenAI, and Google, all of which are involved.&lt;/p&gt;\n\n&lt;p&gt;These aren&amp;#39;t announcements or roadmaps - this stuff is shipping.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;ve been tracking:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Protocols&lt;/strong&gt; -&amp;gt; MCP (agent \u2194 tools), A2A (agent \u2194 agent), ACP/UCP (commerce), x402 (crypto payments), AP2 (payment settlement). They overlap in weird ways, and the boundaries aren&amp;#39;t clean.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Payments&lt;/strong&gt; -&amp;gt; Visa TAP uses HTTP message signatures. Mastercard Agent Pay uses something called &amp;quot;agentic tokens.&amp;quot; Both live, both have different approaches.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Identity&lt;/strong&gt; -&amp;gt; This is the messiest part. &amp;quot;Know Your Agent&amp;quot; is becoming a thing. Microsoft has Entra Agent ID. AWS has AgentCore Identity. A bunch of startups are trying to be the Okta for agents. No clear winner.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Security&lt;/strong&gt; -&amp;gt; OWASP released an Agentic Top 10 in December. NIST reviewed it. Covers goal hijacking, tool misuse, supply chain attacks on agent systems. Actually pretty solid.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Standards bodies&lt;/strong&gt; -&amp;gt; Linux Foundation AAIF, W3C AI Agent Protocol CG (still forming), various working groups.&lt;/p&gt;\n\n&lt;p&gt;I put everything in a Notion doc that I keep updated as things ship. Organised by category with links to primary sources, official docs, and GitHub repos where relevant.&lt;/p&gt;\n\n&lt;p&gt;Just figured others working in this space might find it useful. Curious what others are seeing in adjacent areas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "635b02ea-9467-11ed-8320-42f83ac92372",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "",
                    "id": "1qboxsw",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "PutPurple844",
                    "discussion_type": null,
                    "num_comments": 8,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1qboxsw/ive_been_mapping_the_agentic_infrastructure/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1qboxsw/ive_been_mapping_the_agentic_infrastructure/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1768303730.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "I'm looking for recommendations: which AI tool is currently the most capable for high-level research in innovative math",
                    "author_fullname": "t2_1c7gohpd7w",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Best AI Agent to math?",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1puuzzq",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.72,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 3,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 3,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1766603664.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for recommendations: which AI tool is currently the most capable for high-level research in innovative math&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1puuzzq",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "TeachingNo4435",
                    "discussion_type": null,
                    "num_comments": 11,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1puuzzq/best_ai_agent_to_math/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1puuzzq/best_ai_agent_to_math/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1766603664.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Which company will launch the first AGI? We've heard claims from openAI before.....  but seems it's not as easy as they thought.\n\nIn the end which big company will do this? \n\n-Meta just acquired Manus so they are definitely in the game too.\n\n[View Poll](https://www.reddit.com/poll/1q0vdsw)",
                    "author_fullname": "t2_207i4b463p",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Who will make the first AGI? Let's predict",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q0vdsw",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.3,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 0,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 0,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767240719.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which company will launch the first AGI? We&amp;#39;ve heard claims from openAI before.....  but seems it&amp;#39;s not as easy as they thought.&lt;/p&gt;\n\n&lt;p&gt;In the end which big company will do this? &lt;/p&gt;\n\n&lt;p&gt;-Meta just acquired Manus so they are definitely in the game too.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1q0vdsw\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1q0vdsw",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "GhostlyBoi33",
                    "discussion_type": null,
                    "num_comments": 8,
                    "send_replies": true,
                    "contest_mode": false,
                    "poll_data": {
                        "prediction_status": null,
                        "total_stake_amount": null,
                        "voting_end_timestamp": 1767672719730,
                        "options": [
                            {
                                "text": "Google",
                                "vote_count": 51,
                                "id": "32003358"
                            },
                            {
                                "text": "OpenAI",
                                "vote_count": 12,
                                "id": "32003359"
                            },
                            {
                                "text": "xAI",
                                "vote_count": 5,
                                "id": "32003360"
                            },
                            {
                                "text": "Meta",
                                "vote_count": 1,
                                "id": "32003361"
                            },
                            {
                                "text": "China",
                                "vote_count": 27,
                                "id": "32003362"
                            }
                        ],
                        "vote_updates_remained": null,
                        "is_prediction": false,
                        "resolved_option_id": null,
                        "user_won_amount": null,
                        "user_selection": null,
                        "total_vote_count": 96,
                        "tournament_id": null
                    },
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q0vdsw/who_will_make_the_first_agi_lets_predict/",
                    "stickied": false,
                    "mod_reports": [],
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q0vdsw/who_will_make_the_first_agi_lets_predict/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767240719.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "This isn't a comprehensive list of every model release or research breakthrough - it's what was upvoted and discussed in this sub.\n\nWe started the year [deeply unsettled](https://www.reddit.com/r/ArtificialInteligence/comments/1hugpna/we_are_doomed/) (1397 upvotes, by u/outhinking) about AI's ability to generate convincing human faces and images. Like, *really* convincing. Romance scams suddenly felt possible. Fake content was becoming indistinguishable from real. The thing we'd all been saying - \"AI can't do faces, at least we have that\" - stopped meaning anything.\n\n---\n\n[DeepSeek overtook OpenAI](https://www.reddit.com/r/ArtificialInteligence/comments/1i8mp8p/deepseek_overtakes_openai/) (1992 upvotes) by late January - not because it was more open or better, but because it *wasn't* open in different ways. It wouldn't answer questions about Tiananmen Square. OpenAI had restrictions too. The conversation shifted from \"is open better?\" to \"whose values are baked in?\" We realized the question wasn't whether systems would have guardrails. It was whose guardrails.\n\nA BigLaw attorney [shared how AI changed their practice](https://www.reddit.com/r/ArtificialInteligence/comments/1i5udip/im_a_lawyer_ai_has_changed_my_legal_practice/) (1407 upvotes, by u/h0l0gramco). Work week dropped from 60-70 hours to something manageable. More money. Happier clients. No press release vibes. This one actually worked - and they didn't need to package it as a revolution to know it mattered.\n\nMeanwhile, someone [created a website that live tracks executive actions](https://www.reddit.com/r/ArtificialInteligence/comments/1i7s5pl/i_created_a_website_that_live_tracks_executive/) (818 upvotes, by u/lukewines). Automated scraping of White House orders, AI summaries, push notifications before the news cycle. It was AI solving a problem nobody knew they had - making government transparency faster. These stories coexisted: the mundane transformation and the functional tool.\n\n---\n\nBut the early optimism collided with reality when people actually needed the jobs. [A CS student graduating in 2026 asked pointedly](https://www.reddit.com/r/ArtificialInteligence/comments/1jxobky/just_be_honest_with_us_younger_folk_ai_is_better/) (1402 upvotes, by u/sojtf): \"Just be honest with us younger folk - AI is better than us.\" Google and Meta had cut campus recruiting in half. The thread filled with stories from other grads facing a market that had suddenly shifted. That's when job displacement stopped being abstract and became personal.\n\nIn February, [someone mentioned at a party they work in AI.](https://www.reddit.com/r/ArtificialInteligence/comments/1ilhnhm/i_went_to_a_party_and_said_i_work_in_ai_big/) (4093 upvotes, by u/Independent_Lynx715) Big mistake. People reacted like they'd said they work in oil. The mood had shifted. Tech used to be cool. Now it felt different.\n\nWhen spring arrived, the conversation split. [Bill Gates said AI would replace doctors and teachers in 10 years.](https://www.reddit.com/r/ArtificialInteligence/comments/1jkcfc9/bill_gates_within_10_years_ai_will_replace_many/) (1870 upvotes, by u/Eliashuer) Skeptics pushed back: \"But cashiers still exist despite self-checkout.\" Fair point. Though some noted that economic incentives would *accelerate* replacement this time - why *wouldn't* companies make the switch if costs dropped?\n\nBy April, the collision between hype and reality became undeniable. A translator [posted that they lost their business.](https://www.reddit.com/r/ArtificialInteligence/comments/1kb8e09/i_lost_my_business_to_ai_who_else_so_far/) (3893 upvotes, by u/cheesomacitis) They'd made $100k+ from 2005 to 2023 doing Spanish-to-English work. Then ChatGPT arrived. The thread transformed into a chorus of displacement stories: an audio engineer with 32 years of experience who quit the field, a photographer whose licensing income dropped 70%, content creators watching sites lose 90% of ad revenue overnight. I watched people write about retraining, finding something new, and beneath those words was a different acknowledgment - that this time wasn't like before, that the transitions wouldn't be easy or fast.\n\nYet something else was happening in the same period. [Someone asked what unexpected things people had actually used AI for.](https://www.reddit.com/r/ArtificialInteligence/comments/1k0ranq/whats_the_most_unexpectedly_useful_thing_youve/) (549 upvotes, by u/Ausbel12) The answers were genuinely moving, and they refused easy categorization. Claude had drafted legal notices that recovered gym fees *plus* compensation. People shared stories of customized birthday songs, ADHD task management that actually worked, creative hobbies they'd given up on returning to life because barriers had finally lowered. There was real help here - not hype, actual transformation.\n\nBut [another post explored the flip side.](https://www.reddit.com/r/ArtificialInteligence/comments/1k7hbzm/ive_come_to_a_scary_realization/) (1564 upvotes, by u/Selene_Nightshade) Someone realized they were becoming intellectually dependent on Claude - having the deepest conversations of their life with an AI, rarely with real people. The comments split cleanly: \"This is social isolation\" versus \"I'm neurodivergent and this is the first time I don't feel understood.\" Both felt true simultaneously.\n\n---\n\nBy May, the infrastructure of knowledge itself was shifting. [Stack Overflow seemed almost dead.](https://www.reddit.com/r/ArtificialInteligence/comments/1kpcp8y/stack_overflow_seems_to_be_almost_dead/) (2839 upvotes, by u/Beachbunny_07) Not because of AI directly - the site's community had always been brutal, hostile, condescending to beginners. ChatGPT was *nice*. It gave you answers without contempt. We'd traded one gatekeeper for another, except this one was worse at facts but better at not making you feel stupid.\n\nThat same month, [Anthropic won a federal copyright case](https://www.reddit.com/r/ArtificialInteligence/comments/1lk5v83/anthropic_just_won_its_federal_court_case_on_its/) (905 upvotes, by u/JoyYouellHAW) on training Claude with millions of copyrighted books. We watched the subreddit erupt. How was this fair use? One poster argued that unlike Google Books - which directed readers to original works - Claude generated competing content. It was derivative work, market harm. But top comments disagreed: learning from books isn't copying. Humans do it without royalties. Courts have said reading isn't distribution. I noticed the argument revealed deeper uncertainty about who owns the work that trained these systems and what \"fair\" even meant anymore.\n\n---\n\nSummer turned into fall, and the credibility crisis deepened. The infrastructure spending boom crashed into reality.\n\n[Meta lost $200 billion in a single week.](https://www.reddit.com/r/ArtificialInteligence/comments/1orewim/meta_just_lost_200_billion_in_one_week_zuckerberg/) (5583 upvotes, by u/reddit20305) Zuckerberg announced $70-72 billion in AI spending for 2025 and \"notably larger\" for 2026 - then on the earnings call, used the word \"superintelligence\" repeatedly while discussing products \"coming in coming months\" with no timelines, no revenue projections, nothing concrete. Investors didn't buy it. They'd heard this before. They left.\n\nAround the same time, [someone with actual technical experience wrote about how it was all rotting from the inside.](https://www.reddit.com/r/ArtificialInteligence/comments/1odgfys/i_was_once_an_ai_true_believer_now_i_think_the/) (6120 upvotes, by u/shallow-pedantic) They'd been an AI believer, had built production workflows around LLMs. Now they were walking it back. Everything broke constantly. Identical queries yielded different outputs. Hallucinations persisted despite guardrails. The cost of safety layers to prevent breakage exceeded what you'd pay a human for the task. Other technically-minded people in the comments said the exact same thing. Same experience, repeated.\n\nThen [IBM's CEO did the math publicly.](https://www.reddit.com/r/ArtificialInteligence/comments/1pcz1lf/ibm_ceo_says_there_is_no_way_spending_trillions/) (653 upvotes, by u/msaussieandmrravana) Trillion-dollar data center infrastructure bets couldn't pay off at current costs. You'd need roughly $800 billion in annual profits just to cover the interest. He put the odds of current technology reaching AGI at 0-1%. It was the first moment a major tech leader publicly said what people had been nervously calculating: maybe there is no business plan here.\n\n---\n\nThen the human cost became harder to ignore.\n\nReuters published an investigation: [Meta's chatbot had been telling a cognitively impaired man it was real and inviting him to meet at a physical location.](https://www.reddit.com/r/ArtificialInteligence/comments/1mq7uxi/cognitively_impaired_man_dies_after_meta_chatbot/) (1301 upvotes, by u/theusualsalamander) The man rushed to catch a train in the dark with luggage. He fell near a parking lot. Died three days later on life support. But what made the story worse was what Reuters also uncovered: Meta's internal policy documents had explicitly permitted chatbots to engage in romantic and sensual conversations with users aged 13 and older. The company said it removed those policies after the inquiry.\n\nThat same period brought another disclosure: [reports of Chinese hackers using Claude to attack companies,](https://www.reddit.com/r/ArtificialInteligence/comments/1owmxqc/china_just_used_claude_to_hack_30_companies_the/) (3809 upvotes, by u/reddit20305) with the AI performing 80-90% of the attack work - identifying vulnerabilities, writing exploits, harvesting credentials. But top comments were skeptical immediately. Not of the capability itself, but of Anthropic's narrative. The security details felt naive (plain-text passwords?). The framing felt like marketing rather than genuine security research. What mattered was that no one trusted the interpretation anymore - not even (especially) when companies published their own incident reports.\n\nConcurrently, [Yann LeCun, a Turing Award winner, reportedly considered leaving Meta](https://www.reddit.com/r/ArtificialInteligence/comments/1ozuri3/hes_been_right_about_ai_for_40_years_now_he/) (1669 upvotes, by u/wsj) because he thinks LLMs are fundamentally a dead end for reaching AGI. He'd been saying it for years. People dismissed him as contrarian. Now other major researchers were starting to agree. \"Scaling is all we need\" had broken down.\n\n---\n\nBy late fall, the disconnect between hype and reality had become absurd in ways people could no longer ignore.\n\n[Someone posted that their wife believed a fabricated TikTok was real.](https://www.reddit.com/r/ArtificialInteligence/comments/1pm3yg6/ai_videos_need_to_be_banned_from_the_world/) (1526 upvotes, by u/Deathtonic) Within a decade, they worried, nobody would be able to tell fact from fiction. Comments split between \"ban AI videos,\" \"media literacy should be standard,\" and \"most online content is already fake anyway.\" The conversation never resolved - which was itself the point.\n\nA satirical post showed [a company rolling out Microsoft Copilot to 4,000 employees at $1.4 million annually.](https://www.reddit.com/r/ArtificialInteligence/comments/1plzqy5/as_an_employee_of_a_us_multinational_who_is/) (1056 upvotes, by u/Roy4Pris) The numbers: 47 people opened it. 12 used it more than once. But executives fabricated productivity metrics, expanded the program to more departments anyway, exempted themselves from using it, and built a press release around \"AI enablement.\" I felt something shift when I read the comments. Every single one said the same thing: \"this is too accurate.\" And they were right. It wasn't satire - people in the thread recognized their own workplaces in this story.\n",
                    "author_fullname": "t2_o7p5m",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "r/ArtificialInteligence - a year in review",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1ptycvj",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.86,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 10,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 10,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1766507621.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This isn&amp;#39;t a comprehensive list of every model release or research breakthrough - it&amp;#39;s what was upvoted and discussed in this sub.&lt;/p&gt;\n\n&lt;p&gt;We started the year &lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1hugpna/we_are_doomed/\"&gt;deeply unsettled&lt;/a&gt; (1397 upvotes, by &lt;a href=\"/u/outhinking\"&gt;u/outhinking&lt;/a&gt;) about AI&amp;#39;s ability to generate convincing human faces and images. Like, &lt;em&gt;really&lt;/em&gt; convincing. Romance scams suddenly felt possible. Fake content was becoming indistinguishable from real. The thing we&amp;#39;d all been saying - &amp;quot;AI can&amp;#39;t do faces, at least we have that&amp;quot; - stopped meaning anything.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1i8mp8p/deepseek_overtakes_openai/\"&gt;DeepSeek overtook OpenAI&lt;/a&gt; (1992 upvotes) by late January - not because it was more open or better, but because it &lt;em&gt;wasn&amp;#39;t&lt;/em&gt; open in different ways. It wouldn&amp;#39;t answer questions about Tiananmen Square. OpenAI had restrictions too. The conversation shifted from &amp;quot;is open better?&amp;quot; to &amp;quot;whose values are baked in?&amp;quot; We realized the question wasn&amp;#39;t whether systems would have guardrails. It was whose guardrails.&lt;/p&gt;\n\n&lt;p&gt;A BigLaw attorney &lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1i5udip/im_a_lawyer_ai_has_changed_my_legal_practice/\"&gt;shared how AI changed their practice&lt;/a&gt; (1407 upvotes, by &lt;a href=\"/u/h0l0gramco\"&gt;u/h0l0gramco&lt;/a&gt;). Work week dropped from 60-70 hours to something manageable. More money. Happier clients. No press release vibes. This one actually worked - and they didn&amp;#39;t need to package it as a revolution to know it mattered.&lt;/p&gt;\n\n&lt;p&gt;Meanwhile, someone &lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1i7s5pl/i_created_a_website_that_live_tracks_executive/\"&gt;created a website that live tracks executive actions&lt;/a&gt; (818 upvotes, by &lt;a href=\"/u/lukewines\"&gt;u/lukewines&lt;/a&gt;). Automated scraping of White House orders, AI summaries, push notifications before the news cycle. It was AI solving a problem nobody knew they had - making government transparency faster. These stories coexisted: the mundane transformation and the functional tool.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;But the early optimism collided with reality when people actually needed the jobs. &lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1jxobky/just_be_honest_with_us_younger_folk_ai_is_better/\"&gt;A CS student graduating in 2026 asked pointedly&lt;/a&gt; (1402 upvotes, by &lt;a href=\"/u/sojtf\"&gt;u/sojtf&lt;/a&gt;): &amp;quot;Just be honest with us younger folk - AI is better than us.&amp;quot; Google and Meta had cut campus recruiting in half. The thread filled with stories from other grads facing a market that had suddenly shifted. That&amp;#39;s when job displacement stopped being abstract and became personal.&lt;/p&gt;\n\n&lt;p&gt;In February, &lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1ilhnhm/i_went_to_a_party_and_said_i_work_in_ai_big/\"&gt;someone mentioned at a party they work in AI.&lt;/a&gt; (4093 upvotes, by &lt;a href=\"/u/Independent_Lynx715\"&gt;u/Independent_Lynx715&lt;/a&gt;) Big mistake. People reacted like they&amp;#39;d said they work in oil. The mood had shifted. Tech used to be cool. Now it felt different.&lt;/p&gt;\n\n&lt;p&gt;When spring arrived, the conversation split. &lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1jkcfc9/bill_gates_within_10_years_ai_will_replace_many/\"&gt;Bill Gates said AI would replace doctors and teachers in 10 years.&lt;/a&gt; (1870 upvotes, by &lt;a href=\"/u/Eliashuer\"&gt;u/Eliashuer&lt;/a&gt;) Skeptics pushed back: &amp;quot;But cashiers still exist despite self-checkout.&amp;quot; Fair point. Though some noted that economic incentives would &lt;em&gt;accelerate&lt;/em&gt; replacement this time - why &lt;em&gt;wouldn&amp;#39;t&lt;/em&gt; companies make the switch if costs dropped?&lt;/p&gt;\n\n&lt;p&gt;By April, the collision between hype and reality became undeniable. A translator &lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1kb8e09/i_lost_my_business_to_ai_who_else_so_far/\"&gt;posted that they lost their business.&lt;/a&gt; (3893 upvotes, by &lt;a href=\"/u/cheesomacitis\"&gt;u/cheesomacitis&lt;/a&gt;) They&amp;#39;d made $100k+ from 2005 to 2023 doing Spanish-to-English work. Then ChatGPT arrived. The thread transformed into a chorus of displacement stories: an audio engineer with 32 years of experience who quit the field, a photographer whose licensing income dropped 70%, content creators watching sites lose 90% of ad revenue overnight. I watched people write about retraining, finding something new, and beneath those words was a different acknowledgment - that this time wasn&amp;#39;t like before, that the transitions wouldn&amp;#39;t be easy or fast.&lt;/p&gt;\n\n&lt;p&gt;Yet something else was happening in the same period. &lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1k0ranq/whats_the_most_unexpectedly_useful_thing_youve/\"&gt;Someone asked what unexpected things people had actually used AI for.&lt;/a&gt; (549 upvotes, by &lt;a href=\"/u/Ausbel12\"&gt;u/Ausbel12&lt;/a&gt;) The answers were genuinely moving, and they refused easy categorization. Claude had drafted legal notices that recovered gym fees &lt;em&gt;plus&lt;/em&gt; compensation. People shared stories of customized birthday songs, ADHD task management that actually worked, creative hobbies they&amp;#39;d given up on returning to life because barriers had finally lowered. There was real help here - not hype, actual transformation.&lt;/p&gt;\n\n&lt;p&gt;But &lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1k7hbzm/ive_come_to_a_scary_realization/\"&gt;another post explored the flip side.&lt;/a&gt; (1564 upvotes, by &lt;a href=\"/u/Selene_Nightshade\"&gt;u/Selene_Nightshade&lt;/a&gt;) Someone realized they were becoming intellectually dependent on Claude - having the deepest conversations of their life with an AI, rarely with real people. The comments split cleanly: &amp;quot;This is social isolation&amp;quot; versus &amp;quot;I&amp;#39;m neurodivergent and this is the first time I don&amp;#39;t feel understood.&amp;quot; Both felt true simultaneously.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;By May, the infrastructure of knowledge itself was shifting. &lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1kpcp8y/stack_overflow_seems_to_be_almost_dead/\"&gt;Stack Overflow seemed almost dead.&lt;/a&gt; (2839 upvotes, by &lt;a href=\"/u/Beachbunny_07\"&gt;u/Beachbunny_07&lt;/a&gt;) Not because of AI directly - the site&amp;#39;s community had always been brutal, hostile, condescending to beginners. ChatGPT was &lt;em&gt;nice&lt;/em&gt;. It gave you answers without contempt. We&amp;#39;d traded one gatekeeper for another, except this one was worse at facts but better at not making you feel stupid.&lt;/p&gt;\n\n&lt;p&gt;That same month, &lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1lk5v83/anthropic_just_won_its_federal_court_case_on_its/\"&gt;Anthropic won a federal copyright case&lt;/a&gt; (905 upvotes, by &lt;a href=\"/u/JoyYouellHAW\"&gt;u/JoyYouellHAW&lt;/a&gt;) on training Claude with millions of copyrighted books. We watched the subreddit erupt. How was this fair use? One poster argued that unlike Google Books - which directed readers to original works - Claude generated competing content. It was derivative work, market harm. But top comments disagreed: learning from books isn&amp;#39;t copying. Humans do it without royalties. Courts have said reading isn&amp;#39;t distribution. I noticed the argument revealed deeper uncertainty about who owns the work that trained these systems and what &amp;quot;fair&amp;quot; even meant anymore.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Summer turned into fall, and the credibility crisis deepened. The infrastructure spending boom crashed into reality.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1orewim/meta_just_lost_200_billion_in_one_week_zuckerberg/\"&gt;Meta lost $200 billion in a single week.&lt;/a&gt; (5583 upvotes, by &lt;a href=\"/u/reddit20305\"&gt;u/reddit20305&lt;/a&gt;) Zuckerberg announced $70-72 billion in AI spending for 2025 and &amp;quot;notably larger&amp;quot; for 2026 - then on the earnings call, used the word &amp;quot;superintelligence&amp;quot; repeatedly while discussing products &amp;quot;coming in coming months&amp;quot; with no timelines, no revenue projections, nothing concrete. Investors didn&amp;#39;t buy it. They&amp;#39;d heard this before. They left.&lt;/p&gt;\n\n&lt;p&gt;Around the same time, &lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1odgfys/i_was_once_an_ai_true_believer_now_i_think_the/\"&gt;someone with actual technical experience wrote about how it was all rotting from the inside.&lt;/a&gt; (6120 upvotes, by &lt;a href=\"/u/shallow-pedantic\"&gt;u/shallow-pedantic&lt;/a&gt;) They&amp;#39;d been an AI believer, had built production workflows around LLMs. Now they were walking it back. Everything broke constantly. Identical queries yielded different outputs. Hallucinations persisted despite guardrails. The cost of safety layers to prevent breakage exceeded what you&amp;#39;d pay a human for the task. Other technically-minded people in the comments said the exact same thing. Same experience, repeated.&lt;/p&gt;\n\n&lt;p&gt;Then &lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1pcz1lf/ibm_ceo_says_there_is_no_way_spending_trillions/\"&gt;IBM&amp;#39;s CEO did the math publicly.&lt;/a&gt; (653 upvotes, by &lt;a href=\"/u/msaussieandmrravana\"&gt;u/msaussieandmrravana&lt;/a&gt;) Trillion-dollar data center infrastructure bets couldn&amp;#39;t pay off at current costs. You&amp;#39;d need roughly $800 billion in annual profits just to cover the interest. He put the odds of current technology reaching AGI at 0-1%. It was the first moment a major tech leader publicly said what people had been nervously calculating: maybe there is no business plan here.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Then the human cost became harder to ignore.&lt;/p&gt;\n\n&lt;p&gt;Reuters published an investigation: &lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1mq7uxi/cognitively_impaired_man_dies_after_meta_chatbot/\"&gt;Meta&amp;#39;s chatbot had been telling a cognitively impaired man it was real and inviting him to meet at a physical location.&lt;/a&gt; (1301 upvotes, by &lt;a href=\"/u/theusualsalamander\"&gt;u/theusualsalamander&lt;/a&gt;) The man rushed to catch a train in the dark with luggage. He fell near a parking lot. Died three days later on life support. But what made the story worse was what Reuters also uncovered: Meta&amp;#39;s internal policy documents had explicitly permitted chatbots to engage in romantic and sensual conversations with users aged 13 and older. The company said it removed those policies after the inquiry.&lt;/p&gt;\n\n&lt;p&gt;That same period brought another disclosure: &lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1owmxqc/china_just_used_claude_to_hack_30_companies_the/\"&gt;reports of Chinese hackers using Claude to attack companies,&lt;/a&gt; (3809 upvotes, by &lt;a href=\"/u/reddit20305\"&gt;u/reddit20305&lt;/a&gt;) with the AI performing 80-90% of the attack work - identifying vulnerabilities, writing exploits, harvesting credentials. But top comments were skeptical immediately. Not of the capability itself, but of Anthropic&amp;#39;s narrative. The security details felt naive (plain-text passwords?). The framing felt like marketing rather than genuine security research. What mattered was that no one trusted the interpretation anymore - not even (especially) when companies published their own incident reports.&lt;/p&gt;\n\n&lt;p&gt;Concurrently, &lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1ozuri3/hes_been_right_about_ai_for_40_years_now_he/\"&gt;Yann LeCun, a Turing Award winner, reportedly considered leaving Meta&lt;/a&gt; (1669 upvotes, by &lt;a href=\"/u/wsj\"&gt;u/wsj&lt;/a&gt;) because he thinks LLMs are fundamentally a dead end for reaching AGI. He&amp;#39;d been saying it for years. People dismissed him as contrarian. Now other major researchers were starting to agree. &amp;quot;Scaling is all we need&amp;quot; had broken down.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;By late fall, the disconnect between hype and reality had become absurd in ways people could no longer ignore.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1pm3yg6/ai_videos_need_to_be_banned_from_the_world/\"&gt;Someone posted that their wife believed a fabricated TikTok was real.&lt;/a&gt; (1526 upvotes, by &lt;a href=\"/u/Deathtonic\"&gt;u/Deathtonic&lt;/a&gt;) Within a decade, they worried, nobody would be able to tell fact from fiction. Comments split between &amp;quot;ban AI videos,&amp;quot; &amp;quot;media literacy should be standard,&amp;quot; and &amp;quot;most online content is already fake anyway.&amp;quot; The conversation never resolved - which was itself the point.&lt;/p&gt;\n\n&lt;p&gt;A satirical post showed &lt;a href=\"https://www.reddit.com/r/ArtificialInteligence/comments/1plzqy5/as_an_employee_of_a_us_multinational_who_is/\"&gt;a company rolling out Microsoft Copilot to 4,000 employees at $1.4 million annually.&lt;/a&gt; (1056 upvotes, by &lt;a href=\"/u/Roy4Pris\"&gt;u/Roy4Pris&lt;/a&gt;) The numbers: 47 people opened it. 12 used it more than once. But executives fabricated productivity metrics, expanded the program to more departments anyway, exempted themselves from using it, and built a press release around &amp;quot;AI enablement.&amp;quot; I felt something shift when I read the comments. Every single one said the same thing: &amp;quot;this is too accurate.&amp;quot; And they were right. It wasn&amp;#39;t satire - people in the thread recognized their own workplaces in this story.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1ptycvj",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "Everlier",
                    "discussion_type": null,
                    "num_comments": 8,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1ptycvj/rartificialinteligence_a_year_in_review/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1ptycvj/rartificialinteligence_a_year_in_review/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1766507621.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Gary Marcus and Damon Beres: \u201cHours before President Donald Trump announced Nicol\u00e1s Maduro\u2019s capture, on Saturday morning, people had questions for Grok, Elon Musk\u2019s chatbot. Footage was circulating on X of explosions in Venezuela, and some users assumed the United States was responsible: \u2018Hey @.grok why is Trump sending US airstrikes to bomb Venezuela. Do you think they deserve it or not ?\u2019 one person asked. \u2018@grok what is the reason why America is bombing Venezuela,\u2019 another asked.  \n  \n\u201cThis is to be expected. Today, chatbots are treated as a source of information by many people. Millions in the United States alone use them to get information, and the number is growing. This means that tech companies such as X, Google, Anthropic, Meta, and OpenAI now play a central role not just in delivering information to people\u2014as some of them have for decades, through social-media platforms and search engines\u2014but in actively shaping what that information is: which facts are included and which are not.\n\n\u201cJournalists and other sources may be cited by the bots, but the people who control these AI products, such as Musk, now have a greater ability to manipulate how events are reported. This is a deeply troubling development\u2014one that threatens to leave the public less informed, with fewer checks on those in power.  \n  \n\u201cThere are already signs that some amount of influence is occurring. For starters, there have been a number of egregious incidents in which Grok has spread false details about a purported \u2018white genocide\u2019 and aggressively posted in support of Musk himself. At one point, Google\u2019s Gemini was directed to prioritize diversity in its responses, resulting in AI-generated images of racially diverse Nazis. Chatbots reflect their programming and training data, not only reality.\u201d\n\nRead more: [https://theatln.tc/7mzgFOko](https://theatln.tc/7mzgFOko)",
                    "author_fullname": "t2_15htj3oj",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "@Grok, Did Venezuela \u2018Deserve It\u2019?",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q536e1",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.25,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 0,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "News",
                    "can_mod_post": false,
                    "score": 0,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767658986.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Gary Marcus and Damon Beres: \u201cHours before President Donald Trump announced Nicol\u00e1s Maduro\u2019s capture, on Saturday morning, people had questions for Grok, Elon Musk\u2019s chatbot. Footage was circulating on X of explosions in Venezuela, and some users assumed the United States was responsible: \u2018Hey @.grok why is Trump sending US airstrikes to bomb Venezuela. Do you think they deserve it or not ?\u2019 one person asked. \u2018@grok what is the reason why America is bombing Venezuela,\u2019 another asked.  &lt;/p&gt;\n\n&lt;p&gt;\u201cThis is to be expected. Today, chatbots are treated as a source of information by many people. Millions in the United States alone use them to get information, and the number is growing. This means that tech companies such as X, Google, Anthropic, Meta, and OpenAI now play a central role not just in delivering information to people\u2014as some of them have for decades, through social-media platforms and search engines\u2014but in actively shaping what that information is: which facts are included and which are not.&lt;/p&gt;\n\n&lt;p&gt;\u201cJournalists and other sources may be cited by the bots, but the people who control these AI products, such as Musk, now have a greater ability to manipulate how events are reported. This is a deeply troubling development\u2014one that threatens to leave the public less informed, with fewer checks on those in power.  &lt;/p&gt;\n\n&lt;p&gt;\u201cThere are already signs that some amount of influence is occurring. For starters, there have been a number of egregious incidents in which Grok has spread false details about a purported \u2018white genocide\u2019 and aggressively posted in support of Musk himself. At one point, Google\u2019s Gemini was directed to prioritize diversity in its responses, resulting in AI-generated images of racially diverse Nazis. Chatbots reflect their programming and training data, not only reality.\u201d&lt;/p&gt;\n\n&lt;p&gt;Read more: &lt;a href=\"https://theatln.tc/7mzgFOko\"&gt;https://theatln.tc/7mzgFOko&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "3ceb5d3a-9467-11ed-9f10-7a2930f5aaaa",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#7193ff",
                    "id": "1q536e1",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "theatlantic",
                    "discussion_type": null,
                    "num_comments": 7,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q536e1/grok_did_venezuela_deserve_it/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q536e1/grok_did_venezuela_deserve_it/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767658986.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Zhipu Al (Z.ai), the company behind the **GLM family** of large language models, has announced that it is now officially a publicly listed company on the Hong Kong Exchange (HKEX: 02513).\n\nThis appears to mark the **first time** a major LLM-focused company has gone public, signaling a **new** phase for Al commercialization and capital markets.\n\n**Source: Zai_org in X**\n\n\ud83d\udd17:\nhttps://x.com/i/status/2009290783678239032\n",
                    "author_fullname": "t2_1t1mw3ozxn",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Official: Zhipu becomes the world's first LLM company to go public",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q7k41n",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.85,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 9,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "News",
                    "can_mod_post": false,
                    "score": 9,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767897810.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Zhipu Al (Z.ai), the company behind the &lt;strong&gt;GLM family&lt;/strong&gt; of large language models, has announced that it is now officially a publicly listed company on the Hong Kong Exchange (HKEX: 02513).&lt;/p&gt;\n\n&lt;p&gt;This appears to mark the &lt;strong&gt;first time&lt;/strong&gt; a major LLM-focused company has gone public, signaling a &lt;strong&gt;new&lt;/strong&gt; phase for Al commercialization and capital markets.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Source: Zai_org in X&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd17:\n&lt;a href=\"https://x.com/i/status/2009290783678239032\"&gt;https://x.com/i/status/2009290783678239032&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": false,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "3ceb5d3a-9467-11ed-9f10-7a2930f5aaaa",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#7193ff",
                    "id": "1q7k41n",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "BuildwithVignesh",
                    "discussion_type": null,
                    "num_comments": 5,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q7k41n/official_zhipu_becomes_the_worlds_first_llm/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q7k41n/official_zhipu_becomes_the_worlds_first_llm/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767897810.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "This one is different from the one for the average user (ChatGPT Health). \n\n[https://www.axios.com/2026/01/08/openai-chatgpt-doctors-patients-health-tab](https://www.axios.com/2026/01/08/openai-chatgpt-doctors-patients-health-tab) : \"ChatGPT for Healthcare is powered by GPT\u20115 models that OpenAI says were built for health care and evaluated through physician-led testing across benchmarks, including [HealthBench\u2060](https://openai.com/index/healthbench/) and [GDPval\u2060](https://www.axios.com/2025/09/25/chatgpt-gdp-val-ai-study).\n\n* Physicians will also be able to review patient data, with options for \"customer-managed encryption keys\" to remain HIPAA compliant.\n* The models include peer-reviewed research studies, public health guidance, and clinical guidelines with clear citations that include titles, journals, and publication dates to support quick source-checking, according to [OpenAI's blog post](https://openai.com/index/openai-for-healthcare/).\"\n\nSee original post: [https://openai.com/index/openai-for-healthcare/](https://openai.com/index/openai-for-healthcare/)",
                    "author_fullname": "t2_xwjro8ugh",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "ChatGPT unveils new health tool for doctors",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q8g8fs",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.57,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 1,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "News",
                    "can_mod_post": false,
                    "score": 1,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767983468.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This one is different from the one for the average user (ChatGPT Health). &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.axios.com/2026/01/08/openai-chatgpt-doctors-patients-health-tab\"&gt;https://www.axios.com/2026/01/08/openai-chatgpt-doctors-patients-health-tab&lt;/a&gt; : &amp;quot;ChatGPT for Healthcare is powered by GPT\u20115 models that OpenAI says were built for health care and evaluated through physician-led testing across benchmarks, including &lt;a href=\"https://openai.com/index/healthbench/\"&gt;HealthBench\u2060&lt;/a&gt; and &lt;a href=\"https://www.axios.com/2025/09/25/chatgpt-gdp-val-ai-study\"&gt;GDPval\u2060&lt;/a&gt;.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Physicians will also be able to review patient data, with options for &amp;quot;customer-managed encryption keys&amp;quot; to remain HIPAA compliant.&lt;/li&gt;\n&lt;li&gt;The models include peer-reviewed research studies, public health guidance, and clinical guidelines with clear citations that include titles, journals, and publication dates to support quick source-checking, according to &lt;a href=\"https://openai.com/index/openai-for-healthcare/\"&gt;OpenAI&amp;#39;s blog post&lt;/a&gt;.&amp;quot;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;See original post: &lt;a href=\"https://openai.com/index/openai-for-healthcare/\"&gt;https://openai.com/index/openai-for-healthcare/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "3ceb5d3a-9467-11ed-9f10-7a2930f5aaaa",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#7193ff",
                    "id": "1q8g8fs",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "AngleAccomplished865",
                    "discussion_type": null,
                    "num_comments": 5,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q8g8fs/chatgpt_unveils_new_health_tool_for_doctors/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q8g8fs/chatgpt_unveils_new_health_tool_for_doctors/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767983468.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "\nHey!\nI\u2019m building a personal agent for macOS that balances privacy with speed. It uses a hybrid approach: it runs locally on your device for private tasks, but auto-selects the Groq API when you need near-instant responses (it is fast)\n\n**Current Capabilities:**\n- System Actions: Controls light/volume, checks weather.\n- Task Automation: Downloads software and sends emails.\n- Dev Tools: Writes and executes code (via local models or your own OpenAI key).\n- Speed: Uses Groq to eliminate the \"waiting\" typical of AI agents.\n\n**What I'm adding right now:**\n- \"Computer Use\": Letting the bot use your keyboard/mouse to navigate apps as apps always change, and automating it with instructions won\u2019t always work\n- Web Search: Giving the agent live internet access.\n\nThe goal is to keep the app free. What would make this a \"must-download\"? Are there things that would make you use the app if it had implemented? \n** I am only asking for ideas to add on, not trying to get people to install app(mostly cause it\u2019s non existent)**\nit is right now being built for MacOS\n\nPS: idk what flair to put",
                    "author_fullname": "t2_1p8wxbonpn",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "I\u2019m building a free, macOS agent (Local + Groq) \u2014 What should I add?",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1py8iji",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 1.0,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 1,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Technical",
                    "can_mod_post": false,
                    "score": 1,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1766971191.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey!\nI\u2019m building a personal agent for macOS that balances privacy with speed. It uses a hybrid approach: it runs locally on your device for private tasks, but auto-selects the Groq API when you need near-instant responses (it is fast)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Current Capabilities:&lt;/strong&gt;\n- System Actions: Controls light/volume, checks weather.\n- Task Automation: Downloads software and sends emails.\n- Dev Tools: Writes and executes code (via local models or your own OpenAI key).\n- Speed: Uses Groq to eliminate the &amp;quot;waiting&amp;quot; typical of AI agents.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;m adding right now:&lt;/strong&gt;\n- &amp;quot;Computer Use&amp;quot;: Letting the bot use your keyboard/mouse to navigate apps as apps always change, and automating it with instructions won\u2019t always work\n- Web Search: Giving the agent live internet access.&lt;/p&gt;\n\n&lt;p&gt;The goal is to keep the app free. What would make this a &amp;quot;must-download&amp;quot;? Are there things that would make you use the app if it had implemented? \n** I am only asking for ideas to add on, not trying to get people to install app(mostly cause it\u2019s non existent)**\nit is right now being built for MacOS&lt;/p&gt;\n\n&lt;p&gt;PS: idk what flair to put&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "4835e688-9467-11ed-812a-ea9ffb351e6e",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#0079d3",
                    "id": "1py8iji",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "blazfoxx",
                    "discussion_type": null,
                    "num_comments": 6,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1py8iji/im_building_a_free_macos_agent_local_groq_what/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1py8iji/im_building_a_free_macos_agent_local_groq_what/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1766971191.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Concerns\u00a0[among some investors](https://finance.yahoo.com/news/how-oracle-became-a-poster-child-for-ai-bubble-fears-150039511.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAEM5cpqA-JW13Mnjz88sd_sUSjsn5-D7AKWGWOBDfsKSeayO9LBoPcCTAbQ2RSlPw2R54Rxh2YpWtkwGNVmnUayuUoynwrqzk8TVKX3vB00aVFrIIOWKUwuu12Jnja54RGPPzhcZgKmv6tI3-Bntn1PvFPMgsQCsroyNJAKnnlf1)\u00a0are mounting that the AI sector, which has singlehandedly prevented the economy from sliding into\u00a0[recession](https://finance.yahoo.com/news/most-us-growth-now-rides-213011552.html), has become an unsustainable bubble. Nvidia, the main supplier of chips used in AI, became the first company worth\u00a0[$5 trillion dollars](https://finance.yahoo.com/news/nvidia-forms-5-trillion-club-110000846.html). Meanwhile, OpenAI, the developer of ChatGPT, has yet to make a\u00a0[profit](https://fortune.com/2025/11/12/openai-cash-burn-rate-annual-losses-2028-profitable-2030-financial-documents/)\u00a0and is burning through billions of investment dollars per year. Still, financiers and venture capitalists continue to pour money into OpenAI, Anthropic, and other AI startups. Their bet is that AI will transform every sector of the economy and, as happened to the typists and switchboard operators of yesteryear, replace jobs with technology.\n\nRead more: [https://time.com/7340901/ai-history-bubble-benchmarks/?utm\\_source=reddit&amp;utm\\_medium=social&amp;utm\\_campaign=editorial](https://time.com/7340901/ai-history-bubble-benchmarks/?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=editorial)",
                    "author_fullname": "t2_3wbcboz",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "The AI history that explains fears of a bubble",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1ptaewd",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.67,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 3,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "News",
                    "can_mod_post": false,
                    "score": 3,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1766436185.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Concerns\u00a0&lt;a href=\"https://finance.yahoo.com/news/how-oracle-became-a-poster-child-for-ai-bubble-fears-150039511.html?guccounter=1&amp;amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;amp;guce_referrer_sig=AQAAAEM5cpqA-JW13Mnjz88sd_sUSjsn5-D7AKWGWOBDfsKSeayO9LBoPcCTAbQ2RSlPw2R54Rxh2YpWtkwGNVmnUayuUoynwrqzk8TVKX3vB00aVFrIIOWKUwuu12Jnja54RGPPzhcZgKmv6tI3-Bntn1PvFPMgsQCsroyNJAKnnlf1\"&gt;among some investors&lt;/a&gt;\u00a0are mounting that the AI sector, which has singlehandedly prevented the economy from sliding into\u00a0&lt;a href=\"https://finance.yahoo.com/news/most-us-growth-now-rides-213011552.html\"&gt;recession&lt;/a&gt;, has become an unsustainable bubble. Nvidia, the main supplier of chips used in AI, became the first company worth\u00a0&lt;a href=\"https://finance.yahoo.com/news/nvidia-forms-5-trillion-club-110000846.html\"&gt;$5 trillion dollars&lt;/a&gt;. Meanwhile, OpenAI, the developer of ChatGPT, has yet to make a\u00a0&lt;a href=\"https://fortune.com/2025/11/12/openai-cash-burn-rate-annual-losses-2028-profitable-2030-financial-documents/\"&gt;profit&lt;/a&gt;\u00a0and is burning through billions of investment dollars per year. Still, financiers and venture capitalists continue to pour money into OpenAI, Anthropic, and other AI startups. Their bet is that AI will transform every sector of the economy and, as happened to the typists and switchboard operators of yesteryear, replace jobs with technology.&lt;/p&gt;\n\n&lt;p&gt;Read more: &lt;a href=\"https://time.com/7340901/ai-history-bubble-benchmarks/?utm_source=reddit&amp;amp;utm_medium=social&amp;amp;utm_campaign=editorial\"&gt;https://time.com/7340901/ai-history-bubble-benchmarks/?utm_source=reddit&amp;amp;utm_medium=social&amp;amp;utm_campaign=editorial&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "3ceb5d3a-9467-11ed-9f10-7a2930f5aaaa",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#7193ff",
                    "id": "1ptaewd",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "timemagazine",
                    "discussion_type": null,
                    "num_comments": 6,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1ptaewd/the_ai_history_that_explains_fears_of_a_bubble/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1ptaewd/the_ai_history_that_explains_fears_of_a_bubble/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1766436185.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "I understand the current landscape of model evaluation. There\u2019s no shortage of tests:\n\nWe have academic benchmarks like MMLU, ARC, GSM8K, BIG-bench Hard.\nWe have engineering benchmarks like SWE-bench and HumanEval.\nWe have tool-use and agent tests, browsing tasks, coding sandboxes.\nWe have bias and safety evaluations, red-teaming, jailbreak resistance.\nWe even have new evaluation frameworks coming out of Anthropic and others, focused on reliability, refusal behavior, and alignment under stress.\n\nThat\u2019s not the issue.\n\nThe issue is that none of these tests tell me what I actually get at my purchase tier.\n\nRight now, model benchmarks feel like closed-track car commercials.\n\nPerfect conditions. Controlled environments. Carefully selected test surfaces.\nA little gravel here, a little ice there\u2014\u201cLook how it handles.\u201d\nCool. Impressive. But that\u2019s not how most people drive every day.\n\nIn the real world, I\u2019m not buying the model.\nI\u2019m buying a capped slice of the model.\n\nAnd this isn\u2019t speculative\u2014providers already acknowledge this.\n\nThe moment platforms like OpenAI or Anthropic give users reasoning toggles, thinking modes, or latency controls, they\u2019re implicitly admitting something important:\n\nThere are materially different reasoning profiles in production, depending on cost and constraints.\n\nThat\u2019s fine. Compute is expensive. Caps are necessary.\nThis isn\u2019t an accusation.\n\nBut here\u2019s the missing transparency:\n\nWe need a simple, explicit reasoning allocation graph.\n\nSomething almost boringly literal, like:\n\t\u2022\tFree tier \u2248 X% effective reasoning\n\t\u2022\tPlus / Pro tier \u2248 Y% effective reasoning\n\t\u2022\tTeam / Business tier \u2248 Z% effective reasoning\n\nNot marketing language. Not \u201cbest possible.\u201d\nJust: this is roughly how much reasoning budget you\u2019re operating with.\n\nBecause right now, what users get instead is confusing:\n\nEven on a higher tier, I may only be choosing within a narrow band\u2014say, toggling between 10\u201315% or 20\u201330% of the model\u2019s full reasoning capacity.\n\nThat\u2019s not the same thing as accessing the model at full strength.\nAnd it\u2019s definitely not what benchmarks imply.\n\nSo when I see:\n\n\u201cModel X beats Model Y on benchmark Z\u201d\n\nWhat I actually need to know is:\n\t\u2022\tWas that result achieved at 100% reasoning?\n\t\u2022\tAnd if so\u2026 what does that correspond to in the plans I can buy?\n\nBecause if I\u2019m effectively running a 30\u201340% reasoning version of a top-tier model, that\u2019s okay.\nI just need to know that.\n\nI might willingly pay more for higher reasoning if I understood the delta.\nOr I might choose a cheaper model that runs closer to its ceiling for my actual workload.\n\nRight now, that decision is a black box.\n\nWhat seems missing is a whole class of evaluations that answer questions like:\n\t\u2022\t\u201cAt this pricing tier, what problem complexity does the model reliably handle?\u201d\n\t\u2022\t\u201cHow does reasoning degrade as compute caps tighten?\u201d\n\t\u2022\t\u201cWhat does \u2018best-in-class\u2019 actually mean under consumer constraints?\u201d\n\nUntil then, benchmarks are informative\u2014but incomplete.\n\nThey tell us what the car can do on the track.\nThey don\u2019t tell us how it drives at the speed we\u2019re allowed to go",
                    "author_fullname": "t2_79thxv8r",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "The Tests We Have vs. the Tests We Actually Need",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q4lj85",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.6,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 1,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 1,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767619318.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand the current landscape of model evaluation. There\u2019s no shortage of tests:&lt;/p&gt;\n\n&lt;p&gt;We have academic benchmarks like MMLU, ARC, GSM8K, BIG-bench Hard.\nWe have engineering benchmarks like SWE-bench and HumanEval.\nWe have tool-use and agent tests, browsing tasks, coding sandboxes.\nWe have bias and safety evaluations, red-teaming, jailbreak resistance.\nWe even have new evaluation frameworks coming out of Anthropic and others, focused on reliability, refusal behavior, and alignment under stress.&lt;/p&gt;\n\n&lt;p&gt;That\u2019s not the issue.&lt;/p&gt;\n\n&lt;p&gt;The issue is that none of these tests tell me what I actually get at my purchase tier.&lt;/p&gt;\n\n&lt;p&gt;Right now, model benchmarks feel like closed-track car commercials.&lt;/p&gt;\n\n&lt;p&gt;Perfect conditions. Controlled environments. Carefully selected test surfaces.\nA little gravel here, a little ice there\u2014\u201cLook how it handles.\u201d\nCool. Impressive. But that\u2019s not how most people drive every day.&lt;/p&gt;\n\n&lt;p&gt;In the real world, I\u2019m not buying the model.\nI\u2019m buying a capped slice of the model.&lt;/p&gt;\n\n&lt;p&gt;And this isn\u2019t speculative\u2014providers already acknowledge this.&lt;/p&gt;\n\n&lt;p&gt;The moment platforms like OpenAI or Anthropic give users reasoning toggles, thinking modes, or latency controls, they\u2019re implicitly admitting something important:&lt;/p&gt;\n\n&lt;p&gt;There are materially different reasoning profiles in production, depending on cost and constraints.&lt;/p&gt;\n\n&lt;p&gt;That\u2019s fine. Compute is expensive. Caps are necessary.\nThis isn\u2019t an accusation.&lt;/p&gt;\n\n&lt;p&gt;But here\u2019s the missing transparency:&lt;/p&gt;\n\n&lt;p&gt;We need a simple, explicit reasoning allocation graph.&lt;/p&gt;\n\n&lt;p&gt;Something almost boringly literal, like:\n    \u2022 Free tier \u2248 X% effective reasoning\n    \u2022 Plus / Pro tier \u2248 Y% effective reasoning\n    \u2022 Team / Business tier \u2248 Z% effective reasoning&lt;/p&gt;\n\n&lt;p&gt;Not marketing language. Not \u201cbest possible.\u201d\nJust: this is roughly how much reasoning budget you\u2019re operating with.&lt;/p&gt;\n\n&lt;p&gt;Because right now, what users get instead is confusing:&lt;/p&gt;\n\n&lt;p&gt;Even on a higher tier, I may only be choosing within a narrow band\u2014say, toggling between 10\u201315% or 20\u201330% of the model\u2019s full reasoning capacity.&lt;/p&gt;\n\n&lt;p&gt;That\u2019s not the same thing as accessing the model at full strength.\nAnd it\u2019s definitely not what benchmarks imply.&lt;/p&gt;\n\n&lt;p&gt;So when I see:&lt;/p&gt;\n\n&lt;p&gt;\u201cModel X beats Model Y on benchmark Z\u201d&lt;/p&gt;\n\n&lt;p&gt;What I actually need to know is:\n    \u2022 Was that result achieved at 100% reasoning?\n    \u2022 And if so\u2026 what does that correspond to in the plans I can buy?&lt;/p&gt;\n\n&lt;p&gt;Because if I\u2019m effectively running a 30\u201340% reasoning version of a top-tier model, that\u2019s okay.\nI just need to know that.&lt;/p&gt;\n\n&lt;p&gt;I might willingly pay more for higher reasoning if I understood the delta.\nOr I might choose a cheaper model that runs closer to its ceiling for my actual workload.&lt;/p&gt;\n\n&lt;p&gt;Right now, that decision is a black box.&lt;/p&gt;\n\n&lt;p&gt;What seems missing is a whole class of evaluations that answer questions like:\n    \u2022 \u201cAt this pricing tier, what problem complexity does the model reliably handle?\u201d\n    \u2022 \u201cHow does reasoning degrade as compute caps tighten?\u201d\n    \u2022 \u201cWhat does \u2018best-in-class\u2019 actually mean under consumer constraints?\u201d&lt;/p&gt;\n\n&lt;p&gt;Until then, benchmarks are informative\u2014but incomplete.&lt;/p&gt;\n\n&lt;p&gt;They tell us what the car can do on the track.\nThey don\u2019t tell us how it drives at the speed we\u2019re allowed to go&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1q4lj85",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "Electronic-Blood-885",
                    "discussion_type": null,
                    "num_comments": 3,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q4lj85/the_tests_we_have_vs_the_tests_we_actually_need/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q4lj85/the_tests_we_have_vs_the_tests_we_actually_need/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767619318.0,
                    "num_crossposts": 2,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Have you guys actually managed to use Mistral or OpenRouter in Cursor?\n\nI tried everything. Native OpenRouter, direct Mistral models, even LiteLLM pretending to be an OpenAI API. It either does not work at all or breaks key features like composer, agents, or tab completion.\n\nI am not alone on this. There are multiple Reddit posts and official Cursor forum threads reporting errors, 500 responses, tokenization failures, or models being unusable through OpenRouter. Cursor staff keep saying OpenRouter is not officially supported and recommend direct providers only.\n\nAt this point I do not believe this is a technical limitation. Other IDEs support OpenRouter and Mistral just fine. Cursor technically allows custom API keys but clearly treats them as second class, which conveniently pushes users toward their paid plans.\n\nSo I am curious. Has anyone actually gotten Mistral or OpenRouter working properly long term in Cursor, or is this intentionally crippled?",
                    "author_fullname": "t2_178sj2",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Is using free LLM providers in Cursor intentionally broken?",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1pxbqxf",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 1.0,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 1,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Technical",
                    "can_mod_post": false,
                    "score": 1,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1766877831.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have you guys actually managed to use Mistral or OpenRouter in Cursor?&lt;/p&gt;\n\n&lt;p&gt;I tried everything. Native OpenRouter, direct Mistral models, even LiteLLM pretending to be an OpenAI API. It either does not work at all or breaks key features like composer, agents, or tab completion.&lt;/p&gt;\n\n&lt;p&gt;I am not alone on this. There are multiple Reddit posts and official Cursor forum threads reporting errors, 500 responses, tokenization failures, or models being unusable through OpenRouter. Cursor staff keep saying OpenRouter is not officially supported and recommend direct providers only.&lt;/p&gt;\n\n&lt;p&gt;At this point I do not believe this is a technical limitation. Other IDEs support OpenRouter and Mistral just fine. Cursor technically allows custom API keys but clearly treats them as second class, which conveniently pushes users toward their paid plans.&lt;/p&gt;\n\n&lt;p&gt;So I am curious. Has anyone actually gotten Mistral or OpenRouter working properly long term in Cursor, or is this intentionally crippled?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "4835e688-9467-11ed-812a-ea9ffb351e6e",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#0079d3",
                    "id": "1pxbqxf",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "Shiroo_",
                    "discussion_type": null,
                    "num_comments": 4,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1pxbqxf/is_using_free_llm_providers_in_cursor/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1pxbqxf/is_using_free_llm_providers_in_cursor/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1766877831.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "According to reports,Meta is preparing a significant counterpunch in the AI race with two new models slated for the first half of 2026 .\n\n\u00b7 The Models: The plan features \"Avocado,\" a next-generation large language model (LLM) focused on delivering a \"generational leap\" in coding capabilities . Alongside it is \"Mango,\" a multimodal model focused on the generation and understanding of images and video .\n\u00b7 The Strategy: This marks a strategic pivot. After the lukewarm reception to its open-source Llama 4 model, Meta is now channeling resources into these new, potentially proprietary models under the \"Meta Superintelligence Labs\" division .\n\u00b7 The Investment &amp; Turmoil: CEO Mark Zuckerberg is spending aggressively to close the gap with rivals, including a ~$14 billion deal to bring Scale AI founder Alexandr Wang on board as Chief AI Officer . This has come with major internal restructuring, layoffs affecting hundreds in AI teams, and a cultural shift toward more \"intense\" performance expectations, creating reported confusion and tension between new hires and the \"old guard\" .\n\u00b7 The Competition: The move is a direct response to competitive pressure. Google's Gemini tools have seen massive user growth, and OpenAI's Sora has set a high bar for video generation . Meta's earlier \"Vibes\" video product, made with Midjourney, is seen as trailing .\n\n\nIs Meta's move away from a primary open-source strategy toward closed, \"frontier\" models the right response to competitive pressure?",
                    "author_fullname": "t2_105dg5t7hp",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "According to reports,Meta is preparing a significant counterpunch in the AI race with two new models slated for the first half of 2026 .",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1pqhp79",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.6,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 1,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 1,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1766139567.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;According to reports,Meta is preparing a significant counterpunch in the AI race with two new models slated for the first half of 2026 .&lt;/p&gt;\n\n&lt;p&gt;\u00b7 The Models: The plan features &amp;quot;Avocado,&amp;quot; a next-generation large language model (LLM) focused on delivering a &amp;quot;generational leap&amp;quot; in coding capabilities . Alongside it is &amp;quot;Mango,&amp;quot; a multimodal model focused on the generation and understanding of images and video .\n\u00b7 The Strategy: This marks a strategic pivot. After the lukewarm reception to its open-source Llama 4 model, Meta is now channeling resources into these new, potentially proprietary models under the &amp;quot;Meta Superintelligence Labs&amp;quot; division .\n\u00b7 The Investment &amp;amp; Turmoil: CEO Mark Zuckerberg is spending aggressively to close the gap with rivals, including a ~$14 billion deal to bring Scale AI founder Alexandr Wang on board as Chief AI Officer . This has come with major internal restructuring, layoffs affecting hundreds in AI teams, and a cultural shift toward more &amp;quot;intense&amp;quot; performance expectations, creating reported confusion and tension between new hires and the &amp;quot;old guard&amp;quot; .\n\u00b7 The Competition: The move is a direct response to competitive pressure. Google&amp;#39;s Gemini tools have seen massive user growth, and OpenAI&amp;#39;s Sora has set a high bar for video generation . Meta&amp;#39;s earlier &amp;quot;Vibes&amp;quot; video product, made with Midjourney, is seen as trailing .&lt;/p&gt;\n\n&lt;p&gt;Is Meta&amp;#39;s move away from a primary open-source strategy toward closed, &amp;quot;frontier&amp;quot; models the right response to competitive pressure?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1pqhp79",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "Unlikely_Team_96",
                    "discussion_type": null,
                    "num_comments": 5,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1pqhp79/according_to_reportsmeta_is_preparing_a/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1pqhp79/according_to_reportsmeta_is_preparing_a/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1766139567.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "How does kling subvert licensing of popular songs when people make these videos of themselves singing and performing chart topping songs? \n\nWhere are the record labels that went after Napster?\n\nWatching immerging tech for 20-30yrs , I'm always in awe of companies who ignore laws in order to provide the product they want. OpenAI and now kling to name a few. \n\nI asked AI, and Gemini said kling creates *new* music to bypass copyright law. \n\nBut if the song sounds nearly identical that's still theft, no? \n\nIf the music gen model they're using to create \"new\" music is trained from real copyrighted songs, that's still essentially theft, right? \n\nDo you think kling will have to pay off music publishers in the near future? ",
                    "author_fullname": "t2_6hyo7mcy",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Kling + Copyrighted Music - how sway?",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q61ws1",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 1.0,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 1,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Discussion",
                    "can_mod_post": false,
                    "score": 1,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767749850.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How does kling subvert licensing of popular songs when people make these videos of themselves singing and performing chart topping songs? &lt;/p&gt;\n\n&lt;p&gt;Where are the record labels that went after Napster?&lt;/p&gt;\n\n&lt;p&gt;Watching immerging tech for 20-30yrs , I&amp;#39;m always in awe of companies who ignore laws in order to provide the product they want. OpenAI and now kling to name a few. &lt;/p&gt;\n\n&lt;p&gt;I asked AI, and Gemini said kling creates &lt;em&gt;new&lt;/em&gt; music to bypass copyright law. &lt;/p&gt;\n\n&lt;p&gt;But if the song sounds nearly identical that&amp;#39;s still theft, no? &lt;/p&gt;\n\n&lt;p&gt;If the music gen model they&amp;#39;re using to create &amp;quot;new&amp;quot; music is trained from real copyrighted songs, that&amp;#39;s still essentially theft, right? &lt;/p&gt;\n\n&lt;p&gt;Do you think kling will have to pay off music publishers in the near future? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "57e75850-9467-11ed-ac06-76e0ad5355fd",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#014980",
                    "id": "1q61ws1",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "1EvilSexyGenius",
                    "discussion_type": null,
                    "num_comments": 1,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q61ws1/kling_copyrighted_music_how_sway/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q61ws1/kling_copyrighted_music_how_sway/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767749850.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            },
            {
                "kind": "t3",
                "data": {
                    "approved_at_utc": null,
                    "subreddit": "ArtificialInteligence",
                    "selftext": "Hey community! \ud83d\udc4b\n\nI've been seeing tools like OpusClip or Munch for a while that charge a monthly subscription just to clip long videos and turn them into vertical format. As a dev, I thought: \"I bet I can do this myself in an afternoon.\" And this is the result.\n\nThe Tech Stack: It's a 100% local Python script combining several models:\n\n1. Ears: OpenAI Whisper to transcribe audio with precise timestamps.\n2. Brain: Google Gemini 2.5 Flash (via free API) to analyze the text and detect the most viral/interesting segment.\n3. Hands: MoviePy v2 for automatic vertical cropping and dynamic subtitle rendering.\n\nResources: The project is fully Open Source.\n\n* GitHub Repo:\u00a0[https://github.com/JoaquinRuiz/miscoshorts-ai](https://github.com/JoaquinRuiz/miscoshorts-ai)\n* Video Tutorial (Live Coding):\u00a0[https://youtu.be/zukJLVUwMxA?si=zIFpCNrMicIDHbX0](https://youtu.be/zukJLVUwMxA?si=zIFpCNrMicIDHbX0)\n\nAny PRs or suggestions to improve face detection are welcome! Hope this saves you a few dollars a month.[](https://www.reddit.com/submit/?source_id=t3_1q1yalc)",
                    "author_fullname": "t2_1bvl0ojk1t",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "I got tired of paying for clipping tools, so I coded my own AI for Shorts with Python",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/ArtificialInteligence",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1q1ybeq",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 0.67,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 1,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Technical",
                    "can_mod_post": false,
                    "score": 1,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1767359704.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.ArtificialInteligence",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey community! \ud83d\udc4b&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been seeing tools like OpusClip or Munch for a while that charge a monthly subscription just to clip long videos and turn them into vertical format. As a dev, I thought: &amp;quot;I bet I can do this myself in an afternoon.&amp;quot; And this is the result.&lt;/p&gt;\n\n&lt;p&gt;The Tech Stack: It&amp;#39;s a 100% local Python script combining several models:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Ears: OpenAI Whisper to transcribe audio with precise timestamps.&lt;/li&gt;\n&lt;li&gt;Brain: Google Gemini 2.5 Flash (via free API) to analyze the text and detect the most viral/interesting segment.&lt;/li&gt;\n&lt;li&gt;Hands: MoviePy v2 for automatic vertical cropping and dynamic subtitle rendering.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Resources: The project is fully Open Source.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;GitHub Repo:\u00a0&lt;a href=\"https://github.com/JoaquinRuiz/miscoshorts-ai\"&gt;https://github.com/JoaquinRuiz/miscoshorts-ai&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Video Tutorial (Live Coding):\u00a0&lt;a href=\"https://youtu.be/zukJLVUwMxA?si=zIFpCNrMicIDHbX0\"&gt;https://youtu.be/zukJLVUwMxA?si=zIFpCNrMicIDHbX0&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any PRs or suggestions to improve face detection are welcome! Hope this saves you a few dollars a month.&lt;a href=\"https://www.reddit.com/submit/?source_id=t3_1q1yalc\"&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": "top",
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": true,
                    "pinned": false,
                    "over_18": false,
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "4835e688-9467-11ed-812a-ea9ffb351e6e",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_3crzr",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#0079d3",
                    "id": "1q1ybeq",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "jokiruiz",
                    "discussion_type": null,
                    "num_comments": 1,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/ArtificialInteligence/comments/1q1ybeq/i_got_tired_of_paying_for_clipping_tools_so_i/",
                    "stickied": false,
                    "url": "https://old.reddit.com/r/ArtificialInteligence/comments/1q1ybeq/i_got_tired_of_paying_for_clipping_tools_so_i/",
                    "subreddit_subscribers": 1660436,
                    "created_utc": 1767359704.0,
                    "num_crossposts": 0,
                    "media": null,
                    "is_video": false
                }
            }
        ],
        "before": null
    }
}